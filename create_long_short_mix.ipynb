{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-29 15:46:54,513] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "from fairseq.examples.speech_to_text.data_utils import load_df_from_tsv, save_df_to_tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"/mnt/taurus/data/xixu/llm/llama-2-7b/hf\",\n",
    "    padding_side=\"right\",    \n",
    "    truncation=False,\n",
    "    add_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/taurus/data/xixu/datasets/must-c-v1.0/en-es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_df_from_tsv(os.path.join(root, 'train_mfa.tsv'))\n",
    "dev_df = load_df_from_tsv(os.path.join(root, 'dev_mfa.tsv'))\n",
    "dfs = {\n",
    "    'train': train_df,\n",
    "    'dev': dev_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804f0fb79ce04b9db6eff3070d0da052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5916c75f63849428e72026d279fa221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_dfs = {}\n",
    "for split in ['train', 'dev']:\n",
    "    df = dfs[split]\n",
    "    max_duration = 30\n",
    "    ted_offsets = [(int(df['id'][i].split('_')[1]), int(df['audio'][i].split(':')[1]), i) for i in range(len(df))]\n",
    "    sorted_ted_offsets = sorted(ted_offsets)\n",
    "    indices = [i for _, _, i in sorted_ted_offsets]\n",
    "    df = df.reindex(indices)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    last_ted_id = ''\n",
    "    counter = 0\n",
    "    offset = 0\n",
    "    duration = 0\n",
    "    n_token = 0\n",
    "    src_text_buffer = ''\n",
    "    tgt_text_buffer = ''\n",
    "    speech_word_buffer = []\n",
    "    text_word_buffer = []\n",
    "\n",
    "    columns = list(df.columns)[1:]\n",
    "    new_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        cur_ted_id = df['id'][i].split('_')[1]\n",
    "        path, cur_offset, cur_n_frames = df['audio'][i].split(':')\n",
    "        cur_offset = int(cur_offset)\n",
    "        cur_n_frames = int(cur_n_frames)\n",
    "        if (cur_ted_id == last_ted_id and (cur_offset + cur_n_frames - offset) / 16000 >= max_duration) or cur_ted_id != last_ted_id:\n",
    "            if i > 0 and duration > 0:\n",
    "                cur_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"id\": \"ted_{}_{}\".format(last_ted_id, counter),\n",
    "                        \"audio\": \"{}:{}:{}\".format(path, offset, duration),\n",
    "                        \"n_frames\": duration,\n",
    "                        \"speaker\": df['speaker'][i],\n",
    "                        \"src_text\": src_text_buffer,\n",
    "                        \"tgt_text\": tgt_text_buffer,\n",
    "                        \"src_lang\": \"en\",\n",
    "                        \"tgt_lang\": \"es\",\n",
    "                        \"speech_word\": [speech_word_buffer],\n",
    "                        \"text_word\": [text_word_buffer]\n",
    "                    }\n",
    "                )\n",
    "                new_df = pd.concat([new_df, cur_df], ignore_index=True)\n",
    "\n",
    "            counter = counter + 1 if cur_ted_id == last_ted_id else 0\n",
    "            last_ted_id = cur_ted_id\n",
    "            offset = cur_offset\n",
    "            duration = cur_n_frames\n",
    "            src_text_buffer = df['src_text'][i]\n",
    "            tgt_text_buffer = df['tgt_text'][i]\n",
    "            speech_word_buffer = eval(df['speech_word'][i]) if df['speech_word'][i] != '' else []\n",
    "            text_word_buffer = eval(df['text_word'][i]) if df['text_word'][i] != '' else []\n",
    "            n_token = len(tokenizer(df['src_text'][i], add_special_tokens=False).input_ids)\n",
    "\n",
    "        else:\n",
    "            duration = cur_offset + cur_n_frames - offset\n",
    "            assert duration > 0, (i, offset)\n",
    "            src_text_buffer = src_text_buffer + ' ' + df['src_text'][i]\n",
    "            tgt_text_buffer = tgt_text_buffer + ' ' + df['tgt_text'][i]\n",
    "\n",
    "            cur_speech_word = eval(df['speech_word'][i]) if df['speech_word'][i] != '' else []\n",
    "            cur_text_word = eval(df['text_word'][i]) if df['text_word'][i] != '' else []\n",
    "\n",
    "            offset_diff = (cur_offset - offset) / 16000\n",
    "            speech_word_buffer = speech_word_buffer + [(l + offset_diff, r + offset_diff) for l, r in cur_speech_word]\n",
    "\n",
    "            cur_n_token = len(tokenizer(df['src_text'][i], add_special_tokens=False).input_ids)\n",
    "            n_token_diff = len(tokenizer(src_text_buffer, add_special_tokens=False).input_ids) - cur_n_token\n",
    "            text_word_buffer = text_word_buffer + [(l + n_token_diff, r + n_token_diff) for l, r in cur_text_word]\n",
    "            n_token = n_token_diff + cur_n_token\n",
    "\n",
    "    if duration > 0:\n",
    "        cur_df = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": \"ted_{}_{}\".format(last_ted_id, counter),\n",
    "                \"audio\": \"{}:{}:{}\".format(path, offset, duration),\n",
    "                \"n_frames\": duration,\n",
    "                \"speaker\": df['speaker'][i],\n",
    "                \"src_text\": src_text_buffer,\n",
    "                \"tgt_text\": tgt_text_buffer,\n",
    "                \"src_lang\": \"en\",\n",
    "                \"tgt_lang\": \"es\",\n",
    "                \"speech_word\": [speech_word_buffer],\n",
    "                \"text_word\": [text_word_buffer]\n",
    "            }\n",
    "        )\n",
    "        new_df = pd.concat([new_df, cur_df], ignore_index=True)\n",
    "    new_dfs[split] = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(new_dfs['train'], os.path.join(root, 'train_mfa_30s.tsv'))\n",
    "save_df_to_tsv(new_dfs['dev'], os.path.join(root, 'dev_mfa_30s.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'dev']:\n",
    "    new_dfs[split]['id'] = ['30s_' + id for id in new_dfs[split]['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_df_from_tsv(os.path.join(root, 'train_mfa.tsv'))\n",
    "dev_df = load_df_from_tsv(os.path.join(root, 'dev_mfa.tsv'))\n",
    "dfs = {\n",
    "    'train': train_df,\n",
    "    'dev': dev_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_dfs = {}\n",
    "for split in ['train', 'dev']:\n",
    "    mix_dfs[split] = pd.concat([dfs[split], new_dfs[split]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_to_tsv(mix_dfs['train'], os.path.join(root, 'train_mfa_30s_mix.tsv'))\n",
    "save_df_to_tsv(mix_dfs['dev'], os.path.join(root, 'dev_mfa_30s_mix.tsv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'dev']:\n",
    "    mix_df = mix_dfs[split]\n",
    "    mask = [s is not None and len(s) > 0 for s in mix_df['speech_word']]\n",
    "    save_df_to_tsv(mix_df[mask], os.path.join(root, '{}_mfa_30s_mix_filtered.tsv'.format(split)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sllama_lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
