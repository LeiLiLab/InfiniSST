{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-23 06:39:58,288] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import fairseq\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from fairseq.models import (\n",
    "    FairseqEncoder, \n",
    "    register_model, \n",
    "    register_model_architecture\n",
    ")\n",
    "from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Model, Wav2Vec2Config\n",
    "from fairseq.models import BaseFairseqModel, register_model\n",
    "from fairseq.models.wav2vec import (\n",
    "    TransformerEncoder,\n",
    "    TransformerSentenceEncoderLayer,\n",
    "    Wav2Vec2Model,\n",
    "    Wav2VecEncoder    \n",
    ")\n",
    "from fairseq.data.audio.speech_to_text_dataset import _collate_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_forward = TransformerSentenceEncoderLayer.forward\n",
    "\n",
    "# def generate_2d_causal_mask(seq_len, device='cpu'):\n",
    "#     \"\"\"\n",
    "#     Generates a 2D causal mask for multi-head attention.\n",
    "    \n",
    "#     Args:\n",
    "#         seq_len (int): The length of the sequence.\n",
    "#         device (str): The device on which to create the mask.\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: A 2D causal attention mask.\n",
    "#     \"\"\"\n",
    "#     mask = torch.triu(torch.ones((seq_len, seq_len), device=device), diagonal=1)\n",
    "#     mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "#     return mask\n",
    "\n",
    "# def causal_forward(\n",
    "#     self,\n",
    "#     x: torch.Tensor,\n",
    "#     self_attn_mask: torch.Tensor = None,\n",
    "#     self_attn_padding_mask: torch.Tensor = None,\n",
    "#     need_weights: bool = False,\n",
    "#     att_args=None,\n",
    "# ):\n",
    "#     # Generate the causal mask\n",
    "#     # print(x)\n",
    "#     # print(x.size(2))\n",
    "#     # print(self_attn_mask)\n",
    "#     causal_mask = generate_2d_causal_mask(x.size(0), device=x.device)\n",
    "    \n",
    "#     if self_attn_mask is not None:\n",
    "#         self_attn_mask = self_attn_mask + causal_mask\n",
    "#     else:\n",
    "#         self_attn_mask = causal_mask\n",
    "\n",
    "#     return original_forward(\n",
    "#         self, x, \n",
    "#         self_attn_mask=self_attn_mask, \n",
    "#         self_attn_padding_mask=self_attn_padding_mask, \n",
    "#         need_weights=need_weights,\n",
    "#         att_args=att_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_causal_mask(seq_len, dtype, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates a 2D causal mask for multi-head attention.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): The length of the sequence.\n",
    "        device (str): The device on which to create the mask.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A 2D causal attention mask.\n",
    "    \"\"\"\n",
    "    # mask = torch.triu(torch.ones((seq_len, seq_len), device=device), diagonal=1)\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len), device=device, dtype=dtype), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "def causal_forward(\n",
    "    self,\n",
    "    x: torch.Tensor,\n",
    "    self_attn_mask: torch.Tensor = None,\n",
    "    self_attn_padding_mask: torch.Tensor = None,\n",
    "    need_weights: bool = False,\n",
    "    att_args=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LayerNorm is applied either before or after the self-attention/ffn\n",
    "    modules similar to the original Transformer imlementation.\n",
    "    \"\"\"\n",
    "    # causal_mask = generate_2d_causal_mask(x.size(0), device=x.device)\n",
    "    causal_mask = generate_2d_causal_mask(x.size(0), dtype=x.dtype,device=x.device)\n",
    "    \n",
    "    if self_attn_mask is not None:\n",
    "        self_attn_mask = self_attn_mask + causal_mask\n",
    "    else:\n",
    "        self_attn_mask = causal_mask\n",
    "        \n",
    "    residual = x\n",
    "\n",
    "    if self.layer_norm_first:\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            attn_mask=self_attn_mask,\n",
    "            need_weights=True,\n",
    "        )\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        layer_result = x\n",
    "\n",
    "        x = self.dropout3(x)\n",
    "        x = residual + x\n",
    "    else:\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            need_weights=True,\n",
    "        )\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        layer_result = x\n",
    "\n",
    "        x = self.dropout3(x)\n",
    "        x = residual + x\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "    return x, (attn, layer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try replace multihead attention with causal multihead attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try replace wav2vec forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_forward():\n",
    "    TransformerSentenceEncoderLayer.forward = causal_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/taurus/home/siqiouyang/anaconda3/envs/sllama/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "speech_tower_path = '/mnt/taurus/data/xixu/models/wav2_vec_vox_960h_pl.pt'\n",
    "state = fairseq.checkpoint_utils.load_checkpoint_to_cpu(speech_tower_path)\n",
    "model = Wav2VecEncoder(state['cfg']['model'], None)\n",
    "new = {}\n",
    "for key in state['model'].keys():\n",
    "    new_key = key.replace('w2v_encoder.', '')\n",
    "    if not new_key.startswith('proj'):\n",
    "        new[new_key] = state['model'][key]\n",
    "model.load_state_dict(new, strict=True)\n",
    "model = model.w2v_model\n",
    "replace_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 466, 1024])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Attention Weight\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import numpy\n",
    "from train.dataset import PromptSpeechToTextDatasetCreator, SpeechToTextDatasetItem\n",
    "\n",
    "\n",
    "replace_forward()\n",
    "def visualize_attention_weights(model, plot_size=10):\n",
    "    test_dataset = PromptSpeechToTextDatasetCreator.from_tsv(\"/mnt/data/xixu/datasets/must-c-v1.0/en-es/\", 'tst-COMMON_1' )\n",
    "    for test_data in test_dataset:\n",
    "        source, ref, id = test_data.source, test_data.target, test_data.id                  \n",
    "        speech_batch = _collate_frames([source], is_audio_input=True)\n",
    "\n",
    "        model.eval()\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            result = model.extract_features(speech_batch, padding_mask=None)\n",
    "        # ((x, z, lr))\n",
    "        # changed https://github.com/facebookresearch/fairseq/blob/fad2c4d1ebe14d974876de52dcb06db6d99b0b4a/fairseq/models/wav2vec/wav2vec2.py#L1330C34-L1330C34 \n",
    "        # to get attention weights  \n",
    "        attn = result['layer_results'][0][1]\n",
    "        feature = result[\"x\"]\n",
    "        print(feature.size())\n",
    "        # print(attn.size())\n",
    "        attn = attn[0] if attn.ndim == 3 else attn\n",
    "        # print(attn.size())\n",
    "        # Select a smaller portion of the attention matrix to visualize\n",
    "        small_attn = attn[:plot_size, :plot_size].cpu().numpy()\n",
    "\n",
    "        # Visualize the attention weights\n",
    "        plt.matshow(small_attn)\n",
    "        plt.title(f\"Attention Weights - First {plot_size} Timesteps\")\n",
    "        plt.xlabel(\"Key Positions\")\n",
    "        plt.ylabel(\"Query Positions\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with the model and desired input length\n",
    "visualize_attention_weights(model, plot_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental w2v2 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.modules import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiheadAttention(512, 8, dropout=0.0, self_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = generate_2d_causal_mask(x.size(0), dtype=x.dtype, device=x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "torch.Size([8, 2, 2]) torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "attn, attn_weights = mha.forward(x, x, x, incremental_state=incremental_state, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(3, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = generate_2d_causal_mask(5, dtype=x.dtype, device=x.device)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prev_key': tensor([[[[-3.4721e-01,  6.8438e-02,  2.2853e-01,  ..., -9.4461e-02,\n",
      "           -2.8663e-01, -2.4116e-01],\n",
      "          [-3.7732e-01,  9.6393e-02,  2.8277e-01,  ...,  4.0914e-01,\n",
      "            1.9507e-01, -2.6538e-01]],\n",
      "\n",
      "         [[ 6.2529e-01, -4.8792e-02,  4.1974e-01,  ..., -2.0654e-02,\n",
      "            6.9791e-04, -1.5041e-02],\n",
      "          [ 1.7753e-01, -5.4947e-01,  9.4435e-01,  ...,  6.4753e-01,\n",
      "            1.6760e-01, -3.7800e-02]],\n",
      "\n",
      "         [[ 1.4639e-01, -1.9122e-02,  5.2609e-01,  ..., -4.8424e-01,\n",
      "            1.6980e-01,  2.9214e-01],\n",
      "          [ 1.7522e-01, -4.4485e-01,  7.2312e-01,  ..., -4.6651e-01,\n",
      "           -6.8658e-02,  3.8094e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4122e-01, -5.7706e-02,  4.5700e-02,  ...,  1.0203e-01,\n",
      "           -6.2049e-01,  3.3837e-01],\n",
      "          [-1.7787e-01,  1.5897e-01,  1.5635e-02,  ...,  1.3330e-01,\n",
      "            6.1628e-02,  5.4253e-01]],\n",
      "\n",
      "         [[ 1.7910e-01,  2.5651e-01,  1.2945e-01,  ...,  3.1836e-01,\n",
      "           -1.8421e-01, -2.0488e-01],\n",
      "          [ 6.4342e-01, -2.5835e-02,  2.6876e-01,  ...,  1.2632e-01,\n",
      "            1.0412e-01, -5.1139e-01]],\n",
      "\n",
      "         [[-1.5559e-01, -6.0589e-01, -5.8342e-01,  ...,  4.8206e-01,\n",
      "            3.1285e-01, -2.9716e-01],\n",
      "          [ 4.7728e-01, -9.1020e-02, -2.9905e-01,  ...,  3.2042e-01,\n",
      "            2.3963e-01,  1.0729e-02]]]], grad_fn=<ViewBackward0>), 'prev_value': tensor([[[[-0.4672, -0.3779, -0.5797,  ...,  0.0346, -0.4637,  0.0266],\n",
      "          [-0.3301, -0.5545, -0.7620,  ..., -0.1434, -0.6904,  0.4113]],\n",
      "\n",
      "         [[ 0.5324,  0.2100,  0.3516,  ...,  0.3891, -0.6924, -0.1719],\n",
      "          [ 0.3956, -0.4170,  0.1692,  ...,  0.2522, -0.1988, -0.2220]],\n",
      "\n",
      "         [[-0.2027, -0.8578, -0.0356,  ..., -0.4511,  0.2573, -0.0323],\n",
      "          [ 0.0468, -0.5124,  0.1253,  ..., -0.1604,  0.3941,  0.2984]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3477,  0.2017, -0.3710,  ..., -0.1488, -0.2837,  0.9358],\n",
      "          [-0.1666, -0.1019,  0.0678,  ...,  0.2011, -0.2804,  0.6909]],\n",
      "\n",
      "         [[-0.8544, -0.3870, -0.3005,  ..., -0.3821, -0.0628, -0.0915],\n",
      "          [-0.5406, -0.3082, -0.8331,  ..., -0.5906, -0.2459,  0.0608]],\n",
      "\n",
      "         [[-0.7119, -0.3641, -0.1951,  ..., -0.1893,  0.3785,  0.1234],\n",
      "          [-0.3732, -0.4336, -0.3552,  ...,  0.0025, -0.1264,  0.1926]]]],\n",
      "       grad_fn=<ViewBackward0>), 'prev_key_padding_mask': None}\n",
      "torch.Size([8, 3, 5]) torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "attn, attn_weights = mha.forward(y, y, y, incremental_state=incremental_state, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 5, 64])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(incremental_state.values())[0]['prev_key'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental llama encoding with w2v2 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-23 19:26:11,734] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-01-23 19:26:16 | INFO | accelerate.utils.modeling | We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa7a815611f4dc7bc07456f486b3a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mnt/taurus/data/xixu/runs/sllama/en-es/7b/uni/stage2/checkpoint-2000 were not used when initializing SpeechLlamaForCausalLM: ['model.speech_tower.encoder.layers.18.final_layer_norm.weight', 'model.speech_tower.encoder.layers.1.final_layer_norm.weight', 'model.speech_tower.encoder.layers.22.fc2.weight', 'model.speech_tower.encoder.layers.21.fc2.weight', 'model.mm_mlp_adapter.weight', 'model.speech_tower.encoder.layers.17.final_layer_norm.weight', 'model.speech_tower.encoder.layers.19.fc2.weight', 'model.speech_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.11.fc1.bias', 'model.speech_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.12.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.19.fc1.weight', 'model.speech_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.15.fc2.bias', 'model.speech_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.17.fc2.bias', 'model.speech_tower.encoder.layers.23.final_layer_norm.bias', 'model.speech_tower.encoder.layers.23.fc1.bias', 'model.speech_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.15.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.23.final_layer_norm.weight', 'model.speech_tower.feature_extractor.conv_layers.5.2.1.weight', 'model.speech_tower.encoder.layers.16.final_layer_norm.weight', 'model.speech_tower.encoder.layers.22.fc2.bias', 'model.speech_tower.feature_extractor.conv_layers.1.2.1.bias', 'model.mm_length_adapter.conv_layers.1.weight', 'model.speech_tower.encoder.layers.4.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.10.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.12.final_layer_norm.weight', 'model.speech_tower.encoder.layers.2.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.speech_tower.feature_extractor.conv_layers.1.2.1.weight', 'model.speech_tower.feature_extractor.conv_layers.6.0.weight', 'model.speech_tower.encoder.layers.23.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.16.fc2.bias', 'model.speech_tower.encoder.layers.17.fc1.bias', 'model.speech_tower.encoder.layers.17.final_layer_norm.bias', 'model.speech_tower.encoder.layers.19.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.3.fc1.weight', 'model.speech_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.17.fc1.weight', 'model.speech_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.14.fc1.weight', 'model.speech_tower.encoder.layers.4.final_layer_norm.weight', 'model.speech_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.2.self_attn_layer_norm.weight', 'model.speech_tower.feature_extractor.conv_layers.6.2.1.weight', 'model.speech_tower.encoder.layers.21.fc1.bias', 'model.speech_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.10.fc1.bias', 'model.speech_tower.encoder.layers.14.fc1.bias', 'model.speech_tower.encoder.layers.14.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.11.final_layer_norm.weight', 'model.speech_tower.encoder.layers.1.fc1.bias', 'model.speech_tower.encoder.layers.1.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.10.final_layer_norm.bias', 'model.speech_tower.encoder.layers.6.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.8.final_layer_norm.weight', 'model.speech_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.11.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.11.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.mm_length_adapter.conv_layers.1.bias', 'model.speech_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.18.fc2.bias', 'model.speech_tower.encoder.layers.13.fc2.weight', 'model.speech_tower.encoder.layers.16.final_layer_norm.bias', 'model.speech_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.7.self_attn_layer_norm.weight', 'model.speech_tower.feature_extractor.conv_layers.5.0.weight', 'model.speech_tower.encoder.layers.1.fc1.weight', 'model.speech_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.3.fc1.bias', 'model.speech_tower.encoder.layers.17.fc2.weight', 'model.mm_mlp_adapter.bias', 'model.speech_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.16.fc2.weight', 'model.speech_tower.encoder.layers.20.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.21.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.4.fc1.bias', 'model.speech_tower.encoder.layers.18.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.11.fc2.weight', 'model.speech_tower.encoder.layers.15.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.20.fc1.bias', 'model.speech_tower.encoder.layers.4.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.1.final_layer_norm.bias', 'model.speech_tower.encoder.layers.20.fc1.weight', 'model.speech_tower.encoder.layers.23.fc2.bias', 'model.speech_tower.encoder.layers.7.final_layer_norm.bias', 'model.speech_tower.feature_extractor.conv_layers.2.0.bias', 'model.speech_tower.encoder.layers.3.final_layer_norm.bias', 'model.speech_tower.encoder.layers.4.fc1.weight', 'model.speech_tower.encoder.layers.2.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.8.fc1.bias', 'model.speech_tower.encoder.layers.8.fc2.weight', 'model.speech_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.speech_tower.feature_extractor.conv_layers.0.2.1.weight', 'model.speech_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.22.self_attn.k_proj.bias', 'model.speech_tower.encoder.pos_conv.0.bias', 'model.speech_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.speech_tower.feature_extractor.conv_layers.4.0.weight', 'model.speech_tower.encoder.layers.7.fc2.bias', 'model.speech_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.22.final_layer_norm.bias', 'model.speech_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.21.final_layer_norm.bias', 'model.speech_tower.feature_extractor.conv_layers.3.0.bias', 'model.speech_tower.encoder.layers.8.final_layer_norm.bias', 'model.speech_tower.feature_extractor.conv_layers.1.0.bias', 'model.speech_tower.encoder.layers.1.fc2.weight', 'model.speech_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.18.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.7.final_layer_norm.weight', 'model.speech_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.8.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.10.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.22.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.13.fc1.bias', 'model.speech_tower.encoder.layers.5.fc1.weight', 'model.speech_tower.encoder.layers.5.fc1.bias', 'model.speech_tower.encoder.layers.18.fc1.weight', 'model.speech_tower.feature_extractor.conv_layers.0.0.bias', 'model.speech_tower.encoder.layers.21.fc1.weight', 'model.speech_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.3.fc2.weight', 'model.speech_tower.encoder.layers.4.fc2.bias', 'model.speech_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.speech_tower.feature_extractor.conv_layers.5.0.bias', 'model.speech_tower.post_extract_proj.weight', 'model.speech_tower.encoder.layers.10.fc2.weight', 'model.speech_tower.encoder.layers.21.fc2.bias', 'model.speech_tower.encoder.layers.19.fc2.bias', 'model.speech_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.8.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.8.fc2.bias', 'model.speech_tower.encoder.layers.9.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.12.final_layer_norm.bias', 'model.speech_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.6.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.4.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.2.final_layer_norm.weight', 'model.speech_tower.encoder.layers.9.fc1.bias', 'model.speech_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.0.final_layer_norm.weight', 'model.speech_tower.feature_extractor.conv_layers.1.0.weight', 'model.speech_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.7.self_attn.k_proj.bias', 'model.speech_tower.layer_norm.bias', 'model.speech_tower.encoder.layers.11.final_layer_norm.bias', 'model.speech_tower.encoder.layers.5.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.19.fc1.bias', 'model.speech_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.13.fc2.bias', 'model.speech_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.speech_tower.feature_extractor.conv_layers.2.2.1.weight', 'model.speech_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.6.fc1.bias', 'model.speech_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.8.self_attn_layer_norm.weight', 'model.speech_tower.layer_norm.weight', 'model.speech_tower.encoder.layers.22.final_layer_norm.weight', 'model.speech_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.20.final_layer_norm.bias', 'model.speech_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.3.final_layer_norm.weight', 'model.speech_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.23.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.12.fc2.bias', 'model.speech_tower.encoder.layer_norm.bias', 'model.speech_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.mm_length_adapter.conv_layers.0.bias', 'model.speech_tower.encoder.layers.5.fc2.bias', 'model.speech_tower.encoder.layers.0.fc1.weight', 'model.speech_tower.encoder.layers.12.fc2.weight', 'model.speech_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.22.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.19.final_layer_norm.weight', 'model.speech_tower.encoder.pos_conv.0.weight_g', 'model.speech_tower.encoder.layers.12.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.2.fc1.weight', 'model.speech_tower.encoder.layers.14.fc2.bias', 'model.speech_tower.encoder.pos_conv.0.weight_v', 'model.speech_tower.encoder.layers.1.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.15.final_layer_norm.weight', 'model.speech_tower.encoder.layers.2.fc2.weight', 'model.speech_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.10.fc2.bias', 'model.speech_tower.feature_extractor.conv_layers.3.0.weight', 'model.speech_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.23.fc2.weight', 'model.speech_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.speech_tower.feature_extractor.conv_layers.3.2.1.weight', 'model.speech_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.0.fc2.weight', 'model.speech_tower.encoder.layers.1.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.12.fc1.weight', 'model.speech_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.5.final_layer_norm.bias', 'model.speech_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.14.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.6.fc2.weight', 'model.speech_tower.encoder.layers.22.fc1.weight', 'model.speech_tower.encoder.layers.11.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.18.fc1.bias', 'model.speech_tower.encoder.layers.20.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.speech_tower.mask_emb', 'model.speech_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.0.fc1.bias', 'model.speech_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.14.final_layer_norm.bias', 'model.speech_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.17.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.19.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.0.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.5.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.3.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.0.final_layer_norm.bias', 'model.speech_tower.encoder.layers.3.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.20.fc2.weight', 'model.speech_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.11.fc2.bias', 'model.speech_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.6.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.22.fc1.bias', 'model.speech_tower.encoder.layers.9.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.2.fc1.bias', 'model.speech_tower.encoder.layers.16.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.2.fc2.bias', 'model.speech_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.15.self_attn_layer_norm.weight', 'model.speech_tower.feature_extractor.conv_layers.4.0.bias', 'model.speech_tower.feature_extractor.conv_layers.2.0.weight', 'model.speech_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.5.fc2.weight', 'model.speech_tower.encoder.layers.6.fc2.bias', 'model.speech_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.13.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.18.fc2.weight', 'model.speech_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.3.fc2.bias', 'model.speech_tower.encoder.layers.20.final_layer_norm.weight', 'model.speech_tower.feature_extractor.conv_layers.0.0.weight', 'model.speech_tower.encoder.layers.1.fc2.bias', 'model.speech_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.16.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.23.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layer_norm.weight', 'model.speech_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.10.fc1.weight', 'model.speech_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.4.fc2.weight', 'model.speech_tower.encoder.layers.7.fc2.weight', 'model.speech_tower.encoder.layers.9.final_layer_norm.weight', 'model.speech_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.14.final_layer_norm.weight', 'model.speech_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.7.fc1.weight', 'model.speech_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.0.fc2.bias', 'model.speech_tower.feature_extractor.conv_layers.0.2.1.bias', 'model.speech_tower.encoder.layers.14.fc2.weight', 'model.speech_tower.feature_extractor.conv_layers.4.2.1.weight', 'model.speech_tower.encoder.layers.7.fc1.bias', 'model.speech_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.13.final_layer_norm.bias', 'model.speech_tower.encoder.layers.2.final_layer_norm.bias', 'model.speech_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.0.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.8.fc1.weight', 'model.speech_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.4.final_layer_norm.bias', 'model.speech_tower.encoder.layers.13.self_attn_layer_norm.bias', 'model.speech_tower.feature_extractor.conv_layers.6.2.1.bias', 'model.speech_tower.encoder.layers.9.fc2.bias', 'model.speech_tower.encoder.layers.10.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.20.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.18.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.16.fc1.weight', 'model.speech_tower.encoder.layers.21.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.16.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.19.final_layer_norm.bias', 'model.speech_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.17.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.speech_tower.feature_extractor.conv_layers.3.2.1.bias', 'model.speech_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.23.fc1.weight', 'model.speech_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.15.final_layer_norm.bias', 'model.speech_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.13.fc1.weight', 'model.speech_tower.encoder.layers.12.fc1.bias', 'model.speech_tower.encoder.layers.13.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.6.final_layer_norm.weight', 'model.speech_tower.encoder.layers.17.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.20.fc2.bias', 'model.speech_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.speech_tower.feature_extractor.conv_layers.4.2.1.bias', 'model.speech_tower.feature_extractor.conv_layers.6.0.bias', 'model.speech_tower.encoder.layers.18.final_layer_norm.bias', 'model.speech_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.11.fc1.weight', 'model.speech_tower.encoder.layers.7.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.16.fc1.bias', 'model.speech_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.5.final_layer_norm.weight', 'model.mm_length_adapter.conv_layers.0.weight', 'model.speech_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.3.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.14.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.speech_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.5.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.speech_tower.feature_extractor.conv_layers.2.2.1.bias', 'model.speech_tower.encoder.layers.12.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.15.fc1.weight', 'model.speech_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.9.self_attn_layer_norm.weight', 'model.speech_tower.encoder.layers.6.final_layer_norm.bias', 'model.speech_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.speech_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.speech_tower.feature_extractor.conv_layers.5.2.1.bias', 'model.speech_tower.post_extract_proj.bias', 'model.speech_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.19.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.speech_tower.encoder.layers.0.self_attn.k_proj.bias', 'model.speech_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.9.fc2.weight', 'model.speech_tower.encoder.layers.9.final_layer_norm.bias', 'model.speech_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.21.final_layer_norm.weight', 'model.speech_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.13.final_layer_norm.weight', 'model.speech_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.speech_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.speech_tower.encoder.layers.15.fc2.weight', 'model.speech_tower.encoder.layers.10.final_layer_norm.weight', 'model.speech_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.speech_tower.encoder.layers.21.self_attn_layer_norm.bias', 'model.speech_tower.encoder.layers.9.fc1.weight', 'model.speech_tower.encoder.layers.15.fc1.bias', 'model.speech_tower.encoder.layers.6.fc1.weight', 'model.speech_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.speech_tower.encoder.layers.13.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing SpeechLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpeechLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/taurus/home/siqiouyang/anaconda3/envs/sllama/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Model(\n",
       "  (feature_extractor): ConvFeatureExtractionModel(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Sequential(\n",
       "          (0): TransposeLast()\n",
       "          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): TransposeLast()\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (1-4): 4 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Sequential(\n",
       "          (0): TransposeLast()\n",
       "          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): TransposeLast()\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (5-6): 2 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Sequential(\n",
       "          (0): TransposeLast()\n",
       "          (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): TransposeLast()\n",
       "        )\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "  (quantizer): None\n",
       "  (project_q): None\n",
       "  (encoder): TransformerEncoder(\n",
       "    (pos_conv): Sequential(\n",
       "      (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (1): SamePad()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): FairseqDropout()\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_proj): None\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import argparse, sys, time, json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch, transformers\n",
    "from eval.utils import disable_torch_init\n",
    "from model.model import SpeechLlamaForCausalLM, SpeechLlamaModel, SpeechLlamaConfig\n",
    "from model.utils import KeywordsStoppingCriteria\n",
    "from fairseq.data.audio.speech_to_text_dataset import _collate_frames\n",
    "from train.dataset import PromptSpeechToTextDatasetCreator, SpeechToTextDatasetItem\n",
    "import conversation as conversation_lib\n",
    "from conversation import SeparatorStyle\n",
    "from train.uni_wav2vec_monkey_patch import replace_forward\n",
    "\n",
    "import requests\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.model_name = '/mnt/taurus/data/xixu/runs/sllama/en-es/7b/uni/stage2/checkpoint-2000'\n",
    "args.length_adapter_path = os.path.join(args.model_name, 'length_adapter.bin')\n",
    "args.mlp_adapter_path = os.path.join(args.model_name, 'mlp_adapter.bin')\n",
    "args.speech_tower_path = os.path.join(args.model_name, 'speech_tower.bin')\n",
    "\n",
    "load_type = torch.float16\n",
    "disable_torch_init()\n",
    "model_name = os.path.expanduser(args.model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.model_name,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "config = json.load(open(os.path.join(args.model_name, 'config.json')))\n",
    "config['large_model'] = True\n",
    "update_config = os.path.join(args.model_name, 'config_large.json')\n",
    "json.dump(config, open(update_config, 'w'), indent=2)\n",
    "# replace_llama_attn_with_flash_attn()\n",
    "model = SpeechLlamaForCausalLM.from_pretrained(args.model_name,\n",
    "                                                torch_dtype=load_type,\n",
    "                                                low_cpu_mem_usage=True,\n",
    "                                                device_map='auto',\n",
    "                                                config=update_config,).eval()\n",
    "if 'model.embed_tokens' in model.hf_device_map.keys():\n",
    "    device_input = 'cuda:' + str(model.hf_device_map['model.embed_tokens'])    \n",
    "    device_output = 'cuda:' + str(model.hf_device_map['lm_head'])    \n",
    "else:\n",
    "    device_input = 'cuda'  \n",
    "    device_input = 'cuda'\n",
    "length_after_ssl, length_after_adp = model.model.initialize_speech_modules(\n",
    "    speech_tower_path='/mnt/taurus/data/xixu/models/wav2_vec_vox_960h_pl.pt',\n",
    "    speech_tower_type=None,\n",
    "    len_adapter_channels=model.config.len_adapter_channels,\n",
    "    len_adapter_kernel_sizes=model.config.len_adapter_kernel_sizes,\n",
    "    ssl_fintuned=model.config.ssl_fintuned,\n",
    ")\n",
    "model.model.speech_tower.to(dtype=load_type, device=device_input)\n",
    "\n",
    "length_adapter_weights = torch.load(args.length_adapter_path, map_location='cpu')\n",
    "mlp_adapter_weights = torch.load(args.mlp_adapter_path, map_location='cpu')\n",
    "speech_tower_weights = torch.load(args.speech_tower_path, map_location='cpu')\n",
    "\n",
    "\n",
    "model.model.mm_length_adapter.load_state_dict(length_adapter_weights)\n",
    "model.model.mm_mlp_adapter.load_state_dict(mlp_adapter_weights)\n",
    "model.model.speech_tower.load_state_dict(speech_tower_weights)\n",
    "\n",
    "model.model.mm_length_adapter.to(dtype=load_type, device=device_input)\n",
    "model.model.mm_mlp_adapter.to(dtype=load_type, device=device_input)\n",
    "model.model.speech_tower.to(dtype=load_type, device=device_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_forward = super(SpeechLlamaModel, model.model).forward\n",
    "input_ids = torch.LongTensor([[0, 1, 2, 3, 4]]).to('cuda')\n",
    "llama_output = llama_forward(\n",
    "    input_ids=input_ids,\n",
    "    use_cache=True\n",
    ")\n",
    "llama_output_3 = [\n",
    "    [llama_output.past_key_values[i][j][:, :, :3, :] for j in range(2)]\n",
    "    for i in range(32) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_output_incremental = llama_forward(\n",
    "    input_ids=input_ids[:, 3:],\n",
    "    past_key_values=llama_output_3,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 3, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_output_3[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 5, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_output_incremental.past_key_values[0][0].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
