{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fairseq\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from fairseq.models import (\n",
    "    FairseqEncoder, \n",
    "    register_model, \n",
    "    register_model_architecture\n",
    ")\n",
    "from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Model, Wav2Vec2Config\n",
    "from fairseq.models import BaseFairseqModel, register_model\n",
    "from fairseq.models.wav2vec import (\n",
    "    TransformerEncoder,\n",
    "    TransformerSentenceEncoderLayer,\n",
    "    Wav2Vec2Model,\n",
    "    Wav2VecEncoder    \n",
    ")\n",
    "from fairseq.data.audio.speech_to_text_dataset import _collate_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_forward = TransformerSentenceEncoderLayer.forward\n",
    "\n",
    "# def generate_2d_causal_mask(seq_len, device='cpu'):\n",
    "#     \"\"\"\n",
    "#     Generates a 2D causal mask for multi-head attention.\n",
    "    \n",
    "#     Args:\n",
    "#         seq_len (int): The length of the sequence.\n",
    "#         device (str): The device on which to create the mask.\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: A 2D causal attention mask.\n",
    "#     \"\"\"\n",
    "#     mask = torch.triu(torch.ones((seq_len, seq_len), device=device), diagonal=1)\n",
    "#     mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "#     return mask\n",
    "\n",
    "# def causal_forward(\n",
    "#     self,\n",
    "#     x: torch.Tensor,\n",
    "#     self_attn_mask: torch.Tensor = None,\n",
    "#     self_attn_padding_mask: torch.Tensor = None,\n",
    "#     need_weights: bool = False,\n",
    "#     att_args=None,\n",
    "# ):\n",
    "#     # Generate the causal mask\n",
    "#     # print(x)\n",
    "#     # print(x.size(2))\n",
    "#     # print(self_attn_mask)\n",
    "#     causal_mask = generate_2d_causal_mask(x.size(0), device=x.device)\n",
    "    \n",
    "#     if self_attn_mask is not None:\n",
    "#         self_attn_mask = self_attn_mask + causal_mask\n",
    "#     else:\n",
    "#         self_attn_mask = causal_mask\n",
    "\n",
    "#     return original_forward(\n",
    "#         self, x, \n",
    "#         self_attn_mask=self_attn_mask, \n",
    "#         self_attn_padding_mask=self_attn_padding_mask, \n",
    "#         need_weights=need_weights,\n",
    "#         att_args=att_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_2d_causal_mask(seq_len, dtype, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generates a 2D causal mask for multi-head attention.\n",
    "    \n",
    "    Args:\n",
    "        seq_len (int): The length of the sequence.\n",
    "        device (str): The device on which to create the mask.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A 2D causal attention mask.\n",
    "    \"\"\"\n",
    "    # mask = torch.triu(torch.ones((seq_len, seq_len), device=device), diagonal=1)\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len), device=device, dtype=dtype), diagonal=1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask\n",
    "\n",
    "def causal_forward(\n",
    "    self,\n",
    "    x: torch.Tensor,\n",
    "    self_attn_mask: torch.Tensor = None,\n",
    "    self_attn_padding_mask: torch.Tensor = None,\n",
    "    need_weights: bool = False,\n",
    "    att_args=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    LayerNorm is applied either before or after the self-attention/ffn\n",
    "    modules similar to the original Transformer imlementation.\n",
    "    \"\"\"\n",
    "    # causal_mask = generate_2d_causal_mask(x.size(0), device=x.device)\n",
    "    causal_mask = generate_2d_causal_mask(x.size(0), dtype=x.dtype,device=x.device)\n",
    "    \n",
    "    if self_attn_mask is not None:\n",
    "        self_attn_mask = self_attn_mask + causal_mask\n",
    "    else:\n",
    "        self_attn_mask = causal_mask\n",
    "        \n",
    "    residual = x\n",
    "\n",
    "    if self.layer_norm_first:\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            attn_mask=self_attn_mask,\n",
    "            need_weights=True,\n",
    "        )\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "\n",
    "        residual = x\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        layer_result = x\n",
    "\n",
    "        x = self.dropout3(x)\n",
    "        x = residual + x\n",
    "    else:\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            need_weights=True,\n",
    "        )\n",
    "\n",
    "        x = self.dropout1(x)\n",
    "        x = residual + x\n",
    "\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        layer_result = x\n",
    "\n",
    "        x = self.dropout3(x)\n",
    "        x = residual + x\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "    return x, (attn, layer_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try replace multihead attention with causal multihead attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try replace wav2vec forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_forward():\n",
    "    TransformerSentenceEncoderLayer.forward = causal_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_tower_path = '/mnt/taurus/data/xixu/models/wav2_vec_vox_960h_pl.pt'\n",
    "state = fairseq.checkpoint_utils.load_checkpoint_to_cpu(speech_tower_path)\n",
    "model = Wav2VecEncoder(state['cfg']['model'], None)\n",
    "new = {}\n",
    "for key in state['model'].keys():\n",
    "    new_key = key.replace('w2v_encoder.', '')\n",
    "    if not new_key.startswith('proj'):\n",
    "        new[new_key] = state['model'][key]\n",
    "model.load_state_dict(new, strict=True)\n",
    "model = model.w2v_model\n",
    "replace_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention Weight\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio\n",
    "import numpy\n",
    "from train.dataset import PromptSpeechToTextDatasetCreator, SpeechToTextDatasetItem\n",
    "\n",
    "\n",
    "replace_forward()\n",
    "def visualize_attention_weights(model, plot_size=10):\n",
    "    test_dataset = PromptSpeechToTextDatasetCreator.from_tsv(\"/mnt/data/xixu/datasets/must-c-v1.0/en-es/\", 'tst-COMMON_1' )\n",
    "    for test_data in test_dataset:\n",
    "        source, ref, id = test_data.source, test_data.target, test_data.id                  \n",
    "        speech_batch = _collate_frames([source], is_audio_input=True)\n",
    "\n",
    "        model.eval()\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            result = model.extract_features(speech_batch, padding_mask=None)\n",
    "        # ((x, z, lr))\n",
    "        # changed https://github.com/facebookresearch/fairseq/blob/fad2c4d1ebe14d974876de52dcb06db6d99b0b4a/fairseq/models/wav2vec/wav2vec2.py#L1330C34-L1330C34 \n",
    "        # to get attention weights  \n",
    "        attn = result['layer_results'][0][1]\n",
    "        feature = result[\"x\"]\n",
    "        print(feature.size())\n",
    "        # print(attn.size())\n",
    "        attn = attn[0] if attn.ndim == 3 else attn\n",
    "        # print(attn.size())\n",
    "        # Select a smaller portion of the attention matrix to visualize\n",
    "        small_attn = attn[:plot_size, :plot_size].cpu().numpy()\n",
    "\n",
    "        # Visualize the attention weights\n",
    "        plt.matshow(small_attn)\n",
    "        plt.title(f\"Attention Weights - First {plot_size} Timesteps\")\n",
    "        plt.xlabel(\"Key Positions\")\n",
    "        plt.ylabel(\"Query Positions\")\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with the model and desired input length\n",
    "visualize_attention_weights(model, plot_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental w2v2 encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.modules import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiheadAttention(512, 8, dropout=0.0, self_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = generate_2d_causal_mask(x.size(0), dtype=x.dtype, device=x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn, attn_weights = mha.forward(x, x, x, incremental_state=incremental_state, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(3, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_mask = generate_2d_causal_mask(5, dtype=x.dtype, device=x.device)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn, attn_weights = mha.forward(y, y, y, incremental_state=incremental_state, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(incremental_state.values())[0]['prev_key'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental llama encoding with w2v2 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import argparse, time, json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch, transformers\n",
    "import torch.nn as nn\n",
    "from eval.utils import disable_torch_init\n",
    "from model.model import SpeechLlamaForCausalLM, SpeechLlamaModel, SpeechLlamaConfig\n",
    "from model.utils import KeywordsStoppingCriteria\n",
    "from fairseq.data.audio.speech_to_text_dataset import _collate_frames\n",
    "from train.dataset import PromptSpeechToTextDatasetCreator, SpeechToTextDatasetItem\n",
    "import conversation as conversation_lib\n",
    "from conversation import SeparatorStyle\n",
    "\n",
    "import requests\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "from fairseq.models.speech_to_text import lengths_to_padding_mask\n",
    "from train.uni_wav2vec_monkey_patch import replace_uni_train, replace_uni_decode, uni_self_attn_forward, uni_w2v2_extract_features\n",
    "\n",
    "transformers.set_seed(998244353)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.model_name = '/mnt/taurus/data/xixu/runs/sllama/en-es/7b/uni/stage2/checkpoint-2000'\n",
    "args.length_adapter_path = os.path.join(args.model_name, 'length_adapter.bin')\n",
    "args.mlp_adapter_path = os.path.join(args.model_name, 'mlp_adapter.bin')\n",
    "args.speech_tower_path = os.path.join(args.model_name, 'speech_tower.bin')\n",
    "\n",
    "load_type = torch.float32\n",
    "disable_torch_init()\n",
    "model_name = os.path.expanduser(args.model_name)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.model_name,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "config = json.load(open(os.path.join(args.model_name, 'config.json')))\n",
    "config['large_model'] = True\n",
    "update_config = os.path.join(args.model_name, 'config_large.json')\n",
    "json.dump(config, open(update_config, 'w'), indent=2)\n",
    "# replace_llama_attn_with_flash_attn()\n",
    "\n",
    "replace_uni_train()\n",
    "\n",
    "model = SpeechLlamaForCausalLM.from_pretrained(args.model_name,\n",
    "                                                torch_dtype=load_type,\n",
    "                                                low_cpu_mem_usage=True,\n",
    "                                                device_map='cpu',\n",
    "                                                config=update_config,).eval()\n",
    "\n",
    "device_input = device_output = 'cpu'\n",
    "\n",
    "length_after_ssl, length_after_adp = model.model.initialize_speech_modules(\n",
    "    speech_tower_path='/mnt/taurus/data/xixu/models/wav2_vec_vox_960h_pl.pt',\n",
    "    speech_tower_type=None,\n",
    "    len_adapter_channels=model.config.len_adapter_channels,\n",
    "    len_adapter_kernel_sizes=model.config.len_adapter_kernel_sizes,\n",
    "    ssl_fintuned=model.config.ssl_fintuned,\n",
    ")\n",
    "model.model.speech_tower.to(dtype=load_type, device=device_input)\n",
    "\n",
    "length_adapter_weights = torch.load(args.length_adapter_path, map_location='cpu')\n",
    "mlp_adapter_weights = torch.load(args.mlp_adapter_path, map_location='cpu')\n",
    "speech_tower_weights = torch.load(args.speech_tower_path, map_location='cpu')\n",
    "\n",
    "\n",
    "model.model.mm_length_adapter.load_state_dict(length_adapter_weights)\n",
    "model.model.mm_mlp_adapter.load_state_dict(mlp_adapter_weights)\n",
    "model.model.speech_tower.load_state_dict(speech_tower_weights)\n",
    "\n",
    "model.model.mm_length_adapter.to(dtype=load_type, device=device_input).eval()\n",
    "model.model.mm_mlp_adapter.to(dtype=load_type, device=device_input).eval()\n",
    "model.model.speech_tower.to(dtype=load_type, device=device_input).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siqiouya/anaconda3/envs/sllama/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-02 10:32:52,021] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simuleval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtt_waitk_sllama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S2TAgentStates\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtt_waitk_sllama_incremental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IncrementalS2TAgentStates\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairseq\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspeech_to_text\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lengths_to_padding_mask\n",
      "File \u001b[0;32m~/work/sllama/eval/agents/tt_waitk_sllama.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimuleval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentStates\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimuleval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entrypoint\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimuleval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msegments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpeechSegment\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simuleval'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/taurus/home/siqiouyang/work/projects/SimulEval/')\n",
    "from eval.agents.tt_waitk_sllama import S2TAgentStates\n",
    "from eval.agents.tt_waitk_sllama_incremental import IncrementalS2TAgentStates\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "DEFAULT_SPEECH_TOKEN = \"<speech>\"\n",
    "DEFAULT_SPEECH_PATCH_TOKEN = \"<sp_patch>\"\n",
    "DEFAULT_SPEECH_START_TOKEN = \"<sp_start>\"\n",
    "DEFAULT_SPEECH_END_TOKEN = \"<sp_end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.FloatTensor([1.1]).floor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.speech_tower.encoder.pos_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(states):\n",
    "    source = torch.tensor(states.source).to(\n",
    "        device=model.device, dtype=model.dtype\n",
    "    )\n",
    "    speech_batch = _collate_frames([source], is_audio_input=True)\n",
    "    n_frames = torch.tensor([source.size(0)], dtype=torch.long)\n",
    "    # source = F.layer_norm(source, source.size())\n",
    "    speech_lens = length_after_adp(length_after_ssl(n_frames))\n",
    "\n",
    "    to_adds = [int(speech_len)*DEFAULT_SPEECH_PATCH_TOKEN for speech_len in speech_lens]\n",
    "    to_adds = [DEFAULT_SPEECH_START_TOKEN + to_add + DEFAULT_SPEECH_END_TOKEN for to_add in to_adds]\n",
    "\n",
    "    conv = conversation_lib.default_conversation.copy()\n",
    "    conv.messages = []\n",
    "    conv.append_message(conv.roles[0], to_adds[0])\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt_inputs = conv.get_prompt()\n",
    "\n",
    "    inputs = tokenizer([prompt_inputs])\n",
    "    input_ids = inputs.input_ids[0] + states.target_ids\n",
    "    input_ids_tensor = torch.as_tensor([input_ids])\n",
    "    model.model.speech_features_extracted = False\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.model(\n",
    "            attention_mask=None, # input_ids_tensor.ne(tokenizer.pad_token_id),\n",
    "            input_ids=input_ids_tensor,\n",
    "            speech_batch=speech_batch,\n",
    "            src_lengths=n_frames.to(device=model.device),\n",
    "            after_lens=speech_lens.to(device=model.device),\n",
    "        )\n",
    "        # output = model.model.speech_tower.extract_features(speech_batch, None)\n",
    "        \n",
    "    return output\n",
    "\n",
    "states1 = S2TAgentStates([])\n",
    "states1.source_finished = False\n",
    "states1.source_sample_rate = 16000\n",
    "states1.source = np.random.rand(25600).tolist()\n",
    "\n",
    "states2 = S2TAgentStates([])\n",
    "states2.source_finished = False\n",
    "states2.source_sample_rate = 16000\n",
    "states2.source = states1.source + np.random.rand(5120).tolist()\n",
    "\n",
    "# states3 = S2TAgentStates([])\n",
    "# states3.source_finished = False\n",
    "# states3.source_sample_rate = 16000\n",
    "# states3.source = states2.source + np.random.rand(5120).tolist()\n",
    "\n",
    "o1 = process(states1)\n",
    "o2 = process(states2)\n",
    "# o3 = process(states3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_process(states):\n",
    "    source = torch.tensor(states.source).to(\n",
    "        device=model.device, dtype=model.dtype\n",
    "    )\n",
    "    speech_batch = _collate_frames([source], is_audio_input=True)\n",
    "    n_frames = torch.tensor([source.size(0)], dtype=torch.long)\n",
    "    speech_lens = length_after_adp(length_after_ssl(n_frames))\n",
    "\n",
    "    to_adds = [int(speech_len)*DEFAULT_SPEECH_PATCH_TOKEN for speech_len in speech_lens]\n",
    "    to_adds = [DEFAULT_SPEECH_START_TOKEN + to_add + DEFAULT_SPEECH_END_TOKEN for to_add in to_adds]\n",
    "\n",
    "    conv = conversation_lib.default_conversation.copy()\n",
    "    conv.messages = []\n",
    "    conv.append_message(conv.roles[0], to_adds[0])\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt_inputs = conv.get_prompt()\n",
    "\n",
    "    inputs = tokenizer([prompt_inputs])\n",
    "    input_ids = inputs.input_ids[0] + states.target_ids\n",
    "    input_ids_tensor = torch.as_tensor([input_ids])\n",
    "    model.model.speech_features_extracted = False\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        output = model.model(\n",
    "            attention_mask=input_ids_tensor.ne(tokenizer.pad_token_id),\n",
    "            input_ids=input_ids_tensor,\n",
    "            speech_batch=speech_batch,\n",
    "            src_lengths=n_frames.to(device=model.device),\n",
    "            after_lens=speech_lens.to(device=model.device),\n",
    "            states=states,\n",
    "            use_cache=True\n",
    "        )\n",
    "        # output = uni_w2v2_extract_features(\n",
    "        #     model.model.speech_tower,\n",
    "        #     speech_batch, \n",
    "        #     None,\n",
    "        #     past_key_values=states.w2v2_past_key_values,\n",
    "        #     past_features=states.w2v2_past_features,\n",
    "        # )\n",
    "        # states.w2v2_past_features = output[\"x\"]\n",
    "        \n",
    "    # states.num_frames_read = len(states.source)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_uni_decode()\n",
    "inc_states = IncrementalS2TAgentStates([], [], None, None, -1, 0)\n",
    "inc_states.source_sample_rate = 16000\n",
    "inc_states.source_finished = False\n",
    "inc_states.w2v2_past_key_values = [\n",
    "    {} for _ in range(model.model.speech_tower.cfg.encoder_layers)\n",
    "]\n",
    "inc_states.source = states1.source\n",
    "io1 = incremental_process(inc_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_states.source = states2.source\n",
    "io2 = incremental_process(inc_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check speech encoder trained using waco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-13 15:42:31,265] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "from model.model import SpeechEncoder, SpeechLlamaForCausalLM\n",
    "from train.dataset import PromptSpeechToTextDatasetCreator, SpeechSampler\n",
    "from train.stage0 import DataCollatorForSupervisedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type SpeechLlama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6440c5e60af54e1094ce3509342e9e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = SpeechLlamaForCausalLM.from_pretrained(\n",
    "    '/mnt/taurus/data/xixu/llm/llama-2-7b/hf',\n",
    "    low_cpu_mem_usage=True,\n",
    "    load_in_8bit=False,\n",
    "    device_map='cpu',\n",
    ")\n",
    "llm_embedding = copy.deepcopy(llm.model.embed_tokens)\n",
    "del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    '/mnt/taurus/data/xixu/llm/llama-2-7b/hf',\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/taurus/home/siqiouyang/anaconda3/envs/sllama_lightning/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n"
     ]
    }
   ],
   "source": [
    "w2v2_path = '/mnt/taurus/data/xixu/models/wav2_vec_vox_960h_pl.pt'\n",
    "uni_enc = SpeechEncoder(w2v2_path, True, 1024, '3,3', llm_embedding, True, 0.2, 1e-4, 25000)\n",
    "bi_enc = SpeechEncoder(w2v2_path, True, 1024, '3,3', llm_embedding, False, 0.2, 1e-4, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load('/mnt/taurus/data1/siqiouyang/runs/sllama/en-es/7b/uni/stage0/epoch=22-step=36000.ckpt', map_location='cpu')\n",
    "uni_enc.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load('/mnt/taurus/data1/siqiouyang/runs/sllama/en-es/7b/bi/stage0-bi/epoch=23-step=38000.ckpt', map_location='cpu')\n",
    "bi_enc.load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeechEncoder(\n",
       "  (speech_tower): Wav2Vec2Model(\n",
       "    (feature_extractor): ConvFeatureExtractionModel(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Sequential(\n",
       "            (0): TransposeLast()\n",
       "            (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): TransposeLast()\n",
       "          )\n",
       "          (3): GELU(approximate='none')\n",
       "        )\n",
       "        (1-4): 4 x Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Sequential(\n",
       "            (0): TransposeLast()\n",
       "            (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): TransposeLast()\n",
       "          )\n",
       "          (3): GELU(approximate='none')\n",
       "        )\n",
       "        (5-6): 2 x Sequential(\n",
       "          (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Sequential(\n",
       "            (0): TransposeLast()\n",
       "            (1): Fp32LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): TransposeLast()\n",
       "          )\n",
       "          (3): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_extract_proj): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "    (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "    (quantizer): None\n",
       "    (project_q): None\n",
       "    (encoder): TransformerEncoder(\n",
       "      (pos_conv): Sequential(\n",
       "        (0): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "        (1): SamePad()\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x TransformerSentenceEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (final_proj): None\n",
       "  )\n",
       "  (mm_length_adapter): Conv1dSubsampler(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (1): Conv1d(512, 2048, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    )\n",
       "  )\n",
       "  (mm_mlp_adapter): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (llm_embedding): Embedding(32000, 4096)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_enc.to(device)\n",
    "bi_enc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PromptSpeechToTextDatasetCreator.from_tsv('/mnt/taurus/data/xixu/datasets/must-c-v1.0/en-es', 'dev_mfa')   \n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer, uni_enc.length_after_ssl, uni_enc.length_after_adp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sim(enc, batch, only_left=False, only_right=False):\n",
    "    src_text = batch[\"src_text\"].to(device)\n",
    "    src_speech = batch[\"src_speech\"].to(device)\n",
    "    src_speech_lengths = batch[\"src_speech_lengths\"]\n",
    "    after_speech_lengths = batch[\"after_speech_lengths\"]\n",
    "    text_word = batch[\"text_word\"]\n",
    "    speech_word = batch[\"speech_word\"]\n",
    "\n",
    "    src_text_emb = enc.llm_embedding(src_text).float()\n",
    "    src_speech_emb = enc.get_ssl_feature_w2v(src_speech, src_speech_lengths, after_speech_lengths).transpose(0, 1).float()\n",
    "\n",
    "    speech_word_emb = []\n",
    "    text_word_emb = []\n",
    "    for i in range(len(text_word)):\n",
    "        s_word, t_word = speech_word[i], text_word[i]\n",
    "        if s_word is not None:\n",
    "            for j in range(s_word.size(0)):\n",
    "                s_l, s_r = s_word[j]\n",
    "                t_l, t_r = t_word[j]\n",
    "                if only_left:\n",
    "                    s_word_emb = src_speech_emb[i][s_l: (s_l + s_r) // 2 + 1].mean(dim=0)\n",
    "                elif only_right:\n",
    "                    s_word_emb = src_speech_emb[i][(s_l + s_r) // 2: s_r + 1].mean(dim=0)\n",
    "                else:\n",
    "                    s_word_emb = src_speech_emb[i][s_l : s_r + 1].mean(dim=0)\n",
    "                t_word_emb = src_text_emb[i][t_l : t_r + 1].mean(dim=0)\n",
    "                speech_word_emb.append(s_word_emb)\n",
    "                text_word_emb.append(t_word_emb)\n",
    "    speech_word_emb = torch.stack(speech_word_emb, dim=0)\n",
    "    text_word_emb = torch.stack(text_word_emb, dim=0)\n",
    "\n",
    "    st_sim = F.cosine_similarity(\n",
    "        speech_word_emb, \n",
    "        text_word_emb, \n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    return st_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d1e8385b764fa08c3f6cbd69a03bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uni_st_sims = []\n",
    "uni_st_sims_left = []\n",
    "uni_st_sims_right = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(dataloader):\n",
    "        if batch[\"speech_word\"][0] is not None and batch[\"src_speech_lengths\"] >= 5120:\n",
    "            uni_st_sims.append(compute_sim(uni_enc, batch))\n",
    "            uni_st_sims_left.append(compute_sim(uni_enc, batch, only_left=True))\n",
    "            uni_st_sims_right.append(compute_sim(uni_enc, batch, only_right=True))\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1fda191ade484893c0a0909bbfe256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bi_st_sims = []\n",
    "bi_st_sims_left = []\n",
    "bi_st_sims_right = []\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(dataloader):\n",
    "        if batch[\"speech_word\"][0] is not None and batch[\"src_speech_lengths\"] >= 5120:\n",
    "            bi_st_sims.append(compute_sim(bi_enc, batch))\n",
    "            bi_st_sims_left.append(compute_sim(bi_enc, batch, only_left=True))\n",
    "            bi_st_sims_right.append(compute_sim(bi_enc, batch, only_right=True))\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_st_sims = torch.cat(uni_st_sims, dim=0)\n",
    "uni_st_sims_left = torch.cat(uni_st_sims_left, dim=0)\n",
    "uni_st_sims_right = torch.cat(uni_st_sims_right, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_st_sims = torch.cat(bi_st_sims, dim=0)\n",
    "bi_st_sims_left = torch.cat(bi_st_sims_left, dim=0)\n",
    "bi_st_sims_right = torch.cat(bi_st_sims_right, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6815, device='cuda:0'), tensor(0.7011, device='cuda:0'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_st_sims.mean(), bi_st_sims.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6180, device='cuda:0'), tensor(0.4562, device='cuda:0'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_st_sims_left.mean(), bi_st_sims_left.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6327, device='cuda:0'), tensor(0.7230, device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_st_sims_right.mean(), bi_st_sims_right.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
