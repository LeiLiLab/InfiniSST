#!/usr/bin/env bash


#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=64GB
#SBATCH --gpus=1
##SBATCH --constraint=xeon-4116 
#SBATCH --partition=taurus
#SBATCH --time=3-2:34:56 
##SBATCH --dependency=afterok:job_id
##SBATCH --array=1-3 
#SBATCH --account=xixu
#SBATCH --mail-type=BEGIN,END,FAIL,ALL
#SBATCH --mail-user=xixu@cs.cmu.edu 
#SBATCH --output=results/v3_evl.txt

cd eval
lang=de
model_path=/mnt/taurus/data1/xixu/runs/sllama/wavlm_clean/stage2_v3/checkpoint-2000
data_path=/mnt/data1/xixu/MUSTC_v3
result=${model_path}/v3_evl

# python ../train/zero_to_fp32.py ${model_path} \
#     ${model_path}/pytorch_model.bin

# python ../train/extract_adapter.py \
#   --model_name_or_path ${model_path} \
#   --extracted_name 'mm_length_adapter' \
#   --output ${model_path}/length_adapter.bin 

# python ../train/extract_adapter.py \
#   --model_name_or_path ${model_pasth} \
#   --extracted_name 'mm_mlp_adapter' \
#   --output ${model_path}/mlp_adapter.bin 

# python ../train/extract_adapter.py \
#     --model_name_or_path ${model_path} \
#     --extracted_name 'speech_tower' \
#     --output ${model_path}/speech_tower.bin

export PYTHONPATH=/mnt/taurus/home/xixu/sllama_new

python ./test_dataset_large.py  --model-name ${model_path} --data-path ${data_path} --data-split 'tst-COMMON' --result ${result} --beam 4 \
        --mlp-adapter-path ${model_path}/mlp_adapter.bin --length-adapter-path ${model_path}/length_adapter.bin \
        --speech-tower-type 'wavlm' --speech-tower-path ${model_path}/speech_tower.bin 

python ./compute_bleu.py ${result}/tst-COMMON