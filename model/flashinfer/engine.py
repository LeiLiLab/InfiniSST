import queue
from typing import List
from collections import Counter
import numpy as np
import torch
import flashinfer

from model.llama31 import SpeechLlamaModel

PAGE_SIZE = 16

class PageTable:
    def __init__(self, max_batch_size, max_steps, layer, q_heads, kv_heads, kv_dim, device, dtype=torch.bfloat16, wrapper_type='prefill'):
        self.max_steps = max_steps
        # ğŸ”¥ ç´§æ€¥ä¿®å¤ï¼šå¢åŠ é¡µé¢æ± å¤§å°ï¼Œæ·»åŠ æ›´å¤šç¼“å†²
        # 4 * max_batch_size
        page_multiplier = 4  # å¢åŠ é¡µé¢æ± å€æ•°
        max_num_pages = page_multiplier * max_batch_size * (max_steps + PAGE_SIZE - 1) // PAGE_SIZE
        
        # ğŸ” è®°å½•é¡µé¢æ± å¤§å°
        print(f"ğŸ”§ [PageTable-{wrapper_type}] åˆå§‹åŒ–é¡µé¢æ± :")
        print(f"   - max_batch_size: {max_batch_size}")
        print(f"   - max_steps: {max_steps}")
        print(f"   - page_multiplier: {page_multiplier}")
        print(f"   - æ€»é¡µé¢æ•°: {max_num_pages}")
        print(f"   - é¡µé¢å¤§å°: {PAGE_SIZE}")
        print(f"   - ç†è®ºæœ€å¤§å¹¶å‘æ•°: {max_num_pages // 6} sessions (å‡è®¾æ¯sessionéœ€è¦6é¡µ)")

        self.paged_kv_cache = torch.zeros(
            layer, max_num_pages, 2, PAGE_SIZE, kv_heads, kv_dim, 
            dtype=dtype, device=device
        ) # NHD
        self.paged_queue = list(range(max_num_pages))
        self.page_cnt = torch.zeros(max_num_pages, dtype=torch.int32)
        self.workspace_buffer = torch.empty(256 * 1024 * 1024, dtype=torch.uint8, device=device)
        
        # ğŸ” æ·»åŠ é¡µé¢æ± ç›‘æ§å˜é‡
        self.initial_pages = max_num_pages
        self.peak_usage = 0
        self.allocation_count = 0
        
        # ğŸ”¥ æ–°å¢ï¼šsessionç±»å‹è¿½è¸ª
        self.page_session_map = {}  # {page_id: session_info}
        self.session_pages = {}     # {session_id: [page_ids]}
        self.last_access_time = {}  # {session_id: timestamp}

        self.wrapper_type = wrapper_type
        if wrapper_type == 'prefill':
            self.wrapper = flashinfer.BatchPrefillWithPagedKVCacheWrapper(
                self.workspace_buffer, "NHD"
            )
            self.wrapper.plan(
                torch.tensor([0, 1], dtype=torch.int32, device=device),
                torch.tensor([0, 1], dtype=torch.int32, device=device),
                torch.tensor([0], dtype=torch.int32, device=device),
                torch.tensor([16], dtype=torch.int32, device=device),
                q_heads,
                kv_heads,
                kv_dim,
                PAGE_SIZE,
                causal=True,
                pos_encoding_mode='ROPE_LLAMA',
                rope_scale=1.0,
                rope_theta=1e4,
                q_data_type=dtype,
                kv_data_type=dtype,
            )
        else:
            self.wrapper = flashinfer.BatchDecodeWithPagedKVCacheWrapper(
                self.workspace_buffer, "NHD", use_tensor_cores=True
            )
            self.wrapper.plan(
                torch.tensor([0, 1], dtype=torch.int32, device=device),
                torch.tensor([0], dtype=torch.int32, device=device),
                torch.tensor([16], dtype=torch.int32, device=device),
                q_heads,
                kv_heads,
                kv_dim,
                PAGE_SIZE,
                pos_encoding_mode='ROPE_LLAMA',
                q_data_type=dtype,
                kv_data_type=dtype,
            )
    
    def _emergency_page_reclaim(self, needed_pages: int) -> int:
        """ç´§æ€¥é¡µé¢å›æ”¶ï¼šä»ä¸æ´»è·ƒçš„sessionä¸­å›æ”¶é¡µé¢"""
        import time
        current_time = time.time()
        freed_pages = 0
        
        print(f"ğŸš¨ [EMERGENCY] å¼€å§‹ç´§æ€¥é¡µé¢å›æ”¶ï¼Œéœ€è¦ {needed_pages} é¡µ")
        
        # è·å–æ‰€æœ‰sessionçš„æœ€åè®¿é—®æ—¶é—´ï¼ŒæŒ‰ä¸æ´»è·ƒç¨‹åº¦æ’åº
        inactive_sessions = []
        for session_id, last_access in self.last_access_time.items():
            inactive_time = current_time - last_access
            if session_id in self.session_pages:
                page_count = len(self.session_pages[session_id])
                inactive_sessions.append((session_id, inactive_time, page_count))
        
        # æŒ‰ä¸æ´»è·ƒæ—¶é—´æ’åºï¼ˆæœ€ä¸æ´»è·ƒçš„ä¼˜å…ˆå›æ”¶ï¼‰
        inactive_sessions.sort(key=lambda x: x[1], reverse=True)
        
        print(f"ğŸ” [EMERGENCY] æ‰¾åˆ° {len(inactive_sessions)} ä¸ªsessionå¯ä¾›å›æ”¶")
        
        # ä¼˜å…ˆå›æ”¶5åˆ†é’Ÿä»¥ä¸Šä¸æ´»è·ƒçš„session
        for session_id, inactive_time, page_count in inactive_sessions:
            if freed_pages >= needed_pages:
                break
                
            # åªå›æ”¶è¶…è¿‡5åˆ†é’Ÿä¸æ´»è·ƒçš„session
            if inactive_time > 300:  # 5åˆ†é’Ÿ
                print(f"ğŸ”„ [EMERGENCY] å›æ”¶session {session_id} ({page_count} é¡µï¼Œä¸æ´»è·ƒ {inactive_time:.1f}s)")
                
                # é‡Šæ”¾è¯¥sessionçš„æ‰€æœ‰é¡µé¢
                if session_id in self.session_pages:
                    pages_to_free = self.session_pages[session_id].copy()
                    
                    # å‡å°‘é¡µé¢å¼•ç”¨è®¡æ•°
                    for page_id in pages_to_free:
                        if self.page_cnt[page_id] > 0:
                            self.page_cnt[page_id] -= 1
                            
                            # å¦‚æœå¼•ç”¨è®¡æ•°ä¸º0ï¼Œæ”¾å›é¡µé¢æ± 
                            if self.page_cnt[page_id] == 0:
                                self.paged_queue.append(page_id)
                                freed_pages += 1
                                
                                # æ¸…ç†æ˜ å°„
                                if page_id in self.page_session_map:
                                    del self.page_session_map[page_id]
                    
                    # æ¸…ç†sessionè®°å½•
                    del self.session_pages[session_id]
                    if session_id in self.last_access_time:
                        del self.last_access_time[session_id]
                    
                    print(f"âœ… [EMERGENCY] Session {session_id} é‡Šæ”¾äº† {len(pages_to_free)} é¡µ")
        
        # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œè€ƒè™‘å›æ”¶è¾ƒæ–°çš„sessionï¼ˆä½†ç»™å‡ºè­¦å‘Šï¼‰
        if freed_pages < needed_pages:
            remaining_needed = needed_pages - freed_pages
            print(f"âš ï¸ [EMERGENCY] ä»éœ€ {remaining_needed} é¡µï¼Œè€ƒè™‘å›æ”¶è¾ƒæ–°session")
            
            for session_id, inactive_time, page_count in inactive_sessions:
                if freed_pages >= needed_pages:
                    break
                    
                # å›æ”¶1åˆ†é’Ÿä»¥ä¸Šä¸æ´»è·ƒçš„session
                if inactive_time > 60 and session_id in self.session_pages:  # 1åˆ†é’Ÿ
                    print(f"ğŸ”„ [EMERGENCY] å¼ºåˆ¶å›æ”¶session {session_id} ({page_count} é¡µï¼Œä¸æ´»è·ƒ {inactive_time:.1f}s)")
                    
                    pages_to_free = self.session_pages[session_id].copy()
                    for page_id in pages_to_free:
                        if self.page_cnt[page_id] > 0:
                            self.page_cnt[page_id] -= 1
                            if self.page_cnt[page_id] == 0:
                                self.paged_queue.append(page_id)
                                freed_pages += 1
                                if page_id in self.page_session_map:
                                    del self.page_session_map[page_id]
                    
                    del self.session_pages[session_id]
                    if session_id in self.last_access_time:
                        del self.last_access_time[session_id]
        
        print(f"ğŸ [EMERGENCY] ç´§æ€¥å›æ”¶å®Œæˆï¼Œé‡Šæ”¾äº† {freed_pages} é¡µ")
        return freed_pages
    
    def track_session_page_usage(self, session_id: str, allocated_pages: list):
        """è¿½è¸ªsessionçš„é¡µé¢ä½¿ç”¨"""
        import time
        
        if session_id not in self.session_pages:
            self.session_pages[session_id] = []
        
        # æ·»åŠ æ–°åˆ†é…çš„é¡µé¢
        self.session_pages[session_id].extend(allocated_pages)
        
        # æ›´æ–°é¡µé¢åˆ°sessionçš„æ˜ å°„
        for page_id in allocated_pages:
            self.page_session_map[page_id] = {
                'session_id': session_id,
                'allocated_time': time.time()
            }
        
        # æ›´æ–°æœ€åè®¿é—®æ—¶é—´
        self.last_access_time[session_id] = time.time()
        
        print(f"ğŸ“ˆ [TRACKING] Session {session_id} ç°åœ¨ä½¿ç”¨ {len(self.session_pages[session_id])} é¡µ")
    
    def update_session_access_time(self, session_id: str):
        """æ›´æ–°sessionçš„æœ€åè®¿é—®æ—¶é—´"""
        import time
        self.last_access_time[session_id] = time.time()

class LLMCache:
    paged_kv_indices: torch.Tensor = None
    paged_kv_last_page_len: int = None

    def __init__(self):
        self.paged_kv_indices = []
        self.paged_kv_last_page_len = PAGE_SIZE

class SpeechCache:
    src: torch.Tensor = None
    src_len: int = 0

    paged_kv_indices: torch.Tensor = None
    paged_kv_last_page_len: int = None

    def __init__(self, src=None, src_len=0):
        self.src = src
        self.src_len = src_len

        self.paged_kv_indices = []
        self.paged_kv_last_page_len = PAGE_SIZE

def get_cache_size(paged_kv_indices, paged_kv_last_page_len):
    return (len(paged_kv_indices) - 1) * PAGE_SIZE + paged_kv_last_page_len

def init_paged_kv_cache(
    max_batch_size, 
    max_speech_steps, speech_layer, speech_kv_heads, speech_kv_dim, 
    max_llm_steps, llm_layer, llm_q_heads, llm_kv_heads, llm_kv_dim, 
    dtype=torch.bfloat16, device_prefill='cuda:0', device_decode='cuda:1'
):
    # speech prefill
    speech_pagetable = PageTable(
        max_batch_size, max_speech_steps, speech_layer, speech_kv_heads, speech_kv_heads, speech_kv_dim, 
        device_prefill, dtype=dtype, wrapper_type='prefill'
    )

    # llm prefill
    llm_prefill_pagetable = PageTable(
        max_batch_size, max_llm_steps, llm_layer, llm_q_heads, llm_kv_heads, llm_kv_dim, 
        device_prefill, dtype=dtype, wrapper_type='prefill'
    )

    # llm decode
    llm_decode_pagetable = PageTable(
        max_batch_size, max_llm_steps, llm_layer, llm_q_heads, llm_kv_heads, llm_kv_dim, 
        device_decode, dtype=dtype, wrapper_type='decode'
    )

    if device_prefill == device_decode:
        llm_decode_pagetable.paged_queue = llm_prefill_pagetable.paged_queue
        llm_decode_pagetable.paged_kv_cache = llm_prefill_pagetable.paged_kv_cache
        llm_decode_pagetable.page_cnt = llm_prefill_pagetable.page_cnt

    return speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable

def allocate_paged_kv_cache(
    pagetable,
    paged_kv_indices,
    paged_kv_last_page_len,
    n,  # næ˜¯éœ€è¦çš„tokenæ•°é‡
    session_priority='normal',  # ğŸ”¥ æ–°å¢ï¼šsessionä¼˜å…ˆçº§
    is_existing_session=True,   # ğŸ”¥ æ–°å¢ï¼šæ˜¯å¦ä¸ºå·²æœ‰session
    session_id=None,            # ğŸ”¥ æ–°å¢ï¼šsessionæ ‡è¯†ç¬¦
):
    # ğŸ” é¡µé¢åˆ†é…å‰çš„çŠ¶æ€æ£€æŸ¥
    available_pages = len(pagetable.paged_queue)
    used_pages = pagetable.initial_pages - available_pages
    pagetable.allocation_count += 1
    
    print(f"ğŸ” [MEMORY] é¡µé¢åˆ†é…è¯·æ±‚ #{pagetable.allocation_count}:")
    print(f"   - éœ€è¦tokenæ•°: {n}")
    print(f"   - å½“å‰é¡µé¢æ•°: {len(paged_kv_indices)}")
    print(f"   - æœ€åé¡µå‰©ä½™: {PAGE_SIZE - paged_kv_last_page_len} slots")
    print(f"   - å¯ç”¨é¡µé¢æ± : {available_pages} é¡µ")
    print(f"   - å·²ä½¿ç”¨é¡µé¢: {used_pages} é¡µ")
    print(f"   - æ€»é¡µé¢æ•°: {pagetable.initial_pages}")
    print(f"   - ä½¿ç”¨ç‡: {100*used_pages/pagetable.initial_pages:.1f}%")
    print(f"   - Sessionç±»å‹: {'å·²æœ‰' if is_existing_session else 'æ–°å»º'}")
    print(f"   - ä¼˜å…ˆçº§: {session_priority}")
    print(f"   - Session ID: {session_id or 'Unknown'}")
    
    # ğŸ”¥ æ›´æ–°sessionè®¿é—®æ—¶é—´
    if session_id and hasattr(pagetable, 'update_session_access_time'):
        pagetable.update_session_access_time(session_id)
    
    # è®¡ç®—æ˜¯å¦éœ€è¦æ–°é¡µé¢
    if paged_kv_last_page_len + n <= PAGE_SIZE:
        # å½“å‰é¡µé¢è¶³å¤Ÿ
        paged_kv_last_page_len += n
        print(f"âœ… [MEMORY] å½“å‰é¡µè¶³å¤Ÿï¼Œæ›´æ–°æœ€åé¡µé•¿åº¦: {paged_kv_last_page_len}")
    else:
        # éœ€è¦æ–°é¡µé¢
        num_new_pages = (n - (PAGE_SIZE - paged_kv_last_page_len) + PAGE_SIZE - 1) // PAGE_SIZE
        print(f"ğŸ“ˆ [MEMORY] éœ€è¦æ–°é¡µé¢: {num_new_pages} é¡µ")
        
        # ğŸ”¥ æ™ºèƒ½é¡µé¢ä¸è¶³å¤„ç†ç­–ç•¥
        if available_pages < num_new_pages:
            # è®¡ç®—é¡µé¢ä½¿ç”¨ç‡
            usage_rate = used_pages / pagetable.initial_pages
            
            print(f"âŒ [MEMORY] é¡µé¢æ± ä¸è¶³ï¼šéœ€è¦ {num_new_pages} é¡µï¼Œä½†åªæœ‰ {available_pages} é¡µå¯ç”¨")
            print(f"ğŸ” [MEMORY] é¡µé¢ä½¿ç”¨ç»Ÿè®¡:")
            print(f"   - ä½¿ç”¨ç‡: {usage_rate:.1%}")
            
            # ğŸ”¥ ç­–ç•¥1ï¼šå·²æœ‰sessionä¼˜å…ˆä¿æŠ¤
            if is_existing_session and usage_rate > 0.9:  # 90%ä»¥ä¸Šä½¿ç”¨ç‡
                print(f"ğŸ›¡ï¸ [MEMORY] å·²æœ‰sessionä¼˜å…ˆä¿æŠ¤ç­–ç•¥å¯ç”¨")
                
                # å°è¯•ç´§æ€¥å›æ”¶é¡µé¢
                emergency_freed = pagetable._emergency_page_reclaim(num_new_pages)
                if emergency_freed >= num_new_pages:
                    print(f"âœ… [MEMORY] ç´§æ€¥å›æ”¶æˆåŠŸï¼Œé‡Šæ”¾äº† {emergency_freed} é¡µ")
                    available_pages = len(pagetable.paged_queue)
                else:
                    print(f"âš ï¸ [MEMORY] ç´§æ€¥å›æ”¶ä¸è¶³ï¼Œå·²æœ‰sessionè¯·æ±‚å°†è¢«å»¶è¿Ÿå¤„ç†")
                    raise RuntimeError(f"GPUå†…å­˜é¡µé¢æ± è€—å°½ï¼šå·²æœ‰sessionéœ€è¦ {num_new_pages} é¡µä½†åªèƒ½å›æ”¶ {emergency_freed} é¡µ")
            
            # ğŸ”¥ ç­–ç•¥2ï¼šæ–°sessioné™çº§å¤„ç†
            elif not is_existing_session:
                print(f"ğŸ”„ [MEMORY] æ–°sessioné™çº§ç­–ç•¥ï¼šå»¶è¿Ÿåˆ›å»º")
                raise RuntimeError(f"GPUå†…å­˜é¡µé¢æ± è€—å°½ï¼šæ–°sessionåˆ›å»ºè¢«é˜»æ­¢ä»¥ä¿æŠ¤å·²æœ‰sessionï¼ˆéœ€è¦ {num_new_pages} é¡µä½†åªæœ‰ {available_pages} é¡µï¼‰")
            
            # ğŸ”¥ ç­–ç•¥3ï¼šç³»ç»Ÿè¿‡è½½ä¿æŠ¤
            else:
                print(f"ğŸš¨ [MEMORY] ç³»ç»Ÿå†…å­˜ä¸¥é‡ä¸è¶³")
                if available_pages == 0:
                    print(f"âŒ [MEMORY] é¡µé¢æ± å®Œå…¨è€—å°½ï¼Œæ— æ³•åˆ†é…å†…å­˜")
                
                # æ˜¾ç¤ºå¼•ç”¨è®¡æ•°åˆ†å¸ƒ
                unique_counts, count_frequencies = torch.unique(pagetable.page_cnt, return_counts=True)
                print(f"ğŸ“Š [MEMORY] é¡µé¢å¼•ç”¨è®¡æ•°åˆ†å¸ƒ:")
                for count, freq in zip(unique_counts.cpu().numpy(), count_frequencies.cpu().numpy()):
                    print(f"   - å¼•ç”¨è®¡æ•° {count}: {freq} é¡µ")
                
                raise RuntimeError(f"GPUå†…å­˜é¡µé¢æ± è€—å°½ï¼šéœ€è¦ {num_new_pages} é¡µä½†æ— å¯ç”¨é¡µé¢")
        
        # åˆ†é…é¡µé¢
        allocated_indices = []
        for _ in range(num_new_pages):
            if len(pagetable.paged_queue) == 0:
                raise RuntimeError("é¡µé¢åˆ†é…è¿‡ç¨‹ä¸­é˜Ÿåˆ—æ„å¤–ä¸ºç©º")
            
            page_idx = pagetable.paged_queue.pop(0)
            allocated_indices.append(page_idx)
            pagetable.page_cnt[page_idx] += 1
        
        paged_kv_indices.extend(allocated_indices)
        
        # ğŸ”¥ æ–°å¢ï¼šè¿½è¸ªsessioné¡µé¢ä½¿ç”¨
        if session_id and hasattr(pagetable, 'track_session_page_usage'):
            pagetable.track_session_page_usage(session_id, allocated_indices)
        
        # è®¡ç®—æ–°çš„æœ€åé¡µé•¿åº¦
        paged_kv_last_page_len = (n - (PAGE_SIZE - paged_kv_last_page_len) - 1) % PAGE_SIZE + 1
        
        # æ›´æ–°å³°å€¼ä½¿ç”¨ç»Ÿè®¡
        current_usage = pagetable.initial_pages - len(pagetable.paged_queue)
        if current_usage > pagetable.peak_usage:
            pagetable.peak_usage = current_usage
        
        print(f"âœ… [MEMORY] é¡µé¢åˆ†é…æˆåŠŸ:")
        print(f"   - åˆ†é…çš„é¡µé¢: {allocated_indices}")
        print(f"   - æ–°çš„æœ€åé¡µé•¿åº¦: {paged_kv_last_page_len}")
        print(f"   - å½“å‰ä½¿ç”¨: {current_usage}/{pagetable.initial_pages} é¡µ")
        print(f"   - å³°å€¼ä½¿ç”¨: {pagetable.peak_usage} é¡µ")
        print(f"   - å‰©ä½™å¯ç”¨: {len(pagetable.paged_queue)} é¡µ")
    
    return pagetable, paged_kv_indices, paged_kv_last_page_len

def pop_paged_kv_cache(
    pagetable,
    paged_kv_indices,
    paged_kv_last_page_len,
    max_steps, # preserve kv cache for last max_steps of tokens
    max_steps_start=0, # preserve kv cache for first max_steps_start tokens
):
    kv_cache_size = get_cache_size(paged_kv_indices, paged_kv_last_page_len)
    kv_cache_size_start = (max_steps_start + PAGE_SIZE - 1) // PAGE_SIZE * PAGE_SIZE

    if kv_cache_size - kv_cache_size_start > max_steps:
        num_pages_to_pop = (kv_cache_size - kv_cache_size_start - max_steps + PAGE_SIZE - 1) // PAGE_SIZE
        num_pages_start = kv_cache_size_start // PAGE_SIZE

        # Convert page indices to tensor for faster operations
        page_indices_to_pop = torch.tensor(paged_kv_indices[num_pages_start : num_pages_start + num_pages_to_pop])
        
        # Batch update page counts
        pagetable.page_cnt[page_indices_to_pop] -= 1
        
        # Get indices of pages that can be freed
        free_mask = pagetable.page_cnt[page_indices_to_pop] == 0
        free_indices = page_indices_to_pop[free_mask]
        
        # Update paged queue and indices in one operation
        if len(free_indices) > 0:
            # Convert to list only once for queue update
            free_indices_list = free_indices.tolist()
            pagetable.paged_queue.extend(free_indices_list)
            
            # Update paged_kv_indices in one operation
            paged_kv_indices = paged_kv_indices[:num_pages_start] + paged_kv_indices[num_pages_start + num_pages_to_pop:]

    return pagetable, paged_kv_indices, paged_kv_last_page_len


def copy_paged_kv_cache(
    src_paged_kv_indices,
    src_paged_kv_last_page_len,
    src_pagetable,
    tgt_pagetable,
):
    src_device = src_pagetable.paged_kv_cache.device
    tgt_device = tgt_pagetable.paged_kv_cache.device

    src_size = get_cache_size(src_paged_kv_indices, src_paged_kv_last_page_len)

    # layer, max_num_pages, 2, PAGE_SIZE, kv_heads, kv_dim, 
    tgt_paged_kv_indices = []
    tgt_paged_kv_last_page_len = 16
    tgt_pagetable, tgt_paged_kv_indices, tgt_paged_kv_last_page_len = \
        allocate_paged_kv_cache(
            tgt_pagetable,
            tgt_paged_kv_indices,
            tgt_paged_kv_last_page_len,
            src_size
        )

    tgt_pagetable.paged_kv_cache[:, tgt_paged_kv_indices] = \
        src_pagetable.paged_kv_cache[:, src_paged_kv_indices].to(tgt_device)
    
    return tgt_pagetable, tgt_paged_kv_indices, tgt_paged_kv_last_page_len

def duplicate_paged_kv_cache(
    paged_kv_indices,
    paged_kv_last_page_len,
    pagetable,
):
    # layer, max_num_pages, 2, PAGE_SIZE, kv_heads, kv_dim, 
    tgt_paged_kv_indices = paged_kv_indices[:-1]
    tgt_paged_kv_last_page_len = 16

    pagetable.page_cnt[tgt_paged_kv_indices] += 1

    pagetable, tgt_paged_kv_indices, tgt_paged_kv_last_page_len = \
        allocate_paged_kv_cache(
            pagetable,
            tgt_paged_kv_indices,
            tgt_paged_kv_last_page_len,
            paged_kv_last_page_len
        )

    pagetable.paged_kv_cache[:, tgt_paged_kv_indices[-1]] = \
        pagetable.paged_kv_cache[:, paged_kv_indices[-1]]
    
    return pagetable, tgt_paged_kv_indices, tgt_paged_kv_last_page_len


def move_paged_kv_cache(
    src_paged_kv_indices,
    src_paged_kv_last_page_len,
    src_pagetable,
    tgt_pagetable,
):
    if src_pagetable.paged_kv_cache.device == tgt_pagetable.paged_kv_cache.device:
        return src_pagetable, tgt_pagetable, src_paged_kv_indices, src_paged_kv_last_page_len

    tgt_pagetable, tgt_paged_kv_indices, tgt_paged_kv_last_page_len = \
        copy_paged_kv_cache(
            src_paged_kv_indices,
            src_paged_kv_last_page_len,
            src_pagetable,
            tgt_pagetable,
        )

    src_pagetable, _, _ = \
        pop_paged_kv_cache(
            src_pagetable,
            src_paged_kv_indices,
            src_paged_kv_last_page_len,
            0,
        )
    
    return src_pagetable, tgt_pagetable, tgt_paged_kv_indices, tgt_paged_kv_last_page_len