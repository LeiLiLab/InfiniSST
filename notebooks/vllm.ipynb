{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 05:03:46 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b797dde46c49ec92520ba3107db474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 05:03:51 config.py:526] This model supports multiple tasks: {'embed', 'classify', 'generate', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 02-06 05:03:51 config.py:1383] Defaulting to use mp for distributed inference\n",
      "WARNING 02-06 05:03:51 arg_utils.py:1119] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-06 05:03:51 config.py:1538] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 02-06 05:03:51 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=61952, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6531d614ceb7483181571e92fb9fbda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3116f887011447cbe62a1b47dbce27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197fac65d3c843b18f4e70bfec6bdc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-06 05:03:53 multiproc_worker_utils.py:298] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-06 05:03:53 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:03:53 multiproc_worker_utils.py:227] Worker ready; awaiting tasks\n",
      "INFO 02-06 05:03:54 cuda.py:235] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:03:54 cuda.py:235] Using Flash Attention backend.\n",
      "INFO 02-06 05:03:54 utils.py:938] Found nccl from library libnccl.so.2\n",
      "INFO 02-06 05:03:54 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:03:54 utils.py:938] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:03:54 pynccl.py:67] vLLM is using nccl==2.21.5\n",
      "INFO 02-06 05:03:55 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/siqiouya/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m WARNING 02-06 05:03:55 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 02-06 05:03:55 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/siqiouya/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m WARNING 02-06 05:03:55 custom_all_reduce.py:143] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 02-06 05:03:55 shm_broadcast.py:256] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_8de0dd3c'), local_subscribe_port=33705, remote_subscribe_port=None)\n",
      "INFO 02-06 05:03:55 model_runner.py:1111] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:03:55 model_runner.py:1111] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
      "INFO 02-06 05:03:55 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:03:55 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a4f002cf9848cabef277740ddfe7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000002.safetensors:   0%|          | 0.00/8.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f188b39b0b434990a388e43faf4633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000002.safetensors:   0%|          | 0.00/6.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fd4642b0814aa1ba61cf312481c011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521456c385ff44d3a4c5b4467c4417e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 05:17:58 model_runner.py:1116] Loading model weights took 7.1441 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:17:58 model_runner.py:1116] Loading model weights took 7.1441 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:18:01 worker.py:266] Memory profiling takes 2.28 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:18:01 worker.py:266] the current vLLM instance can use total_gpu_memory (44.42GiB) x gpu_memory_utilization (0.90) = 39.98GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:18:01 worker.py:266] model weights take 7.14GiB; non_torch_memory takes 0.28GiB; PyTorch activation peak memory takes 0.19GiB; the rest of the memory reserved for KV Cache is 32.37GiB.\n",
      "INFO 02-06 05:18:01 worker.py:266] Memory profiling takes 2.39 seconds\n",
      "INFO 02-06 05:18:01 worker.py:266] the current vLLM instance can use total_gpu_memory (44.42GiB) x gpu_memory_utilization (0.90) = 39.98GiB\n",
      "INFO 02-06 05:18:01 worker.py:266] model weights take 7.14GiB; non_torch_memory takes 0.28GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 31.15GiB.\n",
      "INFO 02-06 05:18:01 executor_base.py:108] # CUDA blocks: 72918, # CPU blocks: 9362\n",
      "INFO 02-06 05:18:01 executor_base.py:113] Maximum concurrency for 61952 tokens per request: 18.83x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:18:04 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-06 05:18:04 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-06 05:18:19 model_runner.py:1563] Graph capturing finished in 15 secs, took 0.42 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1763452)\u001b[0;0m INFO 02-06 05:18:19 model_runner.py:1563] Graph capturing finished in 15 secs, took 0.42 GiB\n",
      "INFO 02-06 05:18:19 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 20.46 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", max_model_len=61952, tensor_parallel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You will be provided with instructions, and a sentence in English, and your task is to segment the sentence according to the instructions. Always answer i|n the following JSON format:{{'segments':List[English]}} \n",
    "\n",
    "Instructions: \n",
    "1. Break down the sentence into manageable phrases that each contain enough information to be accurately interpreted. \n",
    "2. Each phrase should be no more than five words while preserving grammatical integrity and interpretability. \n",
    "\n",
    "English: {}\"\"\"\n",
    "sent = \"And in some countries -- let me pick Afghanistan -- 75 percent of the little girls don't go to school. And I don't mean that they drop out of school in the third or fourth grade -- they don't go.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_template.format(sent)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(top_k=1, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.81s/it, est. speed input: 24.11 toks/s, output: 75.94 toks/s]\n"
     ]
    }
   ],
   "source": [
    "output = llm.chat(messages, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, so I need to help the user segment the given sentence into manageable phrases. The instructions say each phrase should be no more than five words and still be grammatically correct and understandable. Let me read the sentence again: \"And in some countries -- let me pick Afghanistan -- 75 percent of the little girls don't go to school. And I don't mean that they drop out of school in the third or fourth grade -- they don't go.\"\n",
      "\n",
      "Hmm, the first part is \"And in some countries -- let me pick Afghanistan --\". That's a bit long. Maybe I can split it into two parts. \"And in some countries\" is four words, but then \"-- let me pick Afghanistan --\" is another part. Wait, but the user's example split it into \"And in some countries -- let me pick Afghanistan\" and \" -- 75 percent of the little girls don't go to school.\" So maybe I should do the same.\n",
      "\n",
      "Next, \"And I don't mean that they drop out of school in the third or fourth grade -- they don't go.\" That's a long phrase. I can split it into \"And I don't mean that they drop out of school in the third or fourth grade\" and \"they don't go.\" That makes sense because each part is under five words and still conveys the meaning.\n",
      "\n",
      "I should make sure each segment is clear and grammatically correct. Let me check each part. The first segment is \"And in some countries -- let me pick Afghanistan --\", which is split into two parts as per the example. The second part is \"75 percent of the little girls don't go to school.\" That's fine. The last part is split into two as well. I think that works. Each segment is concise and retains the original meaning without losing clarity.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"segments\": [\n",
      "    \"And in some countries -- let me pick Afghanistan --\",\n",
      "    \"75 percent of the little girls don't go to school.\",\n",
      "    \"And I don't mean that they drop out of school in the third or fourth grade --\",\n",
      "    \"they don't go.\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
