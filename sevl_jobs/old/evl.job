#!/usr/bin/env bash


#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=64GB
#SBATCH --gpus=1
##SBATCH --constraint=xeon-4116 
#SBATCH --partition=taurus
#SBATCH --time=3-2:34:56 
##SBATCH --dependency=afterok:job_id
##SBATCH --array=1-3 
#SBATCH --account=xixu
#SBATCH --mail-type=BEGIN,END,FAIL,ALL
##SBATCH --mail-user=xixu@cs.cmu.edu 
#SBATCH --output=stdout_evl_uni_b1.txt
#SBATCH --error=stderr_evl_uni_b1.txt

cd eval
source $HOME/sllama/bin/activate

export PYTHONPATH=$PYTHONPATH:$HOME/work/sllama

model_path=/compute/babel-14-13/xixu/runs/3.1-8B-s2-fr/checkpoint-2103
data_path=/compute/babel-6-17/xixu/datasets/must-c-v1.0/en-fr
result=/home/yuanjinw/work/sllama/result/fr/3.1_8b_s2

export NCCL_P2P_DISABLE=1
export NCCL_IB_DISABLE=1

python ../train/zero_to_fp32.py ${model_path} ${model_path}/pytorch_model.bin

python ../train/extract_adapter.py \
  --model_name_or_path ${model_path} \
  --extracted_name 'mm_length_adapter' \
  --output ${model_path}/length_adapter.bin 

python ../train/extract_adapter.py \
  --model_name_or_path ${model_path} \
  --extracted_name 'mm_mlp_adapter' \
  --output ${model_path}/mlp_adapter.bin 

python ../train/extract_adapter.py \
    --model_name_or_path ${model_path} \
    --extracted_name 'speech_tower' \
    --output ${model_path}/speech_tower.bin    

export CUDA_VISIBLE_DEVICES=0

source_lang="English"
target_lang="French"

python ./test_dataset_large.py --batch-size 4 --model-name ${model_path} --data-path ${data_path} --data-split 'tst-COMMON' --result ${result} --beam 4 \
        --mlp-adapter-path ${model_path}/mlp_adapter.bin --length-adapter-path ${model_path}/length_adapter.bin --speech-tower-path ${model_path}/speech_tower.bin \
        --speech-tower-type w2v  --source-lang "${source_lang}"  --target-lang "${target_lang}"

python ./compute_bleu.py ${result}/tst-COMMON