#!/usr/bin/env python3
"""
InfiniSST æ•´åˆæœåŠ¡å¯åŠ¨è„šæœ¬
å°†schedulerã€inference engineå’ŒAPIæœåŠ¡ä¸²è”èµ·æ¥ï¼Œå®ç°å®Œæ•´çš„å¤šè¯·æ±‚å¹¶å‘æ¨ç†ç³»ç»Ÿ
"""

import argparse
import logging
import sys
import os
import signal
import time
import threading
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥ç›¸å…³æ¨¡å—
from serve.api_with_scheduler import InfiniSSTAPIWithScheduler
from serve.inference_engine import MultiGPUInferenceEngine, EngineConfig
from serve.scheduler import LLMScheduler

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('infinisst_integrated.log')
    ]
)
logger = logging.getLogger(__name__)

class InfiniSSTIntegratedServer:
    """
    InfiniSST æ•´åˆæœåŠ¡å™¨
    ç®¡ç†schedulerã€inference engineå’ŒAPIæœåŠ¡çš„ç”Ÿå‘½å‘¨æœŸ
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ•´åˆæœåŠ¡å™¨
        
        Args:
            config: æœåŠ¡å™¨é…ç½®
        """
        self.config = config
        self.is_running = False
        
        # ç»„ä»¶å®ä¾‹
        self.api_server = None
        self.inference_engine = None
        self.scheduler = None
        
        # ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info("InfiniSSTæ•´åˆæœåŠ¡å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _signal_handler(self, signum, frame):
        """å¤„ç†åœæ­¢ä¿¡å·"""
        logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹åœæ­¢æœåŠ¡...")
        self.stop()
        sys.exit(0)
    
    def _create_model_args(self, gpu_id: int) -> Any:
        """åˆ›å»ºæ¨¡å‹å‚æ•°ï¼ˆmockå®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥æ ¹æ®å®é™…éœ€æ±‚åˆ›å»ºæ¨¡å‹å‚æ•°
        # ç°åœ¨å…ˆè¿”å›ä¸€ä¸ªmockå‚æ•°å¯¹è±¡
        class MockModelArgs:
            def __init__(self):
                self.model_name = "mock_model"
                self.gpu_id = gpu_id
                self.batch_size = 32
                self.max_new_tokens = 20
                self.beam_size = 4
                # æ·»åŠ æ›´å¤šéœ€è¦çš„å‚æ•°...
        
        return MockModelArgs()
    
    def start(self):
        """å¯åŠ¨æ•´åˆæœåŠ¡å™¨"""
        if self.is_running:
            logger.warning("æœåŠ¡å™¨å·²åœ¨è¿è¡Œ")
            return
        
        try:
            logger.info("å¼€å§‹å¯åŠ¨InfiniSSTæ•´åˆæœåŠ¡å™¨...")
            
            # 1. åˆ›å»ºæ¨ç†å¼•æ“
            self._create_inference_engine()
            
            # 2. åˆ›å»ºå’Œé…ç½®APIæœåŠ¡å™¨
            self._create_api_server()
            
            # 3. è¿æ¥ç»„ä»¶
            self._connect_components()
            
            # 4. å¯åŠ¨æ‰€æœ‰ç»„ä»¶
            self._start_components()
            
            self.is_running = True
            logger.info("ğŸš€ InfiniSSTæ•´åˆæœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼")
            
            # å¯åŠ¨APIæœåŠ¡å™¨ï¼ˆé˜»å¡ï¼‰
            self._run_api_server()
            
        except Exception as e:
            logger.error(f"å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {e}")
            self.stop()
            raise
    
    def _create_inference_engine(self):
        """åˆ›å»ºæ¨ç†å¼•æ“"""
        logger.info("åˆ›å»ºæ¨ç†å¼•æ“...")
        
        gpu_language_map = self.config.get('gpu_language_map', {0: "English -> Chinese"})
        
        # åˆ›å»ºæ¨¡å‹å‚æ•°æ˜ å°„
        model_args_map = {}
        for gpu_id in gpu_language_map.keys():
            model_args_map[gpu_id] = self._create_model_args(gpu_id)
        
        # åˆ›å»ºå¤šGPUæ¨ç†å¼•æ“
        self.inference_engine = MultiGPUInferenceEngine(
            gpu_language_map=gpu_language_map,
            model_args_map=model_args_map
        )
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get('load_models', False):
            logger.info("åŠ è½½æ¨¡å‹...")
            success = self.inference_engine.load_all_models()
            if not success:
                logger.warning("éƒ¨åˆ†æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨ç†")
        else:
            logger.info("è·³è¿‡æ¨¡å‹åŠ è½½ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨ç†")
        
        logger.info("æ¨ç†å¼•æ“åˆ›å»ºå®Œæˆ")
    
    def _create_api_server(self):
        """åˆ›å»ºAPIæœåŠ¡å™¨"""
        logger.info("åˆ›å»ºAPIæœåŠ¡å™¨...")
        
        gpu_language_map = self.config.get('gpu_language_map', {0: "English -> Chinese"})
        
        self.api_server = InfiniSSTAPIWithScheduler(
            gpu_language_map=gpu_language_map,
            max_batch_size=self.config.get('max_batch_size', 32),
            batch_timeout=self.config.get('batch_timeout', 0.1)
        )
        
        self.scheduler = self.api_server.scheduler
        logger.info("APIæœåŠ¡å™¨åˆ›å»ºå®Œæˆ")
    
    def _connect_components(self):
        """è¿æ¥å„ä¸ªç»„ä»¶"""
        logger.info("è¿æ¥ç³»ç»Ÿç»„ä»¶...")
        
        # å°†æ¨ç†å¼•æ“è¿æ¥åˆ°è°ƒåº¦å™¨
        if self.scheduler and self.inference_engine:
            self.scheduler.set_inference_engine(self.inference_engine)
            logger.info("æ¨ç†å¼•æ“å·²è¿æ¥åˆ°è°ƒåº¦å™¨")
        
        logger.info("ç»„ä»¶è¿æ¥å®Œæˆ")
    
    def _start_components(self):
        """å¯åŠ¨æ‰€æœ‰ç»„ä»¶"""
        logger.info("å¯åŠ¨ç³»ç»Ÿç»„ä»¶...")
        
        # å¯åŠ¨æ¨ç†å¼•æ“
        if self.inference_engine:
            self.inference_engine.start_all()
            logger.info("æ¨ç†å¼•æ“å·²å¯åŠ¨")
        
        # å¯åŠ¨è°ƒåº¦å™¨
        if self.scheduler:
            self.scheduler.start()
            logger.info("è°ƒåº¦å™¨å·²å¯åŠ¨")
        
        logger.info("æ‰€æœ‰ç»„ä»¶å¯åŠ¨å®Œæˆ")
    
    def _run_api_server(self):
        """è¿è¡ŒAPIæœåŠ¡å™¨"""
        if self.api_server:
            host = self.config.get('host', '0.0.0.0')
            port = self.config.get('port', 8000)
            debug = self.config.get('debug', False)
            
            logger.info(f"å¯åŠ¨APIæœåŠ¡å™¨ï¼Œç›‘å¬ {host}:{port}")
            self.api_server.run(host=host, port=port, debug=debug)
    
    def stop(self):
        """åœæ­¢æ•´åˆæœåŠ¡å™¨"""
        if not self.is_running:
            return
        
        logger.info("å¼€å§‹åœæ­¢InfiniSSTæ•´åˆæœåŠ¡å™¨...")
        
        # åœæ­¢æ¨ç†å¼•æ“
        if self.inference_engine:
            self.inference_engine.stop_all()
            logger.info("æ¨ç†å¼•æ“å·²åœæ­¢")
        
        # åœæ­¢è°ƒåº¦å™¨
        if self.scheduler:
            self.scheduler.stop()
            logger.info("è°ƒåº¦å™¨å·²åœæ­¢")
        
        self.is_running = False
        logger.info("InfiniSSTæ•´åˆæœåŠ¡å™¨å·²åœæ­¢")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        status = {
            'is_running': self.is_running,
            'config': self.config,
            'scheduler_stats': None,
            'inference_engine_stats': None
        }
        
        if self.scheduler:
            status['scheduler_stats'] = self.scheduler.get_queue_stats()
        
        if self.inference_engine:
            status['inference_engine_stats'] = self.inference_engine.get_all_stats()
        
        return status

def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        'gpu_language_map': {
            0: "English -> Chinese",
            # å¯ä»¥æ·»åŠ æ›´å¤šGPUå’Œè¯­è¨€å¯¹
            # 1: "English -> German",
        },
        'max_batch_size': 32,
        'batch_timeout': 0.1,
        'load_models': False,  # æ˜¯å¦åŠ è½½å®é™…æ¨¡å‹
        'host': '0.0.0.0',
        'port': 8000,
        'debug': False
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='InfiniSSTæ•´åˆæœåŠ¡å™¨')
    
    # æœåŠ¡å™¨é…ç½®
    parser.add_argument('--host', default='0.0.0.0', help='æœåŠ¡å™¨åœ°å€')
    parser.add_argument('--port', type=int, default=8000, help='æœåŠ¡å™¨ç«¯å£')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    
    # æ¨ç†é…ç½®
    parser.add_argument('--max-batch-size', type=int, default=32, help='æœ€å¤§æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--batch-timeout', type=float, default=0.1, help='æ‰¹å¤„ç†è¶…æ—¶æ—¶é—´')
    parser.add_argument('--load-models', action='store_true', help='åŠ è½½å®é™…æ¨¡å‹ï¼ˆé»˜è®¤ä½¿ç”¨æ¨¡æ‹Ÿï¼‰')
    
    # GPUé…ç½®
    parser.add_argument('--gpus', type=str, default='0', help='ä½¿ç”¨çš„GPU IDåˆ—è¡¨ï¼Œé€—å·åˆ†éš”')
    parser.add_argument('--languages', type=str, default='English -> Chinese', 
                       help='è¯­è¨€å¯¹åˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼Œä¸GPUå¯¹åº”')
    
    args = parser.parse_args()
    
    # è§£æGPUå’Œè¯­è¨€æ˜ å°„
    gpu_ids = [int(x.strip()) for x in args.gpus.split(',')]
    languages = [x.strip() for x in args.languages.split(',')]
    
    if len(gpu_ids) != len(languages):
        logger.error("GPUæ•°é‡å’Œè¯­è¨€å¯¹æ•°é‡ä¸åŒ¹é…")
        sys.exit(1)
    
    gpu_language_map = dict(zip(gpu_ids, languages))
    
    # åˆ›å»ºé…ç½®
    config = create_default_config()
    config.update({
        'gpu_language_map': gpu_language_map,
        'max_batch_size': args.max_batch_size,
        'batch_timeout': args.batch_timeout,
        'load_models': args.load_models,
        'host': args.host,
        'port': args.port,
        'debug': args.debug
    })
    
    # åˆ›å»ºå¹¶å¯åŠ¨æœåŠ¡å™¨
    server = InfiniSSTIntegratedServer(config)
    
    try:
        server.start()
    except KeyboardInterrupt:
        logger.info("æ¥æ”¶åˆ°é”®ç›˜ä¸­æ–­ä¿¡å·")
    except Exception as e:
        logger.error(f"æœåŠ¡å™¨è¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)
    finally:
        server.stop()

if __name__ == '__main__':
    main() 