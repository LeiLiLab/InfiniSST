#!/usr/bin/env python3
"""
InfiniSST Evaluation Runner
è¿è¡Œå¹¶å‘ç”¨æˆ·è¯„ä¼°æµ‹è¯•çš„ç¤ºä¾‹è„šæœ¬
"""

import asyncio
import sys
import os
import numpy as np

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from evaluation_framework import EvaluationFramework, TestConfig

async def run_quick_test():
    """è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼š8ä¸ªç”¨æˆ·ï¼Œ30ç§’"""
    config = TestConfig(
        num_users=8,
        language_split=0.5,  # 50% Chinese, 50% Italian
        arrival_rate=1.0,    # 1 user per second
        test_duration=30,    # 30 seconds
        server_url="http://localhost:8000",
        output_dir="evaluation_results/quick_test",
        use_dynamic_schedule=False,
        max_batch_size=16,
        batch_timeout=0.1,
        latency_range=[1, 2, 3, 4],  # ğŸ”¥ æ–°å¢ï¼šæµ‹è¯•æ‰€æœ‰latency
        latency_distribution=None    # ğŸ”¥ ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
    )
    
    framework = EvaluationFramework(config)
    results = await framework.run_evaluation()
    
    print(f"\nğŸ‰ Quick Test Results:")
    print(f"   - Completed users: {results.completed_users}")
    print(f"   - Failed users: {results.failed_users}")
    print(f"   - Average streamLAAL: {results.avg_stream_laal:.3f}s")
    print(f"   - Test duration: {results.total_duration:.1f}s")
    
    return results

async def run_moderate_test():
    """è¿è¡Œä¸­ç­‰è§„æ¨¡æµ‹è¯•ï¼š16ä¸ªç”¨æˆ·ï¼Œ20åˆ†é’Ÿ"""
    config = TestConfig(
        num_users=16,
        language_split=0.5,
        arrival_rate=2.0,    # 2 users per second
        test_duration=1200,   # 20 minutes
        server_url="http://localhost:8000",
        output_dir="evaluation_results/moderate_test",
        use_dynamic_schedule=False,
        max_batch_size=32,
        batch_timeout=0.1,
        latency_range=[1, 2, 3, 4],  # ğŸ”¥ æ–°å¢ï¼šæµ‹è¯•æ‰€æœ‰latency
        latency_distribution=None    # ğŸ”¥ ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
    )
    
    framework = EvaluationFramework(config)
    results = await framework.run_evaluation()
    
    print(f"\nğŸ‰ Moderate Test Results:")
    print(f"   - Completed users: {results.completed_users}")
    print(f"   - Failed users: {results.failed_users}")
    print(f"   - Average streamLAAL: {results.avg_stream_laal:.3f}s")
    print(f"   - Test duration: {results.total_duration:.1f}s")
    
    return results

async def run_large_scale_test():
    """è¿è¡Œå¤§è§„æ¨¡æµ‹è¯•ï¼š32ä¸ªç”¨æˆ·ï¼Œ50åˆ†é’Ÿ"""
    config = TestConfig(
        num_users=32,
        language_split=0.5,
        arrival_rate=2.0,    # 2 users per second
        test_duration=3000,   # 50 minutes
        server_url="http://localhost:8000",
        output_dir="evaluation_results/large_scale_test",
        use_dynamic_schedule=False,
        max_batch_size=32,
        batch_timeout=0.1,
        latency_range=[1, 2, 3, 4],  # ğŸ”¥ æ–°å¢ï¼šæµ‹è¯•æ‰€æœ‰latency
        latency_distribution=None    # ğŸ”¥ ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
    )
    
    framework = EvaluationFramework(config)
    results = await framework.run_evaluation()
    
    print(f"\nğŸ‰ Large Scale Test Results:")
    print(f"   - Completed users: {results.completed_users}")
    print(f"   - Failed users: {results.failed_users}")
    print(f"   - Average streamLAAL: {results.avg_stream_laal:.3f}s")
    print(f"   - Test duration: {results.total_duration:.1f}s")
    
    return results

async def run_dynamic_schedule_comparison():
    """è¿è¡ŒåŠ¨æ€è°ƒåº¦å¯¹æ¯”æµ‹è¯•"""
    print("ğŸš€ Running Dynamic Scheduling Comparison Test...")
    
    # æµ‹è¯•é™æ€è°ƒåº¦
    print("\n1ï¸âƒ£ Testing Static Scheduling...")
    static_config = TestConfig(
        num_users=16,
        language_split=0.5,
        arrival_rate=2.0,
        test_duration=120,
        server_url="http://localhost:8000",
        output_dir="evaluation_results/static_schedule",
        use_dynamic_schedule=False,
        max_batch_size=32,
        batch_timeout=0.1,
        latency_range=[1, 2, 3, 4],  # ğŸ”¥ æ–°å¢ï¼šæµ‹è¯•æ‰€æœ‰latency
        latency_distribution=None    # ğŸ”¥ ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
    )
    
    static_framework = EvaluationFramework(static_config)
    static_results = await static_framework.run_evaluation()
    
    # æµ‹è¯•åŠ¨æ€è°ƒåº¦
    print("\n2ï¸âƒ£ Testing Dynamic Scheduling...")
    dynamic_config = TestConfig(
        num_users=16,
        language_split=0.5,
        arrival_rate=2.0,
        test_duration=120,
        server_url="http://localhost:8000",
        output_dir="evaluation_results/dynamic_schedule",
        use_dynamic_schedule=True,
        max_batch_size=32,
        batch_timeout=0.05,  # æ›´çŸ­çš„batch timeout
        latency_range=[1, 2, 3, 4],  # ğŸ”¥ æ–°å¢ï¼šæµ‹è¯•æ‰€æœ‰latency
        latency_distribution=None    # ğŸ”¥ ä½¿ç”¨å‡åŒ€åˆ†å¸ƒ
    )
    
    dynamic_framework = EvaluationFramework(dynamic_config)
    dynamic_results = await dynamic_framework.run_evaluation()
    
    # æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š Comparison Results:")
    print(f"   Static Scheduling:")
    print(f"     - Average streamLAAL: {static_results.avg_stream_laal:.3f}s")
    print(f"     - Completed users: {static_results.completed_users}")
    print(f"     - Failed users: {static_results.failed_users}")
    print(f"   Dynamic Scheduling:")
    print(f"     - Average streamLAAL: {dynamic_results.avg_stream_laal:.3f}s")
    print(f"     - Completed users: {dynamic_results.completed_users}")
    print(f"     - Failed users: {dynamic_results.failed_users}")
    
    improvement = (static_results.avg_stream_laal - dynamic_results.avg_stream_laal) / static_results.avg_stream_laal * 100
    print(f"   Improvement: {improvement:+.1f}% (negative means dynamic is slower)")
    
    return static_results, dynamic_results

async def run_latency_comparison():
    """è¿è¡Œä¸åŒlatencyå¯¹æ¯”æµ‹è¯•"""
    print("ğŸš€ Running Latency Comparison Test...")
    
    latency_tests = []
    
    # åˆ†åˆ«æµ‹è¯•æ¯ä¸ªlatencyå€¼
    for latency in [1, 2, 3, 4]:
        print(f"\n{latency}ï¸âƒ£ Testing Latency {latency}x...")
        config = TestConfig(
            num_users=12,  # æ¯ä¸ªlatencyæµ‹è¯•12ä¸ªç”¨æˆ·
            language_split=0.5,
            arrival_rate=2.0,
            test_duration=90,  # 1.5åˆ†é’Ÿ
            server_url="http://localhost:8000",
            output_dir=f"evaluation_results/latency_{latency}x",
            use_dynamic_schedule=False,
            max_batch_size=32,
            batch_timeout=0.1,
            latency_range=[latency],  # ğŸ”¥ åªä½¿ç”¨å•ä¸€latency
            latency_distribution=None
        )
        
        framework = EvaluationFramework(config)
        results = await framework.run_evaluation()
        latency_tests.append((latency, results))
        
        print(f"   âœ… Latency {latency}x: {results.completed_users} completed, avg streamLAAL = {results.avg_stream_laal:.3f}s")
    
    # æ··åˆlatencyæµ‹è¯•
    print(f"\nğŸ”€ Testing Mixed Latency Distribution...")
    mixed_config = TestConfig(
        num_users=16,
        language_split=0.5,
        arrival_rate=2.0,
        test_duration=120,
        server_url="http://localhost:8000",
        output_dir="evaluation_results/mixed_latency",
        use_dynamic_schedule=False,
        max_batch_size=32,
        batch_timeout=0.1,
        latency_range=[1, 2, 3, 4],  # ğŸ”¥ æ··åˆæ‰€æœ‰latency
        latency_distribution=[0.2, 0.3, 0.3, 0.2]  # ğŸ”¥ è‡ªå®šä¹‰åˆ†å¸ƒï¼šæ›´å¤š2xå’Œ3x
    )
    
    mixed_framework = EvaluationFramework(mixed_config)
    mixed_results = await mixed_framework.run_evaluation()
    latency_tests.append(("mixed", mixed_results))
    
    # æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š Latency Comparison Results:")
    print(f"{'Latency':<8} {'Users':<6} {'Completed':<10} {'Avg streamLAAL':<15} {'Std Dev':<10}")
    print("-" * 60)
    
    for latency, results in latency_tests:
        latency_str = f"{latency}x" if isinstance(latency, int) else latency
        print(f"{latency_str:<8} {results.config.num_users:<6} {results.completed_users:<10} "
              f"{results.avg_stream_laal:<15.3f} {results.std_stream_laal:<10.3f}")
    
    # åˆ†ælatencyä¸streamLAALçš„å…³ç³»
    single_latency_results = [(lat, res) for lat, res in latency_tests if isinstance(lat, int)]
    if len(single_latency_results) >= 2:
        print(f"\nğŸ“ˆ Latency Impact Analysis:")
        latencies = [lat for lat, _ in single_latency_results]
        stream_laals = [res.avg_stream_laal for _, res in single_latency_results]
        
        # è®¡ç®—ç›¸å…³æ€§
        if len(latencies) > 1:
            correlation = np.corrcoef(latencies, stream_laals)[0, 1]
            print(f"   - Correlation between latency multiplier and streamLAAL: {correlation:.3f}")
            
            # æ˜¾ç¤ºè¶‹åŠ¿
            if correlation > 0.5:
                print(f"   - Strong positive correlation: Higher latency â†’ Higher streamLAAL")
            elif correlation < -0.5:
                print(f"   - Strong negative correlation: Higher latency â†’ Lower streamLAAL")
            else:
                print(f"   - Weak correlation: Latency has limited impact on streamLAAL")
    
    return latency_tests

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="InfiniSST Evaluation Runner")
    parser.add_argument("--test", choices=["quick", "moderate", "large", "dynamic_comparison", "latency_comparison"], 
                       default="quick", help="Type of test to run")
    parser.add_argument("--server-url", default="http://localhost:8000", 
                       help="Server URL")
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Starting InfiniSST Evaluation Test: {args.test}")
    print(f"ğŸ“¡ Server URL: {args.server_url}")
    
    try:
        if args.test == "quick":
            results = await run_quick_test()
        elif args.test == "moderate":
            results = await run_moderate_test()
        elif args.test == "large":
            results = await run_large_scale_test()
        elif args.test == "dynamic_comparison":
            results = await run_dynamic_schedule_comparison()
        elif args.test == "latency_comparison":
            results = await run_latency_comparison()
        
        print(f"\nâœ… Evaluation completed successfully!")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ Evaluation interrupted by user")
    except Exception as e:
        print(f"\nâŒ Evaluation failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main()) 