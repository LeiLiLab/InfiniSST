#!/usr/bin/env python3
"""
Ray-based InfiniSST Evaluation Framework
åŸºäºRayçš„InfiniSSTè¯„ä¼°æ¡†æ¶
"""

import asyncio
import aiohttp
import numpy as np
import time
import json
import logging
import argparse
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import pandas as pd
import websockets
import sys
import os

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RayTestConfig:
    """Rayæµ‹è¯•é…ç½®"""
    server_url: str = "http://localhost:8000"
    websocket_url: str = "ws://localhost:8000"
    
    # Test parameters
    num_users: int = 8
    test_duration: int = 120  # seconds
    arrival_rate: float = 1.0  # users per second
    
    # Language distribution
    language_pairs: List[str] = field(default_factory=lambda: [
        "English -> Chinese",
        "English -> Italian"
    ])
    language_weights: List[float] = field(default_factory=lambda: [0.5, 0.5])
    
    # Audio configuration
    audio_duration: float = 10.0  # seconds
    chunk_size: int = 1600  # samples per chunk (0.1s at 16kHz)
    sample_rate: int = 16000
    
    # Ray-specific configuration
    test_batch_sizes: List[int] = field(default_factory=lambda: [1, 4, 8, 16, 32])
    test_batch_timeouts: List[float] = field(default_factory=lambda: [50.0, 100.0, 200.0])
    
    # Output
    output_dir: str = "ray_evaluation_results"

@dataclass
class UserSimulation:
    """ç”¨æˆ·æ¨¡æ‹Ÿ"""
    user_id: str
    language_pair: str
    arrival_time: float
    session_id: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    total_latency: float = 0.0
    response_times: List[float] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)
    chunks_sent: int = 0
    chunks_received: int = 0
    status: str = "pending"  # pending, running, completed, failed

@dataclass
class BatchTestResult:
    """æ‰¹å¤„ç†æµ‹è¯•ç»“æœ"""
    batch_size: int
    batch_timeout: float
    total_requests: int
    avg_latency: float
    throughput: float
    success_rate: float
    error_count: int

class RayEvaluationFramework:
    """Rayè¯„ä¼°æ¡†æ¶"""
    
    def __init__(self, config: RayTestConfig):
        self.config = config
        self.results: List[UserSimulation] = []
        self.batch_results: List[BatchTestResult] = []
        
        # Create output directory
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
        
        logger.info(f"Rayè¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.config.output_dir}")
    
    async def run_basic_functionality_test(self) -> bool:
        """åŸºæœ¬åŠŸèƒ½æµ‹è¯•"""
        logger.info("ğŸ§ª å¼€å§‹åŸºæœ¬åŠŸèƒ½æµ‹è¯•...")
        
        try:
            async with aiohttp.ClientSession() as session:
                # æµ‹è¯•å¥åº·æ£€æŸ¥
                async with session.get(f"{self.config.server_url}/health") as response:
                    if response.status != 200:
                        logger.error("å¥åº·æ£€æŸ¥å¤±è´¥")
                        return False
                    
                    health_data = await response.json()
                    logger.info(f"å¥åº·æ£€æŸ¥é€šè¿‡: {health_data.get('status', 'unknown')}")
                
                # æµ‹è¯•Rayç»Ÿè®¡ä¿¡æ¯
                async with session.get(f"{self.config.server_url}/ray/stats") as response:
                    if response.status == 200:
                        ray_stats = await response.json()
                        logger.info(f"Rayç»Ÿè®¡ä¿¡æ¯: {ray_stats.get('success', False)}")
                    else:
                        logger.warning("Rayç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥")
                
                # æµ‹è¯•ä¼šè¯åˆ›å»º
                init_data = {
                    "agent_type": "InfiniSST",
                    "language_pair": "English -> Chinese",
                    "client_id": "test_user_001"
                }
                
                async with session.post(f"{self.config.server_url}/init", json=init_data) as response:
                    if response.status != 200:
                        logger.error("ä¼šè¯åˆ›å»ºå¤±è´¥")
                        return False
                    
                    session_data = await response.json()
                    session_id = session_data.get("session_id")
                    if not session_id:
                        logger.error("æœªè·å–åˆ°æœ‰æ•ˆçš„ä¼šè¯ID")
                        return False
                    
                    logger.info(f"ä¼šè¯åˆ›å»ºæˆåŠŸ: {session_id}")
                
                # æµ‹è¯•WebSocketè¿æ¥
                websocket_url = f"{self.config.websocket_url}/wss/{session_id}"
                try:
                    async with websockets.connect(websocket_url) as websocket:
                        # ç­‰å¾…READYæ¶ˆæ¯
                        ready_msg = await websocket.recv()
                        if "READY" not in ready_msg:
                            logger.error(f"æœªæ”¶åˆ°READYæ¶ˆæ¯: {ready_msg}")
                            return False
                        
                        logger.info("WebSocketè¿æ¥æµ‹è¯•æˆåŠŸ")
                        
                        # å‘é€æµ‹è¯•éŸ³é¢‘æ•°æ®
                        test_audio = np.random.randn(1600).astype(np.float32)
                        await websocket.send(test_audio.tobytes())
                        
                        # ç­‰å¾…å“åº”æˆ–è¶…æ—¶
                        try:
                            response = await asyncio.wait_for(websocket.recv(), timeout=10.0)
                            logger.info(f"æ”¶åˆ°å“åº”: {response[:100]}...")
                        except asyncio.TimeoutError:
                            logger.warning("æœªåœ¨10ç§’å†…æ”¶åˆ°å“åº”ï¼Œä½†è¿æ¥æ­£å¸¸")
                        
                        # å‘é€EOF
                        await websocket.send("EOF")
                        final_response = await websocket.recv()
                        logger.info(f"æœ€ç»ˆå“åº”: {final_response}")
                        
                except Exception as e:
                    logger.error(f"WebSocketæµ‹è¯•å¤±è´¥: {e}")
                    return False
                
                # æ¸…ç†ä¼šè¯
                await session.post(f"{self.config.server_url}/delete_session", json={"session_id": session_id})
                logger.info("æµ‹è¯•ä¼šè¯å·²æ¸…ç†")
            
            logger.info("âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    async def run_batch_performance_test(self) -> List[BatchTestResult]:
        """æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•...")
        
        batch_results = []
        
        for batch_size in self.config.test_batch_sizes:
            for batch_timeout in self.config.test_batch_timeouts:
                logger.info(f"æµ‹è¯•æ‰¹å¤„ç†é…ç½®: size={batch_size}, timeout={batch_timeout}ms")
                
                # é…ç½®Rayç³»ç»Ÿ
                async with aiohttp.ClientSession() as session:
                    config_data = {
                        "max_batch_size": batch_size,
                        "batch_timeout_ms": batch_timeout,
                        "enable_dynamic_batching": True
                    }
                    
                    async with session.post(f"{self.config.server_url}/ray/configure", json=config_data) as response:
                        if response.status != 200:
                            logger.warning(f"æ‰¹å¤„ç†é…ç½®å¤±è´¥: {await response.text()}")
                            continue
                
                # è¿è¡Œè´Ÿè½½æµ‹è¯•
                result = await self._run_load_test_for_batch_config(batch_size, batch_timeout)
                if result:
                    batch_results.append(result)
                    logger.info(f"æ‰¹å¤„ç†æµ‹è¯•å®Œæˆ: {result}")
                
                # ç­‰å¾…ç³»ç»Ÿç¨³å®š
                await asyncio.sleep(5)
        
        self.batch_results = batch_results
        return batch_results
    
    async def _run_load_test_for_batch_config(self, batch_size: int, batch_timeout: float) -> Optional[BatchTestResult]:
        """ä¸ºç‰¹å®šæ‰¹å¤„ç†é…ç½®è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        
        # åˆ›å»ºå¹¶å‘ç”¨æˆ·
        concurrent_users = min(batch_size * 2, 16)  # ä¸è¶…è¿‡16ä¸ªå¹¶å‘ç”¨æˆ·
        
        users = []
        for i in range(concurrent_users):
            user = UserSimulation(
                user_id=f"batch_test_user_{i}",
                language_pair=np.random.choice(self.config.language_pairs, p=self.config.language_weights),
                arrival_time=time.time() + np.random.exponential(1.0 / 2.0)  # é«˜é¢‘ç‡åˆ°è¾¾
            )
            users.append(user)
        
        # å¹¶å‘è¿è¡Œç”¨æˆ·æ¨¡æ‹Ÿ
        start_time = time.time()
        tasks = [self._simulate_user(user) for user in users]
        
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()
            
            # ç»Ÿè®¡ç»“æœ
            successful_users = []
            error_count = 0
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    users[i].status = "failed"
                    users[i].errors.append(str(result))
                    error_count += 1
                else:
                    if users[i].status == "completed":
                        successful_users.append(users[i])
            
            if not successful_users:
                logger.warning(f"æ‰¹å¤„ç†æµ‹è¯•æ— æˆåŠŸç”¨æˆ·: batch_size={batch_size}")
                return None
            
            # è®¡ç®—æŒ‡æ ‡
            total_requests = sum(user.chunks_sent for user in users)
            avg_latency = np.mean([np.mean(user.response_times) for user in successful_users if user.response_times])
            throughput = total_requests / (end_time - start_time)
            success_rate = len(successful_users) / len(users)
            
            return BatchTestResult(
                batch_size=batch_size,
                batch_timeout=batch_timeout,
                total_requests=total_requests,
                avg_latency=avg_latency,
                throughput=throughput,
                success_rate=success_rate,
                error_count=error_count
            )
            
        except Exception as e:
            logger.error(f"è´Ÿè½½æµ‹è¯•å¼‚å¸¸: {e}")
            return None
    
    async def run_concurrent_users_test(self) -> List[UserSimulation]:
        """å¹¶å‘ç”¨æˆ·æµ‹è¯•"""
        logger.info(f"ğŸ‘¥ å¼€å§‹å¹¶å‘ç”¨æˆ·æµ‹è¯•: {self.config.num_users} ç”¨æˆ·")
        
        # ç”Ÿæˆç”¨æˆ·æ¨¡æ‹Ÿ
        users = []
        for i in range(self.config.num_users):
            user = UserSimulation(
                user_id=f"concurrent_user_{i:03d}",
                language_pair=np.random.choice(self.config.language_pairs, p=self.config.language_weights),
                arrival_time=time.time() + np.random.exponential(1.0 / self.config.arrival_rate)
            )
            users.append(user)
        
        # æŒ‰åˆ°è¾¾æ—¶é—´æ’åº
        users.sort(key=lambda u: u.arrival_time)
        
        # å¹¶å‘è¿è¡Œç”¨æˆ·æ¨¡æ‹Ÿ
        tasks = []
        for user in users:
            # å»¶è¿Ÿå¯åŠ¨ç”¨æˆ·
            delay = max(0, user.arrival_time - time.time())
            task = asyncio.create_task(self._delayed_user_start(user, delay))
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ç”¨æˆ·å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                users[i].status = "failed"
                users[i].errors.append(str(result))
                logger.error(f"ç”¨æˆ· {users[i].user_id} å¤±è´¥: {result}")
        
        self.results = users
        return users
    
    async def _delayed_user_start(self, user: UserSimulation, delay: float) -> UserSimulation:
        """å»¶è¿Ÿå¯åŠ¨ç”¨æˆ·"""
        if delay > 0:
            await asyncio.sleep(delay)
        
        return await self._simulate_user(user)
    
    async def _simulate_user(self, user: UserSimulation) -> UserSimulation:
        """æ¨¡æ‹Ÿå•ä¸ªç”¨æˆ·"""
        user.start_time = time.time()
        user.status = "running"
        
        try:
            # åˆ›å»ºä¼šè¯
            session_id = await self._create_session(user)
            if not session_id:
                user.status = "failed"
                user.errors.append("ä¼šè¯åˆ›å»ºå¤±è´¥")
                return user
            
            user.session_id = session_id
            
            # WebSocketé€šä¿¡
            await self._websocket_communication(user)
            
            # æ¸…ç†ä¼šè¯
            await self._cleanup_session(user)
            
            user.end_time = time.time()
            user.status = "completed"
            
        except Exception as e:
            user.status = "failed"
            user.errors.append(str(e))
            user.end_time = time.time()
            logger.error(f"ç”¨æˆ·æ¨¡æ‹Ÿå¤±è´¥ {user.user_id}: {e}")
        
        return user
    
    async def _create_session(self, user: UserSimulation) -> Optional[str]:
        """åˆ›å»ºä¼šè¯"""
        async with aiohttp.ClientSession() as session:
            init_data = {
                "agent_type": "InfiniSST",
                "language_pair": user.language_pair,
                "client_id": user.user_id,
                "evaluation_mode": "true"
            }
            
            try:
                async with session.post(f"{self.config.server_url}/init", json=init_data) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("session_id")
                    else:
                        logger.error(f"ä¼šè¯åˆ›å»ºå¤±è´¥ {user.user_id}: {response.status}")
                        return None
            except Exception as e:
                logger.error(f"ä¼šè¯åˆ›å»ºå¼‚å¸¸ {user.user_id}: {e}")
                return None
    
    async def _websocket_communication(self, user: UserSimulation):
        """WebSocketé€šä¿¡"""
        websocket_url = f"{self.config.websocket_url}/wss/{user.session_id}"
        
        async with websockets.connect(websocket_url) as websocket:
            # ç­‰å¾…READYæ¶ˆæ¯
            ready_msg = await asyncio.wait_for(websocket.recv(), timeout=30.0)
            if "READY" not in ready_msg:
                raise Exception(f"æœªæ”¶åˆ°READYæ¶ˆæ¯: {ready_msg}")
            
            # ç”Ÿæˆå¹¶å‘é€éŸ³é¢‘æ•°æ®
            total_samples = int(self.config.audio_duration * self.config.sample_rate)
            
            for chunk_start in range(0, total_samples, self.config.chunk_size):
                chunk_end = min(chunk_start + self.config.chunk_size, total_samples)
                chunk = np.random.randn(chunk_end - chunk_start).astype(np.float32)
                
                send_time = time.time()
                await websocket.send(chunk.tobytes())
                user.chunks_sent += 1
                
                # å°è¯•æ¥æ”¶å“åº”ï¼ˆéé˜»å¡ï¼‰
                try:
                    response = await asyncio.wait_for(websocket.recv(), timeout=0.1)
                    receive_time = time.time()
                    user.response_times.append(receive_time - send_time)
                    user.chunks_received += 1
                except asyncio.TimeoutError:
                    pass  # æ²¡æœ‰å“åº”ï¼Œç»§ç»­å‘é€ä¸‹ä¸€å—
                
                # æ¨¡æ‹Ÿå®æ—¶éŸ³é¢‘é—´éš”
                await asyncio.sleep(self.config.chunk_size / self.config.sample_rate)
            
            # å‘é€EOFå¹¶ç­‰å¾…æœ€ç»ˆå“åº”
            await websocket.send("EOF")
            try:
                final_response = await asyncio.wait_for(websocket.recv(), timeout=10.0)
                user.chunks_received += 1
            except asyncio.TimeoutError:
                logger.warning(f"ç”¨æˆ· {user.user_id} æœªæ”¶åˆ°æœ€ç»ˆå“åº”")
    
    async def _cleanup_session(self, user: UserSimulation):
        """æ¸…ç†ä¼šè¯"""
        if not user.session_id:
            return
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(f"{self.config.server_url}/delete_session", 
                                      json={"session_id": user.session_id}) as response:
                    if response.status != 200:
                        logger.warning(f"ä¼šè¯æ¸…ç†å¤±è´¥ {user.user_id}: {response.status}")
            except Exception as e:
                logger.warning(f"ä¼šè¯æ¸…ç†å¼‚å¸¸ {user.user_id}: {e}")
    
    def generate_report(self):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        timestamp = int(time.time())
        report_dir = Path(self.config.output_dir) / f"ray_evaluation_{timestamp}"
        report_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆç”¨æˆ·æµ‹è¯•æŠ¥å‘Š
        if self.results:
            self._generate_user_test_report(report_dir)
        
        # ç”Ÿæˆæ‰¹å¤„ç†æµ‹è¯•æŠ¥å‘Š
        if self.batch_results:
            self._generate_batch_test_report(report_dir)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_summary_report(report_dir)
        
        logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_dir}")
    
    def _generate_user_test_report(self, report_dir: Path):
        """ç”Ÿæˆç”¨æˆ·æµ‹è¯•æŠ¥å‘Š"""
        successful_users = [u for u in self.results if u.status == "completed"]
        failed_users = [u for u in self.results if u.status == "failed"]
        
        # ç»Ÿè®¡æ•°æ®
        stats = {
            "total_users": len(self.results),
            "successful_users": len(successful_users),
            "failed_users": len(failed_users),
            "success_rate": len(successful_users) / len(self.results) if self.results else 0,
        }
        
        if successful_users:
            response_times = []
            for user in successful_users:
                response_times.extend(user.response_times)
            
            if response_times:
                stats.update({
                    "avg_response_time": np.mean(response_times),
                    "median_response_time": np.median(response_times),
                    "p95_response_time": np.percentile(response_times, 95),
                    "p99_response_time": np.percentile(response_times, 99),
                })
        
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        with open(report_dir / "user_test_stats.json", "w") as f:
            json.dump(stats, f, indent=2)
        
        # ç”Ÿæˆå›¾è¡¨
        if successful_users and any(u.response_times for u in successful_users):
            plt.figure(figsize=(12, 8))
            
            # å“åº”æ—¶é—´åˆ†å¸ƒ
            plt.subplot(2, 2, 1)
            all_response_times = []
            for user in successful_users:
                all_response_times.extend(user.response_times)
            
            plt.hist(all_response_times, bins=50, alpha=0.7)
            plt.xlabel("Response Time (s)")
            plt.ylabel("Frequency")
            plt.title("Response Time Distribution")
            
            # ç”¨æˆ·æˆåŠŸç‡
            plt.subplot(2, 2, 2)
            labels = ["Successful", "Failed"]
            sizes = [len(successful_users), len(failed_users)]
            colors = ["green", "red"]
            plt.pie(sizes, labels=labels, colors=colors, autopct="%1.1f%%")
            plt.title("User Success Rate")
            
            # è¯­è¨€å¯¹åˆ†å¸ƒ
            plt.subplot(2, 2, 3)
            language_counts = {}
            for user in self.results:
                lang = user.language_pair
                language_counts[lang] = language_counts.get(lang, 0) + 1
            
            plt.bar(language_counts.keys(), language_counts.values())
            plt.xlabel("Language Pair")
            plt.ylabel("Number of Users")
            plt.title("Language Pair Distribution")
            plt.xticks(rotation=45)
            
            # æ¯ä¸ªç”¨æˆ·çš„å“åº”æ—¶é—´
            plt.subplot(2, 2, 4)
            user_avg_times = [np.mean(u.response_times) if u.response_times else 0 for u in successful_users]
            plt.scatter(range(len(user_avg_times)), user_avg_times, alpha=0.6)
            plt.xlabel("User Index")
            plt.ylabel("Average Response Time (s)")
            plt.title("Response Time per User")
            
            plt.tight_layout()
            plt.savefig(report_dir / "user_test_charts.png", dpi=300, bbox_inches="tight")
            plt.close()
    
    def _generate_batch_test_report(self, report_dir: Path):
        """ç”Ÿæˆæ‰¹å¤„ç†æµ‹è¯•æŠ¥å‘Š"""
        if not self.batch_results:
            return
        
        # è½¬æ¢ä¸ºDataFrameä»¥ä¾¿åˆ†æ
        df_data = []
        for result in self.batch_results:
            df_data.append({
                "batch_size": result.batch_size,
                "batch_timeout": result.batch_timeout,
                "total_requests": result.total_requests,
                "avg_latency": result.avg_latency,
                "throughput": result.throughput,
                "success_rate": result.success_rate,
                "error_count": result.error_count
            })
        
        df = pd.DataFrame(df_data)
        
        # ä¿å­˜æ•°æ®
        df.to_csv(report_dir / "batch_test_results.csv", index=False)
        
        # ç”Ÿæˆå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # ååé‡ vs æ‰¹å¤„ç†å¤§å°
        pivot_throughput = df.pivot(index="batch_size", columns="batch_timeout", values="throughput")
        sns.heatmap(pivot_throughput, annot=True, fmt=".2f", ax=axes[0, 0])
        axes[0, 0].set_title("Throughput (requests/s)")
        axes[0, 0].set_ylabel("Batch Size")
        
        # å»¶è¿Ÿ vs æ‰¹å¤„ç†å¤§å°
        pivot_latency = df.pivot(index="batch_size", columns="batch_timeout", values="avg_latency")
        sns.heatmap(pivot_latency, annot=True, fmt=".3f", ax=axes[0, 1])
        axes[0, 1].set_title("Average Latency (s)")
        axes[0, 1].set_ylabel("Batch Size")
        
        # æˆåŠŸç‡ vs æ‰¹å¤„ç†å¤§å°
        pivot_success = df.pivot(index="batch_size", columns="batch_timeout", values="success_rate")
        sns.heatmap(pivot_success, annot=True, fmt=".2f", ax=axes[1, 0])
        axes[1, 0].set_title("Success Rate")
        axes[1, 0].set_ylabel("Batch Size")
        
        # é”™è¯¯æ•°é‡ vs æ‰¹å¤„ç†å¤§å°
        pivot_errors = df.pivot(index="batch_size", columns="batch_timeout", values="error_count")
        sns.heatmap(pivot_errors, annot=True, fmt="d", ax=axes[1, 1])
        axes[1, 1].set_title("Error Count")
        axes[1, 1].set_ylabel("Batch Size")
        
        plt.tight_layout()
        plt.savefig(report_dir / "batch_test_heatmaps.png", dpi=300, bbox_inches="tight")
        plt.close()
        
        # æ‰¾åˆ°æœ€ä¼˜é…ç½®
        best_config = df.loc[df["throughput"].idxmax()]
        best_config_info = {
            "best_batch_size": int(best_config["batch_size"]),
            "best_batch_timeout": float(best_config["batch_timeout"]),
            "best_throughput": float(best_config["throughput"]),
            "best_avg_latency": float(best_config["avg_latency"]),
            "best_success_rate": float(best_config["success_rate"])
        }
        
        with open(report_dir / "best_batch_config.json", "w") as f:
            json.dump(best_config_info, f, indent=2)
    
    def _generate_summary_report(self, report_dir: Path):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        summary = {
            "test_config": {
                "num_users": self.config.num_users,
                "test_duration": self.config.test_duration,
                "language_pairs": self.config.language_pairs,
                "batch_sizes_tested": self.config.test_batch_sizes,
                "batch_timeouts_tested": self.config.test_batch_timeouts
            },
            "test_timestamp": time.time(),
            "results_summary": {}
        }
        
        # ç”¨æˆ·æµ‹è¯•æ€»ç»“
        if self.results:
            successful_users = [u for u in self.results if u.status == "completed"]
            summary["results_summary"]["user_test"] = {
                "total_users": len(self.results),
                "successful_users": len(successful_users),
                "success_rate": len(successful_users) / len(self.results)
            }
        
        # æ‰¹å¤„ç†æµ‹è¯•æ€»ç»“
        if self.batch_results:
            best_throughput = max(r.throughput for r in self.batch_results)
            best_config = next(r for r in self.batch_results if r.throughput == best_throughput)
            
            summary["results_summary"]["batch_test"] = {
                "configurations_tested": len(self.batch_results),
                "best_throughput": best_throughput,
                "best_batch_size": best_config.batch_size,
                "best_batch_timeout": best_config.batch_timeout
            }
        
        with open(report_dir / "summary.json", "w") as f:
            json.dump(summary, f, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(report_dir, summary)
    
    def _generate_markdown_report(self, report_dir: Path, summary: Dict[str, Any]):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        markdown_content = f"""# Ray-based InfiniSST Evaluation Report

## Test Configuration
- **Number of Users**: {summary['test_config']['num_users']}
- **Test Duration**: {summary['test_config']['test_duration']} seconds
- **Language Pairs**: {', '.join(summary['test_config']['language_pairs'])}
- **Batch Sizes Tested**: {summary['test_config']['batch_sizes_tested']}
- **Batch Timeouts Tested**: {summary['test_config']['batch_timeouts_tested']} ms

## Results Summary

### User Test Results
"""
        
        if "user_test" in summary["results_summary"]:
            user_results = summary["results_summary"]["user_test"]
            markdown_content += f"""
- **Total Users**: {user_results['total_users']}
- **Successful Users**: {user_results['successful_users']}
- **Success Rate**: {user_results['success_rate']:.2%}
"""
        
        if "batch_test" in summary["results_summary"]:
            batch_results = summary["results_summary"]["batch_test"]
            markdown_content += f"""
### Batch Test Results
- **Configurations Tested**: {batch_results['configurations_tested']}
- **Best Throughput**: {batch_results['best_throughput']:.2f} requests/s
- **Optimal Batch Size**: {batch_results['best_batch_size']}
- **Optimal Batch Timeout**: {batch_results['best_batch_timeout']} ms
"""
        
        markdown_content += f"""
## Files Generated
- `user_test_stats.json` - Detailed user test statistics
- `user_test_charts.png` - User test visualization
- `batch_test_results.csv` - Batch test raw data
- `batch_test_heatmaps.png` - Batch test performance heatmaps
- `best_batch_config.json` - Optimal batch configuration
- `summary.json` - Complete test summary

## Test Timestamp
{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(summary['test_timestamp']))}
"""
        
        with open(report_dir / "README.md", "w") as f:
            f.write(markdown_content)

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Ray InfiniSST Evaluation Framework")
    parser.add_argument("--server-url", default="http://localhost:8000", help="API server URL")
    parser.add_argument("--num-users", type=int, default=8, help="Number of concurrent users")
    parser.add_argument("--test-duration", type=int, default=120, help="Test duration in seconds")
    parser.add_argument("--output-dir", default="ray_evaluation_results", help="Output directory")
    parser.add_argument("--basic-test-only", action="store_true", help="Run only basic functionality test")
    parser.add_argument("--batch-test-only", action="store_true", help="Run only batch performance test")
    parser.add_argument("--skip-basic-test", action="store_true", help="Skip basic functionality test")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = RayTestConfig(
        server_url=args.server_url,
        websocket_url=args.server_url.replace("http://", "ws://").replace("https://", "wss://"),
        num_users=args.num_users,
        test_duration=args.test_duration,
        output_dir=args.output_dir
    )
    
    # åˆ›å»ºè¯„ä¼°æ¡†æ¶
    framework = RayEvaluationFramework(config)
    
    logger.info(f"ğŸš€ å¼€å§‹Ray InfiniSSTè¯„ä¼°æµ‹è¯•")
    logger.info(f"æœåŠ¡å™¨URL: {config.server_url}")
    logger.info(f"å¹¶å‘ç”¨æˆ·æ•°: {config.num_users}")
    logger.info(f"æµ‹è¯•æ—¶é•¿: {config.test_duration}ç§’")
    
    try:
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        if not args.skip_basic_test and not args.batch_test_only:
            basic_test_passed = await framework.run_basic_functionality_test()
            if not basic_test_passed:
                logger.error("åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢è¯„ä¼°")
                return
        
        if args.basic_test_only:
            logger.info("ä»…è¿è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•ï¼Œå®Œæˆ")
            return
        
        # æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•
        if not args.basic_test_only:
            await framework.run_batch_performance_test()
        
        # å¹¶å‘ç”¨æˆ·æµ‹è¯•
        if not args.batch_test_only and not args.basic_test_only:
            await framework.run_concurrent_users_test()
        
        # ç”ŸæˆæŠ¥å‘Š
        framework.generate_report()
        
        logger.info("âœ… Ray InfiniSSTè¯„ä¼°æµ‹è¯•å®Œæˆ")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        logger.error(f"è¯„ä¼°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main()) 