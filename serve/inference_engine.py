#!/usr/bin/env python3
"""
InfiniSST æ¨ç†å¼•æ“ - æ•´åˆç‰ˆ
è¿æ¥schedulerå’Œå®é™…çš„infinisst_fasteræ¨¡å‹ï¼Œå®ç°å¤šè¯·æ±‚å¹¶å‘æ¨ç†
"""

import asyncio
import logging
import time
import torch
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, Future
import threading
from queue import Queue, Empty

# è®¾ç½®logger
logger = logging.getLogger(__name__)

# å¯¼å…¥ç›¸å…³æ¨¡å—
try:
    from agents.infinisst_faster import InfiniSSTFaster
    INFINISST_AVAILABLE = True
except ImportError as e:
    logger.warning(f"InfiniSSTFasterä¸å¯ç”¨: {e}")
    logger.warning("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨ç†æ¨¡å¼")
    InfiniSSTFaster = None
    INFINISST_AVAILABLE = False
try:
    from agents.infinisst import S2TAgentStates
    AGENTS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"agents.infinisstä¸å¯ç”¨: {e}")
    # åˆ›å»ºå ä½ç¬¦ç±»
    class S2TAgentStates:
        def __init__(self, src_len=0, speech_cache=None, past_key_values=None, 
                     target_ids=None, segment_idx=0, translations_list=None):
            self.source = []
            self.target = []
            self.source_finished = False
            self.source_sample_rate = 16000
            self.src_len = src_len or 0
            self.speech_cache = speech_cache
            self.past_key_values = past_key_values
            self.target_ids = target_ids or []
            self.segment_idx = segment_idx or 0
            self.translations_list = translations_list or []
        
        def reset(self):
            self.source = []
            self.target = []
            self.source_finished = False
            self.src_len = 0
            self.speech_cache = None
            self.past_key_values = None
            self.target_ids = []
            self.segment_idx = 0
            self.translations_list = []
    
    AGENTS_AVAILABLE = False
    
from .scheduler import InferenceRequest, RequestStage

@dataclass
class EngineConfig:
    """æ¨ç†å¼•æ“é…ç½®"""
    max_concurrent_requests: int = 32
    gpu_memory_fraction: float = 0.8
    enable_beam_search: bool = True
    beam_size: int = 4
    max_new_tokens: int = 20
    temperature: float = 1.0
    top_p: float = 0.9

class InferenceEngine:
    """
    InfiniSSTæ¨ç†å¼•æ“
    è´Ÿè´£å®é™…çš„æ¨¡å‹æ¨ç†ï¼Œæ”¯æŒbatchå¤„ç†å’Œå¹¶å‘æ‰§è¡Œ
    """
    
    def __init__(self, 
                 model_args,
                 config: EngineConfig = None,
                 gpu_id: int = 0,
                 language_id: str = "en-zh"):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_args: æ¨¡å‹å‚æ•°
            config: å¼•æ“é…ç½®
            gpu_id: GPUè®¾å¤‡ID
            language_id: è¯­è¨€å¯¹ID (ä¾‹å¦‚ "en-zh")
        """
        self.gpu_id = gpu_id
        self.language_id = language_id
        self.device = f"cuda:{gpu_id}"
        self.config = config or EngineConfig()
        
        # åˆ›å»ºå®Œæ•´çš„æ¨¡å‹å‚æ•°é…ç½®
        self.model_args = self._create_model_args(model_args, language_id)
        
        # æ¨¡å‹å®ä¾‹
        self.model = None
        self.tokenizer = None
        
        # å¤„ç†é˜Ÿåˆ—å’Œçº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.processing_queue = Queue()
        
        # çŠ¶æ€ç®¡ç†
        self.is_loaded = False
        self.is_running = False
        self.stats = {
            'total_requests': 0,
            'completed_requests': 0,
            'failed_requests': 0,
            'total_tokens_generated': 0,
            'average_latency': 0.0
        }
        
        logger.info(f"æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼ŒGPU: {gpu_id}, è¯­è¨€å¯¹: {language_id}")
    
    def _create_model_args(self, base_args, language_id: str):
        """åˆ›å»ºå®Œæ•´çš„æ¨¡å‹å‚æ•°é…ç½®"""
        
        # ä¸ api.py ç›¸åŒçš„è¯­è¨€å¯¹å®šä¹‰
        LANGUAGE_PAIRS = {
            "English -> Chinese": ("English", "Chinese", "en", "zh"),
            "English -> Italian": ("English", "Italian", "en", "it"),
            "English -> German": ("English", "German", "en", "de"),
            "English -> Spanish": ("English", "Spanish", "en", "es"),
        }
        
        # æ¨¡å‹è·¯å¾„å®šä¹‰ï¼ˆä¸ api.py ä¿æŒä¸€è‡´ï¼‰
        model_path_de = "/mnt/aries/data6/xixu/demo/en-de/pytorch_model.bin"
        model_path_es = "/mnt/aries/data6/xixu/demo/en-es/pytorch_model.bin"
        model_path = "/mnt/aries/data6/jiaxuanluo/demo/{}-{}/pytorch_model.bin"
        lora_path = "/mnt/aries/data6/jiaxuanluo/demo/{}-{}/lora.bin"
        
        # è§£æè¯­è¨€å¯¹ï¼ˆä¸ api.py ä¸­çš„é€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
        if language_id in LANGUAGE_PAIRS:
            source_lang, target_lang, src_code, tgt_code = LANGUAGE_PAIRS[language_id]
        else:
            # é»˜è®¤é…ç½®
            source_lang, target_lang, src_code, tgt_code = "English", "Chinese", "en", "zh"
        
        # æ¡ä»¶æ€§æ¨¡å‹å’ŒLoRAåŠ è½½ï¼ˆä¸ api.py é€»è¾‘ä¸€è‡´ï¼‰
        if language_id == "English -> German":
            state_dict_path = model_path_de
            lora_path_final = None
        elif language_id == "English -> Spanish":
            state_dict_path = model_path_es
            lora_path_final = None
        else:
            state_dict_path = model_path.format(src_code, tgt_code)
            lora_path_final = lora_path.format(src_code, tgt_code)
        
        # é»˜è®¤å‚æ•°é…ç½®ï¼ˆä¸ api.sh ä¸­çš„å‚æ•°å®Œå…¨ä¸€è‡´ï¼‰
        default_args = {
            # åŸºç¡€æ¨¡å‹
            'model_name': '/mnt/aries/data6/jiaxuanluo/Qwen2.5-7B-Instruct',
            
            # è¯­éŸ³ç¼–ç å™¨
            'w2v2_path': '/mnt/aries/data6/xixu/demo/wav2_vec_vox_960h_pl.pt',
            'w2v2_type': 'w2v2',
            'ctc_finetuned': True,
            
            # æ¨¡å‹é…ç½®
            'model_type': 'w2v2_qwen25',
            'length_shrink_cfg': "[(1024,2,2)] * 2",
            'block_size': 48,
            'max_cache_size': 576,
            'rope': 1,
            'audio_normalize': 0,
            
            # Stage1/Stage2 æ¨¡å‹è·¯å¾„ï¼ˆåŠ¨æ€è®¾ç½®ï¼‰
            'state_dict_path': state_dict_path,
            
            # LoRAé…ç½®ï¼ˆåŠ¨æ€è®¾ç½®ï¼‰
            'lora_path': lora_path_final,
            'lora_rank': 32,
            
            # ç¼“å­˜é…ç½®
            'max_llm_cache_size': 1000,
            'always_cache_system_prompt': True,
            
            # ç”Ÿæˆå‚æ•°ï¼ˆä¸ api.sh ä¸€è‡´ï¼‰
            'max_len_a': 10,
            'max_len_b': 20,
            'max_new_tokens': 10,
            'beam': 4,
            'repetition_penalty': 1.2,
            'length_penalty': 1.0,
            
            # è¿è¡Œå‚æ•°
            'pseudo_batch_size': 1,
            'min_start_sec': 0,
            'latency_multiplier': 2,
            'max_latency_multiplier': 4,
            
            # ç”Ÿæˆæ§åˆ¶å‚æ•°ï¼ˆä¸ api.sh ä¸€è‡´ï¼‰
            'no_repeat_ngram_size': 5,
            'no_repeat_ngram_lookback': '100d',
            'suppress_non_language': True,
            'do_sample': False,
            'top_p': 0.9,
            'top_k': 50,
            'epsilon_cutoff': 0.0,
            'temperature': 1.0,
            'dpo_sampling': False,
            
            # è¯­è¨€é…ç½®ï¼ˆåŠ¨æ€è®¾ç½®ï¼‰
            'source_lang': source_lang,
            'target_lang': target_lang
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„å‚æ•°
        final_args = {**default_args, **(base_args or {})}
        
        # åˆ›å»ºä¸€ä¸ªç±»ä¼¼äºargparse.Namespaceçš„å¯¹è±¡
        class ModelArgs:
            def __init__(self, **kwargs):
                for key, value in kwargs.items():
                    setattr(self, key, value)
        
        return ModelArgs(**final_args)
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹åˆ°GPU {self.gpu_id}...")
            
            if not INFINISST_AVAILABLE:
                logger.warning("InfiniSSTFasterä¸å¯ç”¨ï¼Œè·³è¿‡å®é™…æ¨¡å‹åŠ è½½")
                self.model = None
                self.tokenizer = None
                self.is_loaded = False
                return False
            
            # åˆ›å»ºInfiniSSTFasterå®ä¾‹
            self.model = InfiniSSTFaster(self.model_args)
            self.tokenizer = self.model.tokenizer
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if hasattr(self.model.model, 'to'):
                self.model.model = self.model.model.to(self.device)
            
            self.is_loaded = True
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒGPU: {self.gpu_id}")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.is_loaded = False
            return False
    
    def start(self):
        """å¯åŠ¨æ¨ç†å¼•æ“"""
        if not self.is_loaded:
            logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•å¯åŠ¨å¼•æ“")
            return False
        
        if self.is_running:
            logger.warning("æ¨ç†å¼•æ“å·²åœ¨è¿è¡Œ")
            return True
        
        self.is_running = True
        logger.info(f"æ¨ç†å¼•æ“å·²å¯åŠ¨ï¼ŒGPU: {self.gpu_id}")
        return True
    
    def stop(self):
        """åœæ­¢æ¨ç†å¼•æ“"""
        self.is_running = False
        self.executor.shutdown(wait=True)
        logger.info(f"æ¨ç†å¼•æ“å·²åœæ­¢ï¼ŒGPU: {self.gpu_id}")
    
    def process_batch(self, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """
        æ‰¹å¤„ç†æ¨ç†è¯·æ±‚
        
        Args:
            requests: æ¨ç†è¯·æ±‚åˆ—è¡¨
            
        Returns:
            æ¨ç†ç»“æœåˆ—è¡¨
        """
        if not self.is_running or not self.is_loaded:
            raise RuntimeError("æ¨ç†å¼•æ“æœªè¿è¡Œæˆ–æ¨¡å‹æœªåŠ è½½")
        
        start_time = time.time()
        results = []
        
        try:
            # æŒ‰é˜¶æ®µåˆ†ç»„å¤„ç†
            prefill_requests = [r for r in requests if r.stage == RequestStage.PREFILL]
            decode_requests = [r for r in requests if r.stage == RequestStage.DECODE]
            
            # å¤„ç†prefillè¯·æ±‚
            if prefill_requests:
                prefill_results = self._process_prefill_batch(prefill_requests)
                results.extend(prefill_results)
            
            # å¤„ç†decodeè¯·æ±‚
            if decode_requests:
                decode_results = self._process_decode_batch(decode_requests)
                results.extend(decode_results)
            
            # ğŸ”¥ é‡è¦ï¼šå¤„ç†å®Œæˆåæ£€æŸ¥å¹¶æ¸…ç†ç»“æŸçš„session
            self._cleanup_finished_sessions(requests, results)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['completed_requests'] += len(results)
            self.stats['total_requests'] += len(requests)
            
            latency = time.time() - start_time
            self.stats['average_latency'] = (
                self.stats['average_latency'] * (self.stats['completed_requests'] - len(results)) + 
                latency * len(results)
            ) / self.stats['completed_requests']
            
            logger.debug(f"æ‰¹å¤„ç†å®Œæˆ: {len(requests)}ä¸ªè¯·æ±‚, è€—æ—¶: {latency:.3f}s")
            
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†å¤±è´¥: {e}")
            self.stats['failed_requests'] += len(requests)
            # è¿”å›é”™è¯¯ç»“æœ
            results = [
                {
                    'request_id': req.request_id,
                    'success': False,
                    'error': str(e),
                    'generated_text': '',
                    'generated_tokens': []
                }
                for req in requests
            ]
        
        return results
    
    def _process_prefill_batch(self, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """å¤„ç†prefillé˜¶æ®µçš„è¯·æ±‚"""
        results = []
        
        for request in requests:
            try:
                # åˆ›å»ºagentçŠ¶æ€
                states = self._create_agent_states(request)
                
                # æ‰§è¡Œæ¨ç†
                action = self.model.policy(states)
                
                # å¤„ç†ç»“æœ
                result = self._process_action(request, action, states)
                results.append(result)
                
            except Exception as e:
                logger.error(f"Prefillå¤„ç†å¤±è´¥ (request_id: {request.request_id}): {e}")
                results.append({
                    'request_id': request.request_id,
                    'success': False,
                    'error': str(e),
                    'generated_text': '',
                    'generated_tokens': []
                })
        
        return results
    
    def _process_decode_batch(self, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """å¤„ç†decodeé˜¶æ®µçš„è¯·æ±‚"""
        results = []
        
        for request in requests:
            try:
                # åˆ›å»ºagentçŠ¶æ€
                states = self._create_agent_states(request)
                
                # æ‰§è¡Œæ¨ç†
                action = self.model.policy(states)
                
                # å¤„ç†ç»“æœ
                result = self._process_action(request, action, states)
                results.append(result)
                
            except Exception as e:
                logger.error(f"Decodeå¤„ç†å¤±è´¥ (request_id: {request.request_id}): {e}")
                results.append({
                    'request_id': request.request_id,
                    'success': False,
                    'error': str(e),
                    'generated_text': '',
                    'generated_tokens': []
                })
        
        return results
    
    def _create_agent_states(self, request: InferenceRequest) -> S2TAgentStates:
        """ä»æ¨ç†è¯·æ±‚åˆ›å»ºagentçŠ¶æ€"""
        # é¦–å…ˆå¤„ç†speech_batchï¼Œç¡®ä¿ç»´åº¦æ­£ç¡®
        speech_data = request.speech_batch
        
        print(f"ğŸ” [INFERENCE] Processing speech_data type: {type(speech_data)}, shape: {speech_data.shape if hasattr(speech_data, 'shape') else 'no shape'}")
        
        if isinstance(speech_data, torch.Tensor):
            # ç¡®ä¿æ˜¯1Dç”¨äºagent states
            if speech_data.dim() == 2:
                # [batch_size, seq_len] -> [seq_len] (å–ç¬¬ä¸€ä¸ªbatch)
                speech_list = speech_data[0].cpu().numpy().tolist()
                print(f"ğŸ” [INFERENCE] Converted 2D tensor to 1D: {len(speech_list)} samples")
            else:
                # [seq_len] -> ç›´æ¥è½¬æ¢
                speech_list = speech_data.cpu().numpy().tolist()
                print(f"ğŸ” [INFERENCE] Converted 1D tensor: {len(speech_list)} samples")
        else:
            # å·²ç»æ˜¯listæˆ–numpy array
            if isinstance(speech_data, np.ndarray):
                if speech_data.ndim == 2:
                    speech_list = speech_data[0].tolist()  # å–ç¬¬ä¸€ä¸ªbatch
                    print(f"ğŸ” [INFERENCE] Converted 2D numpy to 1D: {len(speech_list)} samples")
                else:
                    speech_list = speech_data.tolist()
                    print(f"ğŸ” [INFERENCE] Converted 1D numpy: {len(speech_list)} samples")
            else:
                speech_list = speech_data if isinstance(speech_data, list) else [speech_data]
                print(f"ğŸ” [INFERENCE] Used as list: {len(speech_list)} samples")
        
        # æ£€æŸ¥éŸ³é¢‘æ•°æ®é•¿åº¦
        speech_length = len(speech_list)
        print(f"ğŸ” [INFERENCE] Final speech data length: {speech_length}")
        
        # å¦‚æœéŸ³é¢‘æ•°æ®å¤ªçŸ­ï¼Œè®°å½•è­¦å‘Šä½†ä¸å¡«å……ï¼Œè®©æ¨¡å‹å¤„ç†
        MIN_AUDIO_LENGTH = 160  # 0.01ç§’ @ 16kHzï¼Œæ›´å®½æ¾çš„é˜ˆå€¼
        if speech_length == 0:
            print(f"âš ï¸ [INFERENCE] Received empty speech data for request {request.request_id}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©åç»­å¤„ç†å†³å®šå¦‚ä½•å¤„ç†
        elif speech_length < MIN_AUDIO_LENGTH:
            print(f"âš ï¸ [INFERENCE] Speech data very short ({speech_length} samples) for request {request.request_id}")
        
        if speech_list:
            print(f"ğŸ” [INFERENCE] Audio stats: min={min(speech_list):.6f}, max={max(speech_list):.6f}")
        else:
            print(f"ğŸ” [INFERENCE] Audio list is empty!")
        
        # å¤„ç†input_idsï¼Œç¡®ä¿æ˜¯listæ ¼å¼
        if isinstance(request.input_ids, torch.Tensor):
            if request.input_ids.dim() == 2:
                # [batch_size, seq_len] -> [seq_len] (å–ç¬¬ä¸€ä¸ªbatch)
                input_ids_list = request.input_ids[0].cpu().numpy().tolist()
            else:
                # [seq_len] -> ç›´æ¥è½¬æ¢
                input_ids_list = request.input_ids.cpu().numpy().tolist()
        else:
            input_ids_list = request.input_ids if request.input_ids else []
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šsrc_lenåº”è¯¥æ˜¯å·²å¤„ç†çš„éŸ³é¢‘é•¿åº¦ï¼Œä¸æ˜¯å½“å‰ç‰‡æ®µé•¿åº¦
        # å¯¹äºæ–°è¯·æ±‚ï¼Œsrc_lenåº”è¯¥æ˜¯0ï¼›å¯¹äºåç»­è¯·æ±‚ï¼Œåº”è¯¥ä»sessionçŠ¶æ€è·å–
        current_src_len = getattr(request, 'session_src_len', 0)  # ä»requestè·å–ä¼šè¯çŠ¶æ€
        
        # ğŸ” ä½¿ç”¨ä¼šè¯ä¼ é€’çš„ src_len å€¼ï¼Œè¿™æ ·æ¨¡å‹çš„ _prepare_speech èƒ½æ­£ç¡®å¤„ç†å¢é‡æ•°æ®
        print(f"ğŸ” [INFERENCE] Using session src_len: {current_src_len} (already processed samples)")
        print(f"ğŸ” [INFERENCE] Total audio samples: {speech_length}")
        print(f"ğŸ” [INFERENCE] New samples to process: {speech_length - current_src_len}")
        
        states = S2TAgentStates(
            src_len=current_src_len,  # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ä¼šè¯çš„å·²å¤„ç†é•¿åº¦
            speech_cache=request.speech_cache,
            past_key_values=request.past_key_values,
            target_ids=input_ids_list,
            segment_idx=getattr(request, 'segment_idx', 0),
            translations_list=getattr(request, 'translations_list', [])
        )
        
        # è®¾ç½®éŸ³é¢‘æ•°æ® - ç¡®ä¿æ˜¯1D list
        states.source = speech_list
        states.source_sample_rate = 16000  # é»˜è®¤é‡‡æ ·ç‡
        states.source_finished = (request.stage == RequestStage.DECODE)
        
        print(f"ğŸ” [INFERENCE] Created agent states with {len(states.source)} audio samples")
        print(f"ğŸ” [INFERENCE] states.source type: {type(states.source)}")
        print(f"ğŸ” [INFERENCE] states.src_len: {states.src_len}")
        
        return states
    
    def _process_action(self, request: InferenceRequest, action, states: S2TAgentStates) -> Dict[str, Any]:
        """å¤„ç†æ¨¡å‹è¾“å‡ºçš„action"""
        from simuleval.agents.actions import WriteAction, ReadAction
        
        result = {
            'request_id': request.request_id,
            'success': True,
            'generated_text': '',
            'generated_tokens': [],
            'finished': False,
            'speech_cache': states.speech_cache,
            'past_key_values': states.past_key_values
        }
        
        if isinstance(action, WriteAction):
            result['generated_text'] = action.content
            result['finished'] = action.finished
            
            # å¦‚æœæœ‰æ–°çš„tokenï¼Œæ·»åŠ åˆ°ç»Ÿè®¡ä¸­
            if action.content:
                # ç®€å•çš„tokenè®¡æ•°ï¼ˆå®é™…åº”è¯¥ç”¨tokenizerï¼‰
                token_count = len(action.content.split())
                self.stats['total_tokens_generated'] += token_count
                result['generated_tokens'] = list(range(token_count))  # å ä½ç¬¦
        
        elif isinstance(action, ReadAction):
            result['generated_text'] = ''
            result['finished'] = False
        
        return result
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'gpu_id': self.gpu_id,
            'is_loaded': self.is_loaded,
            'is_running': self.is_running,
            'stats': self.stats.copy(),
            'config': {
                'max_concurrent_requests': self.config.max_concurrent_requests,
                'beam_size': self.config.beam_size,
                'max_new_tokens': self.config.max_new_tokens
            }
        }
    
    def _cleanup_finished_sessions(self, requests: List[InferenceRequest], results: List[Dict[str, Any]]):
        """æ¸…ç†å·²ç»“æŸçš„sessionçš„KV cacheé¡µé¢"""
        try:
            for i, request in enumerate(requests):
                if i < len(results):
                    result = results[i]
                    
                    # æ£€æŸ¥sessionæ˜¯å¦ç»“æŸï¼ˆç¿»è¯‘å®Œæˆæˆ–å‡ºé”™ï¼‰
                    session_finished = (
                        not result.get('success', False) or  # å‡ºé”™äº†
                        result.get('finished', False) or     # æ˜ç¡®æ ‡è®°å®Œæˆ
                        getattr(request, 'is_final', False)   # æ˜¯æœ€åä¸€ä¸ªè¯·æ±‚
                    )
                    
                    if session_finished:
                        logger.info(f"ğŸ§¹ Sessionç»“æŸï¼Œå¼€å§‹æ¸…ç†KV cacheé¡µé¢: {request.request_id}")
                        self._cleanup_session_kv_cache(request)
                        
        except Exception as e:
            logger.error(f"æ¸…ç†sessionæ—¶å‡ºé”™: {e}")
    
    def _cleanup_session_kv_cache(self, request: InferenceRequest):
        """æ¸…ç†å•ä¸ªsessionçš„KV cacheé¡µé¢"""
        try:
            # è¿™é‡Œéœ€è¦è®¿é—®å…·ä½“çš„KV cacheæ•°æ®ç»“æ„
            # å‡è®¾requestä¸­åŒ…å«äº†KV cacheçš„å¼•ç”¨
            
            session_id = getattr(request, 'session_id', request.request_id)
            
            # ğŸ”¥ å…³é”®ï¼šé‡Šæ”¾speech cacheé¡µé¢
            if hasattr(request, 'speech_cache') and request.speech_cache:
                self._release_speech_cache_pages(request.speech_cache, session_id)
            
            # ğŸ”¥ å…³é”®ï¼šé‡Šæ”¾LLM KV cacheé¡µé¢
            if hasattr(request, 'past_key_values') and request.past_key_values:
                self._release_llm_cache_pages(request.past_key_values, session_id)
            
            logger.info(f"âœ… Session {session_id} KV cacheé¡µé¢æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†session {request.request_id} KV cacheæ—¶å‡ºé”™: {e}")
    
    def _release_speech_cache_pages(self, speech_cache, session_id: str):
        """é‡Šæ”¾speech cacheå ç”¨çš„é¡µé¢"""
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„speech cacheç»“æ„æ¥å®ç°
            # å‡è®¾speech_cacheåŒ…å«é¡µé¢ç´¢å¼•ä¿¡æ¯
            
            if hasattr(speech_cache, 'paged_kv_indices') and speech_cache.paged_kv_indices:
                pages_to_release = len(speech_cache.paged_kv_indices)
                
                # è°ƒç”¨é¡µé¢é‡Šæ”¾å‡½æ•°ï¼ˆéœ€è¦ä»flashinferå¼•æ“è·å–pagetableï¼‰
                if hasattr(self.model, 'speech_pagetable'):
                    pagetable = self.model.speech_pagetable
                    self._release_pages_to_pool(pagetable, speech_cache.paged_kv_indices, session_id, 'speech')
                    
                    # æ¸…ç©ºcacheä¸­çš„é¡µé¢å¼•ç”¨
                    speech_cache.paged_kv_indices = []
                    speech_cache.paged_kv_last_page_len = 16  # PAGE_SIZE
                    
                    logger.info(f"ğŸ”„ é‡Šæ”¾äº† {pages_to_release} ä¸ªspeech cacheé¡µé¢åˆ°é¡µé¢æ± ")
                    
        except Exception as e:
            logger.error(f"é‡Šæ”¾speech cacheé¡µé¢æ—¶å‡ºé”™: {e}")
    
    def _release_llm_cache_pages(self, past_key_values, session_id: str):
        """é‡Šæ”¾LLM KV cacheå ç”¨çš„é¡µé¢"""
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„past_key_valuesç»“æ„æ¥å®ç°
            
            if hasattr(past_key_values, 'paged_kv_indices') and past_key_values.paged_kv_indices:
                pages_to_release = len(past_key_values.paged_kv_indices)
                
                # åˆ†åˆ«å¤„ç†prefillå’Œdecode cache
                if hasattr(self.model, 'llm_prefill_pagetable'):
                    self._release_pages_to_pool(self.model.llm_prefill_pagetable, 
                                              past_key_values.paged_kv_indices, 
                                              session_id, 'llm_prefill')
                
                if hasattr(self.model, 'llm_decode_pagetable'):
                    self._release_pages_to_pool(self.model.llm_decode_pagetable, 
                                              past_key_values.paged_kv_indices, 
                                              session_id, 'llm_decode')
                
                # æ¸…ç©ºcacheä¸­çš„é¡µé¢å¼•ç”¨
                past_key_values.paged_kv_indices = []
                past_key_values.paged_kv_last_page_len = 16  # PAGE_SIZE
                
                logger.info(f"ğŸ”„ é‡Šæ”¾äº† {pages_to_release} ä¸ªLLM cacheé¡µé¢åˆ°é¡µé¢æ± ")
                
        except Exception as e:
            logger.error(f"é‡Šæ”¾LLM cacheé¡µé¢æ—¶å‡ºé”™: {e}")
    
    def _release_pages_to_pool(self, pagetable, page_indices: list, session_id: str, cache_type: str):
        """å°†é¡µé¢é‡Šæ”¾å›é¡µé¢æ± """
        try:
            if not page_indices:
                return
            
            import torch
            
            # å‡å°‘é¡µé¢å¼•ç”¨è®¡æ•°
            page_indices_tensor = torch.tensor(page_indices, dtype=torch.long)
            pagetable.page_cnt[page_indices_tensor] -= 1
            
            # æ‰¾å‡ºå¼•ç”¨è®¡æ•°ä¸º0çš„é¡µé¢ï¼ˆå¯ä»¥è¢«é‡Šæ”¾ï¼‰
            free_mask = pagetable.page_cnt[page_indices_tensor] == 0
            free_pages = page_indices_tensor[free_mask]
            
            if len(free_pages) > 0:
                # å°†é¡µé¢æ”¾å›å¯ç”¨é˜Ÿåˆ—
                free_pages_list = free_pages.tolist()
                pagetable.paged_queue.extend(free_pages_list)
                
                logger.info(f"ğŸ”„ [{cache_type}] Session {session_id} é‡Šæ”¾äº† {len(free_pages_list)} ä¸ªé¡µé¢å›é¡µé¢æ± ")
                logger.info(f"ğŸ”„ [{cache_type}] é¡µé¢æ± ç°åœ¨æœ‰ {len(pagetable.paged_queue)} ä¸ªå¯ç”¨é¡µé¢")
                
                # ğŸ” è¯¦ç»†è®°å½•é¡µé¢ä½¿ç”¨æƒ…å†µ
                total_pages = len(pagetable.page_cnt)
                used_pages = torch.sum(pagetable.page_cnt > 0).item()
                logger.info(f"ğŸ“Š [{cache_type}] é¡µé¢ä½¿ç”¨ç»Ÿè®¡: {used_pages}/{total_pages} é¡µè¢«ä½¿ç”¨")
            else:
                logger.warning(f"âš ï¸ [{cache_type}] Session {session_id} çš„ {len(page_indices)} ä¸ªé¡µé¢ä»è¢«å…¶ä»–sessionå¼•ç”¨")
                
        except Exception as e:
            logger.error(f"é‡Šæ”¾é¡µé¢åˆ°æ± æ—¶å‡ºé”™: {e}")
    
    def force_cleanup_all_sessions(self):
        """å¼ºåˆ¶æ¸…ç†æ‰€æœ‰sessionçš„KV cacheï¼ˆç´§æ€¥æƒ…å†µä½¿ç”¨ï¼‰"""
        try:
            logger.warning("ğŸš¨ å¼ºåˆ¶æ¸…ç†æ‰€æœ‰sessionçš„KV cacheé¡µé¢")
            
            # é‡ç½®æ‰€æœ‰é¡µé¢æ± åˆ°åˆå§‹çŠ¶æ€
            if hasattr(self.model, 'speech_pagetable'):
                self._reset_pagetable(self.model.speech_pagetable, 'speech')
            
            if hasattr(self.model, 'llm_prefill_pagetable'):
                self._reset_pagetable(self.model.llm_prefill_pagetable, 'llm_prefill')
            
            if hasattr(self.model, 'llm_decode_pagetable'):
                self._reset_pagetable(self.model.llm_decode_pagetable, 'llm_decode')
            
            logger.info("âœ… å¼ºåˆ¶æ¸…ç†å®Œæˆï¼Œæ‰€æœ‰é¡µé¢å·²é‡ç½®")
            
        except Exception as e:
            logger.error(f"å¼ºåˆ¶æ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _reset_pagetable(self, pagetable, cache_type: str):
        """é‡ç½®é¡µé¢è¡¨åˆ°åˆå§‹çŠ¶æ€"""
        try:
            total_pages = len(pagetable.page_cnt)
            
            # é‡ç½®é¡µé¢å¼•ç”¨è®¡æ•°
            pagetable.page_cnt.zero_()
            
            # é‡å»ºå¯ç”¨é¡µé¢é˜Ÿåˆ—
            pagetable.paged_queue = list(range(total_pages))
            
            logger.info(f"ğŸ”„ [{cache_type}] é¡µé¢è¡¨å·²é‡ç½®: {total_pages} ä¸ªé¡µé¢å…¨éƒ¨å¯ç”¨")
            
        except Exception as e:
            logger.error(f"é‡ç½®é¡µé¢è¡¨æ—¶å‡ºé”™: {e}")

class MultiGPUInferenceEngine:
    """
    å¤šGPUæ¨ç†å¼•æ“ç®¡ç†å™¨
    ç®¡ç†å¤šä¸ªGPUä¸Šçš„æ¨ç†å¼•æ“å®ä¾‹
    """
    
    def __init__(self, gpu_language_map: Dict[int, str], model_args_map: Dict[int, Any] = None):
        """
        åˆå§‹åŒ–å¤šGPUæ¨ç†å¼•æ“
        
        Args:
            gpu_language_map: GPUåˆ°è¯­è¨€å¯¹çš„æ˜ å°„
            model_args_map: GPUåˆ°æ¨¡å‹å‚æ•°çš„æ˜ å°„ï¼ˆå¯é€‰ï¼‰
        """
        self.gpu_language_map = gpu_language_map
        self.model_args_map = model_args_map or {}
        
        # åˆ›å»ºå¼•æ“å®ä¾‹
        self.engines: Dict[int, InferenceEngine] = {}
        for gpu_id, language_pair in gpu_language_map.items():
            model_args = self.model_args_map.get(gpu_id, {})
            
            engine = InferenceEngine(
                model_args=model_args,
                gpu_id=gpu_id,
                language_id=language_pair
            )
            self.engines[gpu_id] = engine
        
        logger.info(f"å¤šGPUæ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒGPU: {list(self.engines.keys())}")
    
    def load_all_models(self) -> bool:
        """åŠ è½½æ‰€æœ‰GPUä¸Šçš„æ¨¡å‹"""
        success = True
        for gpu_id, engine in self.engines.items():
            if not engine.load_model():
                success = False
                logger.error(f"GPU {gpu_id} æ¨¡å‹åŠ è½½å¤±è´¥")
        return success
    
    def start_all(self):
        """å¯åŠ¨æ‰€æœ‰æ¨ç†å¼•æ“"""
        for engine in self.engines.values():
            engine.start()
    
    def stop_all(self):
        """åœæ­¢æ‰€æœ‰æ¨ç†å¼•æ“"""
        for engine in self.engines.values():
            engine.stop()
    
    def get_engine(self, gpu_id: int) -> Optional[InferenceEngine]:
        """è·å–æŒ‡å®šGPUçš„æ¨ç†å¼•æ“"""
        return self.engines.get(gpu_id)
    
    def process_batch(self, gpu_id: int, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """åœ¨æŒ‡å®šGPUä¸Šå¤„ç†æ‰¹è¯·æ±‚"""
        engine = self.get_engine(gpu_id)
        if not engine:
            raise ValueError(f"GPU {gpu_id} ä¸Šæ²¡æœ‰å¯ç”¨çš„æ¨ç†å¼•æ“")
        
        return engine.process_batch(requests)
    
    def get_all_stats(self) -> Dict[int, Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¼•æ“çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {gpu_id: engine.get_stats() for gpu_id, engine in self.engines.items()} 