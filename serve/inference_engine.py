#!/usr/bin/env python3
"""
InfiniSST æ¨ç†å¼•æ“ - æ•´åˆç‰ˆ
è¿æ¥schedulerå’Œå®é™…çš„infinisst_fasteræ¨¡å‹ï¼Œå®ç°å¤šè¯·æ±‚å¹¶å‘æ¨ç†
"""

import asyncio
import logging
import time
import torch
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, Future
import threading
from queue import Queue, Empty

# è®¾ç½®logger
logger = logging.getLogger(__name__)

# å¯¼å…¥ç›¸å…³æ¨¡å—
import sys
import os
# å°†agentsç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from agents.infinisst_faster import InfiniSSTFaster
    INFINISST_AVAILABLE = True
except ImportError as e:
    logger.warning(f"InfiniSSTFasterä¸å¯ç”¨: {e}")
    logger.warning("å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨ç†æ¨¡å¼")
    InfiniSSTFaster = None
    INFINISST_AVAILABLE = False
try:
    from agents.infinisst import S2TAgentStates
    AGENTS_AVAILABLE = True
except ImportError as e:
    logger.warning(f"agents.infinisstä¸å¯ç”¨: {e}")
    # åˆ›å»ºå ä½ç¬¦ç±»
    class S2TAgentStates:
        def __init__(self, src_len=0, speech_cache=None, past_key_values=None, 
                     target_ids=None, segment_idx=0, translations_list=None):
            self.source = []
            self.target = []
            self.source_finished = False
            self.source_sample_rate = 16000
            self.src_len = src_len or 0
            self.speech_cache = speech_cache
            self.past_key_values = past_key_values
            self.target_ids = target_ids or []
            self.segment_idx = segment_idx or 0
            self.translations_list = translations_list or []
        
        def reset(self):
            self.source = []
            self.target = []
            self.source_finished = False
            self.src_len = 0
            self.speech_cache = None
            self.past_key_values = None
            self.target_ids = []
            self.segment_idx = 0
            self.translations_list = []
    
    AGENTS_AVAILABLE = False
    
from scheduler import InferenceRequest, RequestStage

@dataclass
class EngineConfig:
    """æ¨ç†å¼•æ“é…ç½®"""
    max_concurrent_requests: int = 32
    gpu_memory_fraction: float = 0.8
    enable_beam_search: bool = True
    beam_size: int = 4
    max_new_tokens: int = 20
    temperature: float = 1.0
    top_p: float = 0.9

class InferenceEngine:
    """
    InfiniSSTæ¨ç†å¼•æ“
    è´Ÿè´£å®é™…çš„æ¨¡å‹æ¨ç†ï¼Œæ”¯æŒbatchå¤„ç†å’Œå¹¶å‘æ‰§è¡Œ
    """
    
    def __init__(self, 
                 model_args,
                 config: EngineConfig = None,
                 gpu_id: int = 0,
                 language_id: str = "en-zh"):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_args: æ¨¡å‹å‚æ•°
            config: å¼•æ“é…ç½®
            gpu_id: GPUè®¾å¤‡ID
            language_id: è¯­è¨€å¯¹ID (ä¾‹å¦‚ "en-zh")
        """
        self.gpu_id = gpu_id
        self.language_id = language_id
        self.device = f"cuda:{gpu_id}"
        self.config = config or EngineConfig()
        
        # åˆ›å»ºå®Œæ•´çš„æ¨¡å‹å‚æ•°é…ç½®
        self.model_args = self._create_model_args(model_args, language_id)
        
        # æ¨¡å‹å®ä¾‹
        self.model = None
        self.tokenizer = None
        
        # å¤„ç†é˜Ÿåˆ—å’Œçº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.processing_queue = Queue()
        
        # çŠ¶æ€ç®¡ç†
        self.is_loaded = False
        self.is_running = False
        self.stats = {
            'total_requests': 0,
            'completed_requests': 0,
            'failed_requests': 0,
            'total_tokens_generated': 0,
            'average_latency': 0.0
        }
        
        logger.info(f"æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼ŒGPU: {gpu_id}, è¯­è¨€å¯¹: {language_id}")
    
    def _create_model_args(self, base_args, language_id: str):
        """åˆ›å»ºå®Œæ•´çš„æ¨¡å‹å‚æ•°é…ç½®"""
        
        # ä¸ api.py ç›¸åŒçš„è¯­è¨€å¯¹å®šä¹‰
        LANGUAGE_PAIRS = {
            "English -> Chinese": ("English", "Chinese", "en", "zh"),
            "English -> Italian": ("English", "Italian", "en", "it"),
            "English -> German": ("English", "German", "en", "de"),
            "English -> Spanish": ("English", "Spanish", "en", "es"),
        }
        
        # æ¨¡å‹è·¯å¾„å®šä¹‰ï¼ˆä¸ api.py ä¿æŒä¸€è‡´ï¼‰
        model_path_de = "/mnt/aries/data6/xixu/demo/en-de/pytorch_model.bin"
        model_path_es = "/mnt/aries/data6/xixu/demo/en-es/pytorch_model.bin"
        model_path = "/mnt/aries/data6/jiaxuanluo/demo/{}-{}/pytorch_model.bin"
        lora_path = "/mnt/aries/data6/jiaxuanluo/demo/{}-{}/lora.bin"
        
        # è§£æè¯­è¨€å¯¹ï¼ˆä¸ api.py ä¸­çš„é€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
        if language_id in LANGUAGE_PAIRS:
            source_lang, target_lang, src_code, tgt_code = LANGUAGE_PAIRS[language_id]
        else:
            # é»˜è®¤é…ç½®
            source_lang, target_lang, src_code, tgt_code = "English", "Chinese", "en", "zh"
        
        # æ¡ä»¶æ€§æ¨¡å‹å’ŒLoRAåŠ è½½ï¼ˆä¸ api.py é€»è¾‘ä¸€è‡´ï¼‰
        if language_id == "English -> German":
            state_dict_path = model_path_de
            lora_path_final = None
        elif language_id == "English -> Spanish":
            state_dict_path = model_path_es
            lora_path_final = None
        else:
            state_dict_path = model_path.format(src_code, tgt_code)
            lora_path_final = lora_path.format(src_code, tgt_code)
        
        # é»˜è®¤å‚æ•°é…ç½®ï¼ˆä¸ api.sh ä¸­çš„å‚æ•°å®Œå…¨ä¸€è‡´ï¼‰
        default_args = {
            # åŸºç¡€æ¨¡å‹
            'model_name': '/mnt/aries/data6/jiaxuanluo/Qwen2.5-7B-Instruct',
            
            # è¯­éŸ³ç¼–ç å™¨
            'w2v2_path': '/mnt/aries/data6/xixu/demo/wav2_vec_vox_960h_pl.pt',
            'w2v2_type': 'w2v2',
            'ctc_finetuned': True,
            
            # æ¨¡å‹é…ç½®
            'model_type': 'w2v2_qwen25',
            'length_shrink_cfg': "[(1024,2,2)] * 2",
            'block_size': 48,
            'max_cache_size': 576,
            'rope': 1,
            'audio_normalize': 0,
            
            # Stage1/Stage2 æ¨¡å‹è·¯å¾„ï¼ˆåŠ¨æ€è®¾ç½®ï¼‰
            'state_dict_path': state_dict_path,
            
            # LoRAé…ç½®ï¼ˆåŠ¨æ€è®¾ç½®ï¼‰
            'lora_path': lora_path_final,
            'lora_rank': 32,
            
            # ç¼“å­˜é…ç½®
            'max_llm_cache_size': 1000,
            'always_cache_system_prompt': True,
            
            # ç”Ÿæˆå‚æ•°ï¼ˆä¸ api.sh ä¸€è‡´ï¼‰
            'max_len_a': 10,
            'max_len_b': 20,
            'max_new_tokens': 10,
            'beam': 4,
            'repetition_penalty': 1.2,
            'length_penalty': 1.0,
            
            # è¿è¡Œå‚æ•°
            'pseudo_batch_size': 1,
            'min_start_sec': 0,
            'latency_multiplier': 2,
            'max_latency_multiplier': 4,
            
            # ç”Ÿæˆæ§åˆ¶å‚æ•°ï¼ˆä¸ api.sh ä¸€è‡´ï¼‰
            'no_repeat_ngram_size': 5,
            'no_repeat_ngram_lookback': '100d',
            'suppress_non_language': True,
            'do_sample': False,
            'top_p': 0.9,
            'top_k': 50,
            'epsilon_cutoff': 0.0,
            'temperature': 1.0,
            'dpo_sampling': False,
            
            # è¯­è¨€é…ç½®ï¼ˆåŠ¨æ€è®¾ç½®ï¼‰
            'source_lang': source_lang,
            'target_lang': target_lang
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„å‚æ•°
        final_args = {**default_args, **(base_args or {})}
        
        # ğŸ”¥ ç¡®ä¿gpu_idä¼ é€’ç»™æ¨¡å‹å‚æ•°ï¼Œç”¨äºè®¾å¤‡ç»‘å®š
        final_args['gpu_id'] = self.gpu_id
        
        # åˆ›å»ºä¸€ä¸ªç±»ä¼¼äºargparse.Namespaceçš„å¯¹è±¡
        class ModelArgs:
            def __init__(self, **kwargs):
                for key, value in kwargs.items():
                    setattr(self, key, value)
        
        return ModelArgs(**final_args)
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹åˆ°GPU {self.gpu_id}...")
            
            if not INFINISST_AVAILABLE:
                logger.warning("InfiniSSTFasterä¸å¯ç”¨ï¼Œè·³è¿‡å®é™…æ¨¡å‹åŠ è½½")
                self.model = None
                self.tokenizer = None
                self.is_loaded = False
                return False
            
            # åˆ›å»ºInfiniSSTFasterå®ä¾‹
            self.model = InfiniSSTFaster(self.model_args)
            self.tokenizer = self.model.tokenizer
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if hasattr(self.model.model, 'to'):
                self.model.model = self.model.model.to(self.device)
            
            self.is_loaded = True
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒGPU: {self.gpu_id}")
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.is_loaded = False
            return False
    
    def start(self):
        """å¯åŠ¨æ¨ç†å¼•æ“"""
        if not self.is_loaded:
            logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•å¯åŠ¨å¼•æ“")
            return False
        
        if self.is_running:
            logger.warning("æ¨ç†å¼•æ“å·²åœ¨è¿è¡Œ")
            return True
        
        self.is_running = True
        logger.info(f"æ¨ç†å¼•æ“å·²å¯åŠ¨ï¼ŒGPU: {self.gpu_id}")
        return True
    
    def stop(self):
        """åœæ­¢æ¨ç†å¼•æ“"""
        self.is_running = False
        self.executor.shutdown(wait=True)
        logger.info(f"æ¨ç†å¼•æ“å·²åœæ­¢ï¼ŒGPU: {self.gpu_id}")
    
    def process_batch(self, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """
        æ‰¹å¤„ç†æ¨ç†è¯·æ±‚
        
        Args:
            requests: æ¨ç†è¯·æ±‚åˆ—è¡¨
            
        Returns:
            æ¨ç†ç»“æœåˆ—è¡¨
        """
        if not self.is_running or not self.is_loaded:
            raise RuntimeError("inference engine not running or model not loaded not running or model not loaded")
        
        start_time = time.time()
        results = []
        
        try:
            # æŒ‰é˜¶æ®µåˆ†ç»„å¤„ç†
            prefill_requests = [r for r in requests if r.stage.name == RequestStage.PREFILL.name]
            decode_requests = [r for r in requests if r.stage.name == RequestStage.DECODE.name]
            
            # å¤„ç†prefillè¯·æ±‚
            if prefill_requests:
                prefill_results = self._process_prefill_batch(prefill_requests)
                results.extend(prefill_results)
            
            # å¤„ç†decodeè¯·æ±‚
            if decode_requests:
                decode_results = self._process_decode_batch(decode_requests)
                results.extend(decode_results)
            
            # å¤„ç†å®Œæˆåæ£€æŸ¥å¹¶æ¸…ç†ç»“æŸçš„session
            self._cleanup_finished_sessions(requests, results)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats['completed_requests'] += len(results)
            self.stats['total_requests'] += len(requests)
            
            latency = time.time() - start_time
            self.stats['average_latency'] = (
                self.stats['average_latency'] * (self.stats['completed_requests'] - len(results)) + 
                latency * len(results)
            ) / self.stats['completed_requests']
            stage = requests[0].stage.value if requests else "unknown"
            prefill_count = len([r for r in requests if r.stage == RequestStage.PREFILL])
            decode_count = len([r for r in requests if r.stage == RequestStage.DECODE])
            logger.info(f"[IMPORTANT] æ‰¹å¤„ç†å®Œæˆ [{stage}]: {len(requests)} ä¸ªè¯·æ±‚ (Prefill:{prefill_count}, Decode:{decode_count}), è€—æ—¶: {latency*1000:.1f}ms")
            
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†å¤±è´¥: {e}")
            self.stats['failed_requests'] += len(requests)
            # è¿”å›é”™è¯¯ç»“æœ
            results = [
                {
                    'request_id': req.request_id,
                    'success': False,
                    'error': str(e),
                    'generated_text': '',
                    'generated_tokens': []
                }
                for req in requests
            ]
        
        return results
    
    def _process_prefill_batch(self, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """å¤„ç†prefillé˜¶æ®µçš„è¯·æ±‚ - ORCAé£æ ¼ï¼Œä¸€æ¬¡åªåšprefillæ­¥éª¤"""
        #todo :try
        # ğŸ”¥ ORCAæ¶æ„ï¼šä¸ºbatchä¸­çš„æ¯ä¸ªrequeståˆ†åˆ«æ„é€ beam_search.Request
        beam_requests = []
        for req in requests:
            beam_req = self._create_beam_request(req)
            beam_requests.append(beam_req)
        
        print(f"ğŸ” [ORCA-PREFILL] å¤„ç†batch: {len(beam_requests)} ä¸ªrequests")
        
        # ç›´æ¥è°ƒç”¨beam_searchçš„prefillå‡½æ•°
        from model.flashinfer.beam_search import prefill
        
        processed_requests, speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable = prefill(
            requests=beam_requests,
            model=self.model.model,  # ä½¿ç”¨å†…éƒ¨çš„æ¨¡å‹
            tokenizer=self.tokenizer,
            num_beams=self.config.beam_size,
            length_penalty=1.0,
            speech_pagetable=self.model.speech_pagetable,
            llm_prefill_pagetable=self.model.llm_prefill_pagetable,
            llm_decode_pagetable=self.model.llm_decode_pagetable
        )
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°pagetableçŠ¶æ€å¹¶éªŒè¯è¿ç»­æ€§
        self.model.speech_pagetable = speech_pagetable
        self.model.llm_prefill_pagetable = llm_prefill_pagetable
        self.model.llm_decode_pagetable = llm_decode_pagetable
        
        # éªŒè¯pagetableçŠ¶æ€
        self._verify_pagetable_consistency("Prefill", speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable)
        
        # è½¬æ¢ç»“æœå¹¶æ›´æ–°æ¯ä¸ªrequestçš„cacheå¼•ç”¨
        results = []
        for i, (orig_req, processed_req) in enumerate(zip(requests, processed_requests)):
            result = self._convert_beam_result_to_inference_result(orig_req, processed_req, is_prefill=True)
            
            # ğŸ”¥ ORCAå…³é”®ï¼šç«‹å³æ›´æ–°åŸå§‹requestçš„cacheå¼•ç”¨ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            # æ ¹æ®infinisst_faster.pyï¼Œcacheåº”è¯¥æ˜¯åˆ—è¡¨æ ¼å¼
            orig_req.speech_cache = [processed_req.speech_cache]  # è½¬æ¢ä¸ºåˆ—è¡¨
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šprefillå®Œæˆåï¼Œllm_cacheåº”è¯¥å·²ç»æ˜¯beam cacheåˆ—è¡¨
            # ä¸éœ€è¦å†åŒ…è£…ä¸€å±‚åˆ—è¡¨
            if isinstance(processed_req.llm_cache, list):
                # prefillè¿”å›çš„å·²ç»æ˜¯beam cacheåˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                orig_req.past_key_values = [processed_req.llm_cache]  # å¤–å±‚åˆ—è¡¨ç”¨äºsessionç®¡ç†
                print(f"ğŸ” [ORCA-CACHE] Request {orig_req.request_id} prefillå®Œæˆï¼Œä¿å­˜beam cacheåˆ—è¡¨ (å…±{len(processed_req.llm_cache)}ä¸ªbeam)")
            else:
                # å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼ŒæŒ‰å•ä¸ªcacheå¤„ç†ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                orig_req.past_key_values = [[processed_req.llm_cache]]
                print(f"âš ï¸ [ORCA-CACHE] Request {orig_req.request_id} prefillè¿”å›å•ä¸ªcacheï¼ŒåŒ…è£…ä¸ºbeamåˆ—è¡¨")
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿å­˜beam_stateåˆ°åŸå§‹request
            if hasattr(processed_req, 'beam_state'):
                orig_req.beam_state = processed_req.beam_state
                print(f"ğŸ” [ORCA-CACHE] ä¿å­˜beam_stateåˆ°request {orig_req.request_id}")
            
            results.append(result)
        
        print(f"ğŸ” [ORCA-PREFILL] Batchå®Œæˆ: {len(results)} ä¸ªç»“æœ")
        return results
            
        # except Exception as e:
        #     logger.error(f"Prefill batchå¤„ç†å¤±è´¥: {e}")
        #     # è¿”å›é”™è¯¯ç»“æœ
        #     return [
        #         {
        #             'request_id': req.request_id,
        #             'success': False,
        #             'error': str(e),
        #             'generated_text': '',
        #             'generated_tokens': [],
        #             'prefill_finished': False
        #         }
        #         for req in requests
        #     ]
    
    def _process_decode_batch(self, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """å¤„ç†decodeé˜¶æ®µçš„è¯·æ±‚ - ORCAé£æ ¼ï¼Œä¸€æ¬¡åªç”Ÿæˆä¸€ä¸ªtoken"""
        try:
            # ğŸ”¥ ORCAæ¶æ„ï¼šä¸ºbatchä¸­çš„æ¯ä¸ªrequeståˆ†åˆ«æ„é€ beam_search.Request
            beam_requests = []
            for req in requests:
                beam_req = self._create_beam_request(req)
                beam_requests.append(beam_req)
            
            print(f"ğŸ” [ORCA-DECODE] å¤„ç†batch: {len(beam_requests)} ä¸ªrequests")
            
            # ç›´æ¥è°ƒç”¨beam_searchçš„decodeå‡½æ•°
            from model.flashinfer.beam_search import decode
            
            processed_requests, speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable = decode(
                requests=beam_requests,
                model=self.model.model,  # ä½¿ç”¨å†…éƒ¨çš„æ¨¡å‹
                tokenizer=self.tokenizer,
                num_beams=self.config.beam_size,
                length_penalty=1.0,
                speech_pagetable=self.model.speech_pagetable,
                llm_prefill_pagetable=self.model.llm_prefill_pagetable,
                llm_decode_pagetable=self.model.llm_decode_pagetable
            )
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°pagetableçŠ¶æ€å¹¶éªŒè¯è¿ç»­æ€§
            self.model.speech_pagetable = speech_pagetable
            self.model.llm_prefill_pagetable = llm_prefill_pagetable
            self.model.llm_decode_pagetable = llm_decode_pagetable
            
            # éªŒè¯pagetableçŠ¶æ€
            self._verify_pagetable_consistency("Decode", speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable)
            
            # è½¬æ¢ç»“æœå¹¶æ›´æ–°æ¯ä¸ªrequestçš„cacheå¼•ç”¨
            results = []
            for i, (orig_req, processed_req) in enumerate(zip(requests, processed_requests)):
                result = self._convert_beam_result_to_inference_result(orig_req, processed_req, is_prefill=False)
                
                # ğŸ”¥ ORCAå…³é”®ï¼šç«‹å³æ›´æ–°åŸå§‹requestçš„cacheå¼•ç”¨
                # Decodeé˜¶æ®µï¼šæ ¹æ®processed_reqçš„çŠ¶æ€å†³å®šcacheæ ¼å¼
                if hasattr(processed_req, 'decode_finished') and processed_req.decode_finished:
                    # å¦‚æœdecodeå®Œæˆï¼Œè½¬æ¢ä¸ºå•ä¸ªcache
                    orig_req.speech_cache = [processed_req.speech_cache]
                    orig_req.past_key_values = [processed_req.llm_cache]  
                    print(f"ğŸ” [ORCA-CACHE] Request {orig_req.request_id} decodeå®Œæˆï¼Œcacheè½¬æ¢ä¸ºå•ä¸ªæ ¼å¼")
                else:
                    # å¦‚æœdecodeæœªå®Œæˆï¼Œä¿æŒbeam cacheåˆ—è¡¨æ ¼å¼
                    orig_req.speech_cache = [processed_req.speech_cache]
                    if isinstance(processed_req.llm_cache, list):
                        orig_req.past_key_values = processed_req.llm_cache  # ä¿æŒbeamåˆ—è¡¨
                        print(f"ğŸ” [ORCA-CACHE] Request {orig_req.request_id} decodeç»§ç»­ï¼Œä¿æŒbeam cacheåˆ—è¡¨ ({len(processed_req.llm_cache)}ä¸ªbeam)")
                    else:
                        orig_req.past_key_values = [processed_req.llm_cache]
                        print(f"ğŸ” [ORCA-CACHE] Request {orig_req.request_id} decodeç»§ç»­ï¼Œcacheè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼")
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿å­˜beam_stateåˆ°åŸå§‹request
                if hasattr(processed_req, 'beam_state'):
                    orig_req.beam_state = processed_req.beam_state
                    print(f"ğŸ” [ORCA-CACHE] ä¿å­˜beam_stateåˆ°request {orig_req.request_id}")
                
                results.append(result)
            
            print(f"ğŸ” [ORCA-DECODE] Batchå®Œæˆ: {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"Decode batchå¤„ç†å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯ç»“æœ
            return [
                {
                    'request_id': req.request_id,
                    'success': False,
                    'error': str(e),
                    'generated_text': '',
                    'generated_tokens': [],
                    'decode_finished': False
                }
                for req in requests
            ]
    
    def _create_beam_request(self, request: InferenceRequest):
        """å°†InferenceRequestè½¬æ¢ä¸ºbeam_searchçš„Requestæ ¼å¼"""
        from model.flashinfer.beam_search import Request
        from model.flashinfer.engine import SpeechCache, LLMCache
        from agents.infinisst import S2TAgentStates
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåˆ›å»ºS2TAgentStateså¯¹è±¡ï¼Œè®©modelçš„_prepare_speechå’Œ_prepare_inputsæ–¹æ³•å¤„ç†
        states = S2TAgentStates(
            speech_cache=request.speech_cache,
            past_key_values=request.past_key_values,
            target_ids=getattr(request, 'target_ids', []),
            segment_idx=getattr(request, 'segment_idx', 0),
            translations_list=getattr(request, 'translations_list', [])
        )
        
        # è½¬æ¢ä¸ºlistæ ¼å¼ï¼ˆS2TAgentStatesæœŸæœ›çš„æ ¼å¼ï¼‰
        states.source = request.speech_batch
        states.source_finished = getattr(request, 'is_final', False)
        states.source_sample_rate = 16000
        # ğŸ”¥ ç›´æ¥è°ƒç”¨modelçš„prepareæ–¹æ³•ï¼Œå°±åƒinfinisst_faster.policy()é‚£æ ·
        speech_batch = self.model._prepare_speech(states)
        input_ids = self.model._prepare_inputs(states)
        
        print(f"ğŸ”§ [PREPARE-DATA] è°ƒç”¨model._prepare_speechå’Œ_prepare_inputså®Œæˆ:")
        print(f"   - speech_batch shape: {speech_batch.shape}")
        print(f"   - input_ids shape: {input_ids.shape}")
        
        # ç¡®ä¿æ•°æ®ç»´åº¦æ­£ç¡®
        if speech_batch.dim() == 2:
            speech_batch = speech_batch[0]  # [1, seq_len] -> [seq_len]
        if input_ids.dim() == 2:
            input_ids = input_ids[0]  # [1, seq_len] -> [seq_len]
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†cacheç»“æ„
        # æ ¹æ®infinisst_faster.pyï¼Œstates.speech_cacheå’Œstates.past_key_valuesæ˜¯åˆ—è¡¨
        # åœ¨ORCAæ¶æ„ä¸­ï¼Œæ¯ä¸ªrequestå¯¹åº”ä¸€ä¸ªcacheæ¡ç›®
        cache_index = 0  # æ¯ä¸ªrequestä½¿ç”¨ç¬¬ä¸€ä¸ªcacheï¼ˆåœ¨ORCAä¸­æ¯ä¸ªrequestéƒ½æ˜¯ç‹¬ç«‹çš„ï¼‰
        
        if states.speech_cache is None:
            speech_cache_for_request = None
        else:
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–æŒ‡å®šç´¢å¼•ï¼›å¦‚æœä¸æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
            if isinstance(states.speech_cache, list):
                speech_cache_for_request = states.speech_cache[cache_index] if len(states.speech_cache) > cache_index else None
            else:
                speech_cache_for_request = states.speech_cache
        
        if states.past_key_values is None:
            past_key_values_for_request = None
        else:
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šDecodeé˜¶æ®µéœ€è¦ç‰¹æ®Šå¤„ç†
            if request.stage == RequestStage.DECODE:
                # Decodeé˜¶æ®µï¼špast_key_valuesåº”è¯¥æ˜¯beam cacheåˆ—è¡¨
                if isinstance(states.past_key_values, list) and len(states.past_key_values) > cache_index:
                    # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æ˜¯LLMCacheå¯¹è±¡æ¥åˆ¤æ–­æ˜¯å¦ä¸ºbeam cacheåˆ—è¡¨
                    first_element = states.past_key_values[0]
                    if hasattr(first_element, 'paged_kv_indices'):
                        # ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯LLMCacheå¯¹è±¡ï¼Œè¯´æ˜è¿™å°±æ˜¯beam cacheåˆ—è¡¨
                        past_key_values_for_request = states.past_key_values
                        print(f"ğŸ” [DECODE-CACHE] è¯†åˆ«ä¸ºbeam cacheåˆ—è¡¨ï¼Œé•¿åº¦: {len(states.past_key_values)}")
                    else:
                        # ç¬¬ä¸€ä¸ªå…ƒç´ ä¸æ˜¯LLMCacheï¼Œéœ€è¦è¿›ä¸€æ­¥è§£æ
                        past_key_values_cache = states.past_key_values[cache_index]
                        if isinstance(past_key_values_cache, list):
                            # æ£€æŸ¥åµŒå¥—åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ 
                            if len(past_key_values_cache) > 0 and hasattr(past_key_values_cache[0], 'paged_kv_indices'):
                                # è¿™æ˜¯beam cacheåˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                                past_key_values_for_request = past_key_values_cache
                                print(f"ğŸ” [DECODE-CACHE] ä½¿ç”¨åµŒå¥—beam cacheåˆ—è¡¨ï¼Œé•¿åº¦: {len(past_key_values_cache)}")
                            else:
                                # è¿™æ˜¯å¤–å±‚åŒ…è£…åˆ—è¡¨ï¼Œéœ€è¦è¿›ä¸€æ­¥è§£æ
                                if len(past_key_values_cache) > 0 and isinstance(past_key_values_cache[0], list):
                                    # åŒå±‚åŒ…è£…ï¼š[[beam_cache_1, beam_cache_2, ...]]
                                    past_key_values_for_request = past_key_values_cache[0]
                                    print(f"ğŸ” [DECODE-CACHE] è§£æåŒå±‚åŒ…è£…ï¼Œbeam cacheåˆ—è¡¨é•¿åº¦: {len(past_key_values_for_request)}")
                                else:
                                    # å•ä¸ªcacheè¢«åŒ…è£…ï¼š[single_cache]
                                    past_key_values_for_request = past_key_values_cache
                                    print(f"âš ï¸ [DECODE-CACHE] æ£€æµ‹åˆ°å•cacheåŒ…è£…ï¼Œé•¿åº¦: {len(past_key_values_cache)}")
                        else:
                            # å•ä¸ªcacheï¼Œéœ€è¦åŒ…è£…æˆåˆ—è¡¨ï¼ˆè¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿåœ¨æ­£ç¡®çš„prefillä¹‹åï¼‰
                            past_key_values_for_request = [past_key_values_cache]
                            print(f"âš ï¸ [DECODE-CACHE] å•ä¸ªcacheåŒ…è£…ä¸ºåˆ—è¡¨")
                else:
                    past_key_values_for_request = None
                    print(f"âš ï¸ [DECODE-CACHE] æ— æ³•è·å–cache")
            else:
                # Prefillé˜¶æ®µï¼šå•ä¸ªcache
                if isinstance(states.past_key_values, list):
                    past_key_values_for_request = states.past_key_values[cache_index] if len(states.past_key_values) > cache_index else None
                else:
                    past_key_values_for_request = states.past_key_values
        
        print(f"ğŸ” [BEAM-CACHE] CacheçŠ¶æ€:")
        print(f"   - speech_cacheç±»å‹: {type(states.speech_cache)}, é•¿åº¦: {len(states.speech_cache) if isinstance(states.speech_cache, list) else 'N/A'}")
        print(f"   - past_key_valuesç±»å‹: {type(states.past_key_values)}, é•¿åº¦: {len(states.past_key_values) if isinstance(states.past_key_values, list) else 'N/A'}")
        print(f"   - ä½¿ç”¨cacheç´¢å¼•: {cache_index}")
        print(f"   - speech_cache_for_request: {speech_cache_for_request is not None}")
        print(f"   - past_key_values_for_requestç±»å‹: {type(past_key_values_for_request)}")
        if isinstance(past_key_values_for_request, list):
            print(f"   - past_key_values_for_requesté•¿åº¦: {len(past_key_values_for_request)}")
        else:
            print(f"   - past_key_values_for_request: {past_key_values_for_request is not None}")
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæŒ‰ç…§infinisst_faster.pyçš„Requestæ„é€ æ–¹å¼
        beam_req = Request(
            input_ids.view(-1),  # æŒ‰ç…§åŸå§‹ä»£ç ï¼šinput_ids.view(-1)
            speech_batch.view(-1),  # æŒ‰ç…§åŸå§‹ä»£ç ï¼šspeech_batch.view(-1)
            self.model.latency_multiplier * self.model.blocksize,  # blocksizeå‚æ•°
            request.max_new_tokens,  # max_new_tokens
            
            # speechç›¸å…³å‚æ•°
            self.model_args.max_cache_size,  # speech_max_steps
            speech_cache_for_request,  # speech_cache
            
            # LLMç›¸å…³å‚æ•°  
            self.model_args.max_llm_cache_size,  # llm_max_steps
            getattr(self.model, 'system_prompt_size', 0),  # llm_max_steps_start
            past_key_values_for_request  # llm_cache
        )
        
        # è®¾ç½®çŠ¶æ€ - æ ¹æ®request.stageåˆ¤æ–­æ˜¯å¦å·²ç»prefill
        beam_req.prefill_finished = (request.stage == RequestStage.DECODE)
        beam_req.decode_finished = False
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ­£ç¡®è®¾ç½®beam_state
        if request.stage == RequestStage.DECODE and hasattr(request, 'beam_state') and request.beam_state is not None:
            # Decodeé˜¶æ®µï¼šæ¢å¤ä¿å­˜çš„beam_state
            beam_req.beam_state = request.beam_state
            print(f"ğŸ” [BEAM-STATE] æ¢å¤decodeé˜¶æ®µçš„beam_state for {request.request_id}")
        else:
            # Prefillé˜¶æ®µï¼šè®¾ç½®ä¸ºNoneï¼Œå°†ç”±beam_search.prefill()åˆ›å»º
            beam_req.beam_state = None
            print(f"ğŸ” [BEAM-STATE] Prefillé˜¶æ®µï¼Œbeam_stateå°†è¢«åˆ›å»º for {request.request_id}")
        
        print(f"ğŸ” [BEAM-REQUEST] Created beam request for {request.request_id}")
        print(f"   - Speech shape: {speech_batch.shape}")
        print(f"   - Input IDs shape: {input_ids.shape}")
        print(f"   - Max new tokens: {beam_req.max_new_tokens}")
        print(f"   - Blocksize: {self.model.latency_multiplier * self.model.blocksize}")
        
        return beam_req
    
    def _convert_beam_result_to_inference_result(self, orig_request: InferenceRequest, 
                                               processed_request, is_prefill: bool) -> Dict[str, Any]:
        """å°†beam_searchçš„ç»“æœè½¬æ¢ä¸ºInferenceResultæ ¼å¼"""
        
        result = {
            'request_id': orig_request.request_id,
            'success': True,
            'generated_text': '',
            'generated_tokens': [],
            'finished': False,
            'speech_cache': processed_request.speech_cache,
            'past_key_values': processed_request.llm_cache
        }
        
        if is_prefill:
            # Prefillé˜¶æ®µå®Œæˆ
            result['prefill_finished'] = processed_request.prefill_finished
            result['decode_finished'] = False
            
            # Prefillé€šå¸¸ä¸ç”Ÿæˆæ–‡æœ¬ï¼Œåªæ˜¯å‡†å¤‡beamçŠ¶æ€
            if hasattr(processed_request, 'beam_state') and processed_request.beam_state:
                beam_state = processed_request.beam_state
                if hasattr(beam_state, 'generated_ids') and beam_state.generated_ids is not None:
                    # è·å–åˆå§‹çš„beam candidates
                    first_tokens = beam_state.generated_ids[:, 0].tolist()  # ç¬¬ä¸€ä¸ªtoken
                    result['generated_tokens'] = first_tokens
                    
                    # å°è¯•è§£ç ç¬¬ä¸€ä¸ªtoken
                    if len(first_tokens) > 0:
                        try:
                            decoded_text = self.tokenizer.decode([first_tokens[0]], skip_special_tokens=True)
                            result['generated_text'] = decoded_text
                            print(f"ğŸ” [PREFILL-RESULT] Generated first token: {first_tokens[0]} -> '{decoded_text}'")
                        except Exception as e:
                            print(f"âš ï¸ [PREFILL-RESULT] Failed to decode token {first_tokens[0]}: {e}")
            
            print(f"ğŸ” [PREFILL-RESULT] Request {orig_request.request_id} prefillå®Œæˆ")
            
        else:
            # Decodeé˜¶æ®µ - ç”Ÿæˆäº†æ–°çš„token
            result['prefill_finished'] = True
            result['decode_finished'] = processed_request.decode_finished
            
            # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥beam_stateæ˜¯å¦ä¸ºNone
            if hasattr(processed_request, 'beam_state') and processed_request.beam_state is not None:
                beam_state = processed_request.beam_state
                if hasattr(beam_state, 'generated_ids') and beam_state.generated_ids is not None:
                    # è·å–å½“å‰æœ€ä½³beamçš„æ‰€æœ‰token
                    if len(beam_state.generated_ids) > 0:
                        best_sequence = beam_state.generated_ids[0].tolist()  # å–ç¬¬ä¸€ä¸ªbeam
                        result['generated_tokens'] = best_sequence
                        
                        # è§£ç å®Œæ•´åºåˆ—
                        try:
                            decoded_text = self.tokenizer.decode(best_sequence, skip_special_tokens=True)
                            
                            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåå¤„ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼Œè¿‡æ»¤æ‰promptæ ¼å¼token
                            filtered_text = self._filter_prompt_tokens(decoded_text)
                            result['generated_text'] = filtered_text
                            
                            print(f"ğŸ” [DECODE-RESULT] Generated sequence: {best_sequence} -> '{decoded_text}'")
                            print(f"ğŸ” [DECODE-RESULT] Filtered translation: '{filtered_text}'")
                        except Exception as e:
                            print(f"âš ï¸ [DECODE-RESULT] Failed to decode sequence {best_sequence}: {e}")
                            result['generated_text'] = ""
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    result['finished'] = processed_request.decode_finished
                else:
                    print(f"âš ï¸ [DECODE-RESULT] beam_state.generated_ids is None or missing")
                    result['finished'] = True  # å¦‚æœbeam_stateæœ‰é—®é¢˜ï¼Œæ ‡è®°ä¸ºå®Œæˆé¿å…æ— é™å¾ªç¯
            else:
                print(f"âš ï¸ [DECODE-RESULT] beam_state is None or missing")
                result['finished'] = True  # å¦‚æœbeam_stateä¸ºNoneï¼Œæ ‡è®°ä¸ºå®Œæˆ
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆç»“æœ
            if hasattr(processed_request, 'results') and processed_request.results:
                # å¦‚æœå·²ç»æœ‰æœ€ç»ˆç»“æœ
                final_result = processed_request.results
                if isinstance(final_result, dict) and 'sequence' in final_result:
                    sequence = final_result['sequence']
                    result['generated_tokens'] = sequence
                    
                    try:
                        decoded_text = self.tokenizer.decode(sequence, skip_special_tokens=True)
                        result['generated_text'] = decoded_text
                        result['finished'] = True
                        print(f"ğŸ” [DECODE-FINAL] Final result: {sequence} -> '{decoded_text}'")
                    except Exception as e:
                        print(f"âš ï¸ [DECODE-FINAL] Failed to decode final sequence {sequence}: {e}")
                        
            print(f"ğŸ” [DECODE-RESULT] Request {orig_request.request_id} decode stepå®Œæˆ, finished={result['finished']}")
        
        return result

    def _filter_prompt_tokens(self, text: str) -> str:
        """
        è¿‡æ»¤æ‰promptæ ¼å¼tokenï¼Œåªä¿ç•™çœŸæ­£çš„ç¿»è¯‘å†…å®¹
        
        ä¸»è¦è¿‡æ»¤çš„æ ¼å¼tokenåŒ…æ‹¬ï¼š
        - <speech>, <|user|>, <|assistant|>, <|startofprev|>, <|endofprev|> ç­‰
        - æ¢è¡Œç¬¦å’Œå¤šä½™çš„ç©ºæ ¼
        """
        if not text:
            return ""
        
        # éœ€è¦è¿‡æ»¤çš„æ ¼å¼tokenæ¨¡å¼
        format_tokens = [
            '<speech>',
            '<|user|>',
            '<|assistant|>', 
            '<|startofprev|>',
            '<|endofprev|>',
            '<|start_header_id|>',
            '<|end_header_id|>',
            '<|eot_id|>',
            '<sp_patch>',
            '<|',
            '|>',
            'Translate the following speech',
            'from English to Chinese',
            'from English to Italian',
            'from English to German', 
            'from English to Spanish'
        ]
        
        # ç§»é™¤æ ¼å¼token
        filtered_text = text
        for token in format_tokens:
            filtered_text = filtered_text.replace(token, '')
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        filtered_text = filtered_text.strip()
        
        # ç§»é™¤è¿ç»­çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼
        import re
        filtered_text = re.sub(r'\s+', ' ', filtered_text)
        filtered_text = filtered_text.strip()
        
        # ğŸ”¥ ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœç»“æœåªåŒ…å«æ ¼å¼å­—ç¬¦ï¼ˆå¦‚'<'ï¼‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if filtered_text in ['<', '><', '|', '>', ''] or filtered_text.isspace():
            filtered_text = ""
        
        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦è¿˜åœ¨ç”Ÿæˆpromptæ ¼å¼
        if any(pattern in filtered_text.lower() for pattern in ['translate', 'speech', 'english', 'chinese']):
            # å¦‚æœè¿˜åŒ…å«è¿™äº›å…³é”®è¯ï¼Œè¯´æ˜è¿˜åœ¨ç”Ÿæˆpromptï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
            filtered_text = ""
        
        return filtered_text

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'gpu_id': self.gpu_id,
            'is_loaded': self.is_loaded,
            'is_running': self.is_running,
            'stats': self.stats.copy(),
            'config': {
                'max_concurrent_requests': self.config.max_concurrent_requests,
                'beam_size': self.config.beam_size,
                'max_new_tokens': self.config.max_new_tokens
            }
        }
    
    def _cleanup_finished_sessions(self, requests: List[InferenceRequest], results: List[Dict[str, Any]]):
        """æ¸…ç†å·²ç»“æŸçš„sessionçš„KV cacheé¡µé¢"""
        try:
            for i, request in enumerate(requests):
                if i < len(results):
                    result = results[i]
                    
                    # æ£€æŸ¥sessionæ˜¯å¦ç»“æŸï¼ˆç¿»è¯‘å®Œæˆæˆ–å‡ºé”™ï¼‰
                    session_finished = (
                        not result.get('success', False) or  # å‡ºé”™äº†
                        result.get('finished', False) or     # æ˜ç¡®æ ‡è®°å®Œæˆ
                        getattr(request, 'is_final', False)   # æ˜¯æœ€åä¸€ä¸ªè¯·æ±‚
                    )
                    
                    if session_finished:
                        logger.info(f"ğŸ§¹ Sessionç»“æŸï¼Œå¼€å§‹æ¸…ç†KV cacheé¡µé¢: {request.request_id}")
                        self._cleanup_session_kv_cache(request)
                        
        except Exception as e:
            logger.error(f"æ¸…ç†sessionæ—¶å‡ºé”™: {e}")
    
    def _cleanup_session_kv_cache(self, request: InferenceRequest):
        """æ¸…ç†å•ä¸ªsessionçš„KV cacheé¡µé¢"""
        try:
            # è¿™é‡Œéœ€è¦è®¿é—®å…·ä½“çš„KV cacheæ•°æ®ç»“æ„
            # å‡è®¾requestä¸­åŒ…å«äº†KV cacheçš„å¼•ç”¨
            
            session_id = getattr(request, 'session_id', request.request_id)
            
            # ğŸ”¥ å…³é”®ï¼šé‡Šæ”¾speech cacheé¡µé¢
            if hasattr(request, 'speech_cache') and request.speech_cache:
                self._release_speech_cache_pages(request.speech_cache, session_id)
            
            # ğŸ”¥ å…³é”®ï¼šé‡Šæ”¾LLM KV cacheé¡µé¢
            if hasattr(request, 'past_key_values') and request.past_key_values:
                self._release_llm_cache_pages(request.past_key_values, session_id)
            
            logger.info(f"âœ… Session {session_id} KV cacheé¡µé¢æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç†session {request.request_id} KV cacheæ—¶å‡ºé”™: {e}")
    
    def _release_speech_cache_pages(self, speech_cache, session_id: str):
        """é‡Šæ”¾speech cacheå ç”¨çš„é¡µé¢"""
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„speech cacheç»“æ„æ¥å®ç°
            # å‡è®¾speech_cacheåŒ…å«é¡µé¢ç´¢å¼•ä¿¡æ¯
            
            if hasattr(speech_cache, 'paged_kv_indices') and speech_cache.paged_kv_indices:
                pages_to_release = len(speech_cache.paged_kv_indices)
                
                # è°ƒç”¨é¡µé¢é‡Šæ”¾å‡½æ•°ï¼ˆéœ€è¦ä»flashinferå¼•æ“è·å–pagetableï¼‰
                if hasattr(self.model, 'speech_pagetable'):
                    pagetable = self.model.speech_pagetable
                    self._release_pages_to_pool(pagetable, speech_cache.paged_kv_indices, session_id, 'speech')
                    
                    # æ¸…ç©ºcacheä¸­çš„é¡µé¢å¼•ç”¨
                    speech_cache.paged_kv_indices = []
                    speech_cache.paged_kv_last_page_len = 16  # PAGE_SIZE
                    
                    logger.info(f"ğŸ”„ é‡Šæ”¾äº† {pages_to_release} ä¸ªspeech cacheé¡µé¢åˆ°é¡µé¢æ± ")
                    
        except Exception as e:
            logger.error(f"é‡Šæ”¾speech cacheé¡µé¢æ—¶å‡ºé”™: {e}")
    
    def _release_llm_cache_pages(self, past_key_values, session_id: str):
        """é‡Šæ”¾LLM KV cacheå ç”¨çš„é¡µé¢"""
        try:
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„past_key_valuesç»“æ„æ¥å®ç°
            
            if hasattr(past_key_values, 'paged_kv_indices') and past_key_values.paged_kv_indices:
                pages_to_release = len(past_key_values.paged_kv_indices)
                
                # åˆ†åˆ«å¤„ç†prefillå’Œdecode cache
                if hasattr(self.model, 'llm_prefill_pagetable'):
                    self._release_pages_to_pool(self.model.llm_prefill_pagetable, 
                                              past_key_values.paged_kv_indices, 
                                              session_id, 'llm_prefill')
                
                if hasattr(self.model, 'llm_decode_pagetable'):
                    self._release_pages_to_pool(self.model.llm_decode_pagetable, 
                                              past_key_values.paged_kv_indices, 
                                              session_id, 'llm_decode')
                
                # æ¸…ç©ºcacheä¸­çš„é¡µé¢å¼•ç”¨
                past_key_values.paged_kv_indices = []
                past_key_values.paged_kv_last_page_len = 16  # PAGE_SIZE
                
                logger.info(f"ğŸ”„ é‡Šæ”¾äº† {pages_to_release} ä¸ªLLM cacheé¡µé¢åˆ°é¡µé¢æ± ")
                
        except Exception as e:
            logger.error(f"é‡Šæ”¾LLM cacheé¡µé¢æ—¶å‡ºé”™: {e}")
    
    def _release_pages_to_pool(self, pagetable, page_indices: list, session_id: str, cache_type: str):
        """å°†é¡µé¢é‡Šæ”¾å›é¡µé¢æ± """
        try:
            if not page_indices:
                return
            
            import torch
            
            # å‡å°‘é¡µé¢å¼•ç”¨è®¡æ•°
            page_indices_tensor = torch.tensor(page_indices, dtype=torch.long)
            pagetable.page_cnt[page_indices_tensor] -= 1
            
            # æ‰¾å‡ºå¼•ç”¨è®¡æ•°ä¸º0çš„é¡µé¢ï¼ˆå¯ä»¥è¢«é‡Šæ”¾ï¼‰
            free_mask = pagetable.page_cnt[page_indices_tensor] == 0
            free_pages = page_indices_tensor[free_mask]
            
            if len(free_pages) > 0:
                # å°†é¡µé¢æ”¾å›å¯ç”¨é˜Ÿåˆ—
                free_pages_list = free_pages.tolist()
                pagetable.paged_queue.extend(free_pages_list)
                
                logger.info(f"ğŸ”„ [{cache_type}] Session {session_id} é‡Šæ”¾äº† {len(free_pages_list)} ä¸ªé¡µé¢å›é¡µé¢æ± ")
                logger.info(f"ğŸ”„ [{cache_type}] é¡µé¢æ± ç°åœ¨æœ‰ {len(pagetable.paged_queue)} ä¸ªå¯ç”¨é¡µé¢")
                
                # ğŸ” è¯¦ç»†è®°å½•é¡µé¢ä½¿ç”¨æƒ…å†µ
                total_pages = len(pagetable.page_cnt)
                used_pages = torch.sum(pagetable.page_cnt > 0).item()
                logger.info(f"ğŸ“Š [{cache_type}] é¡µé¢ä½¿ç”¨ç»Ÿè®¡: {used_pages}/{total_pages} é¡µè¢«ä½¿ç”¨")
            else:
                logger.warning(f"âš ï¸ [{cache_type}] Session {session_id} çš„ {len(page_indices)} ä¸ªé¡µé¢ä»è¢«å…¶ä»–sessionå¼•ç”¨")
                
        except Exception as e:
            logger.error(f"é‡Šæ”¾é¡µé¢åˆ°æ± æ—¶å‡ºé”™: {e}")
    
    def force_cleanup_all_sessions(self):
        """å¼ºåˆ¶æ¸…ç†æ‰€æœ‰sessionçš„KV cacheï¼ˆç´§æ€¥æƒ…å†µä½¿ç”¨ï¼‰"""
        try:
            logger.warning("ğŸš¨ å¼ºåˆ¶æ¸…ç†æ‰€æœ‰sessionçš„KV cacheé¡µé¢")
            
            # é‡ç½®æ‰€æœ‰é¡µé¢æ± åˆ°åˆå§‹çŠ¶æ€
            if hasattr(self.model, 'speech_pagetable'):
                self._reset_pagetable(self.model.speech_pagetable, 'speech')
            
            if hasattr(self.model, 'llm_prefill_pagetable'):
                self._reset_pagetable(self.model.llm_prefill_pagetable, 'llm_prefill')
            
            if hasattr(self.model, 'llm_decode_pagetable'):
                self._reset_pagetable(self.model.llm_decode_pagetable, 'llm_decode')
            
            logger.info("âœ… å¼ºåˆ¶æ¸…ç†å®Œæˆï¼Œæ‰€æœ‰é¡µé¢å·²é‡ç½®")
            
        except Exception as e:
            logger.error(f"å¼ºåˆ¶æ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _reset_pagetable(self, pagetable, cache_type: str):
        """é‡ç½®é¡µé¢è¡¨åˆ°åˆå§‹çŠ¶æ€"""
        try:
            total_pages = len(pagetable.page_cnt)
            
            # é‡ç½®é¡µé¢å¼•ç”¨è®¡æ•°
            pagetable.page_cnt.zero_()
            
            # é‡å»ºå¯ç”¨é¡µé¢é˜Ÿåˆ—
            pagetable.paged_queue = list(range(total_pages))
            
            logger.info(f"ğŸ”„ [{cache_type}] é¡µé¢è¡¨å·²é‡ç½®: {total_pages} ä¸ªé¡µé¢å…¨éƒ¨å¯ç”¨")
            
        except Exception as e:
            logger.error(f"é‡ç½®é¡µé¢è¡¨æ—¶å‡ºé”™: {e}")

class MultiGPUInferenceEngine:
    """
    å¤šGPUæ¨ç†å¼•æ“ç®¡ç†å™¨
    ç®¡ç†å¤šä¸ªGPUä¸Šçš„æ¨ç†å¼•æ“å®ä¾‹
    """
    
    def __init__(self, gpu_language_map: Dict[int, str], model_args_map: Dict[int, Any] = None):
        """
        åˆå§‹åŒ–å¤šGPUæ¨ç†å¼•æ“
        
        Args:
            gpu_language_map: GPUåˆ°è¯­è¨€å¯¹çš„æ˜ å°„
            model_args_map: GPUåˆ°æ¨¡å‹å‚æ•°çš„æ˜ å°„ï¼ˆå¯é€‰ï¼‰
        """
        self.gpu_language_map = gpu_language_map
        self.model_args_map = model_args_map or {}
        
        # åˆ›å»ºå¼•æ“å®ä¾‹
        self.engines: Dict[int, InferenceEngine] = {}
        for gpu_id, language_pair in gpu_language_map.items():
            model_args = self.model_args_map.get(gpu_id, {})
            
            engine = InferenceEngine(
                model_args=model_args,
                gpu_id=gpu_id,
                language_id=language_pair
            )
            self.engines[gpu_id] = engine
        
        logger.info(f"å¤šGPUæ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œæ”¯æŒGPU: {list(self.engines.keys())}")
    
    def load_all_models(self) -> bool:
        """åŠ è½½æ‰€æœ‰GPUä¸Šçš„æ¨¡å‹"""
        success = True
        for gpu_id, engine in self.engines.items():
            if not engine.load_model():
                success = False
                logger.error(f"GPU {gpu_id} æ¨¡å‹åŠ è½½å¤±è´¥")
        return success
    
    def start_all(self):
        """å¯åŠ¨æ‰€æœ‰æ¨ç†å¼•æ“"""
        for engine in self.engines.values():
            engine.start()
    
    def stop_all(self):
        """åœæ­¢æ‰€æœ‰æ¨ç†å¼•æ“"""
        for engine in self.engines.values():
            engine.stop()
    
    def get_engine(self, gpu_id: int) -> Optional[InferenceEngine]:
        """è·å–æŒ‡å®šGPUçš„æ¨ç†å¼•æ“"""
        return self.engines.get(gpu_id)
    
    def process_batch(self, gpu_id: int, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """åœ¨æŒ‡å®šGPUä¸Šå¤„ç†æ‰¹è¯·æ±‚"""
        engine = self.get_engine(gpu_id)
        if not engine:
            raise ValueError(f"GPU {gpu_id} ä¸Šæ²¡æœ‰å¯ç”¨çš„æ¨ç†å¼•æ“")
        with torch.cuda.device("cuda:"+str(gpu_id)):
            res = engine.process_batch(requests)
        return res
    
    def get_all_stats(self) -> Dict[int, Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¼•æ“çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {gpu_id: engine.get_stats() for gpu_id, engine in self.engines.items()}

# åœ¨æ¨ç†å¼•æ“ç±»ä¸­æ·»åŠ éªŒè¯æ–¹æ³•
def _verify_pagetable_consistency(engine, stage_name: str, speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable):
    """éªŒè¯pagetableçŠ¶æ€çš„è¿ç»­æ€§"""
    try:
        print(f"ğŸ” [PAGETABLE-VERIFY] {stage_name} é˜¶æ®µå pagetable çŠ¶æ€:")
        
        # æ£€æŸ¥speech pagetable
        if hasattr(speech_pagetable, 'paged_queue'):
            available_speech_pages = len(speech_pagetable.paged_queue)
            total_speech_pages = len(speech_pagetable.page_cnt)
            used_speech_pages = torch.sum(speech_pagetable.page_cnt > 0).item()
            print(f"   - Speech: {used_speech_pages}/{total_speech_pages} é¡µè¢«ä½¿ç”¨, {available_speech_pages} é¡µå¯ç”¨")
        
        # æ£€æŸ¥LLM prefill pagetable
        if hasattr(llm_prefill_pagetable, 'paged_queue'):
            available_prefill_pages = len(llm_prefill_pagetable.paged_queue)
            total_prefill_pages = len(llm_prefill_pagetable.page_cnt)
            used_prefill_pages = torch.sum(llm_prefill_pagetable.page_cnt > 0).item()
            print(f"   - LLM Prefill: {used_prefill_pages}/{total_prefill_pages} é¡µè¢«ä½¿ç”¨, {available_prefill_pages} é¡µå¯ç”¨")
        
        # æ£€æŸ¥LLM decode pagetable
        if hasattr(llm_decode_pagetable, 'paged_queue'):
            available_decode_pages = len(llm_decode_pagetable.paged_queue)
            total_decode_pages = len(llm_decode_pagetable.page_cnt)
            used_decode_pages = torch.sum(llm_decode_pagetable.page_cnt > 0).item()
            print(f"   - LLM Decode: {used_decode_pages}/{total_decode_pages} é¡µè¢«ä½¿ç”¨, {available_decode_pages} é¡µå¯ç”¨")
        
        print(f"âœ… [PAGETABLE-VERIFY] {stage_name} pagetableçŠ¶æ€éªŒè¯å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ [PAGETABLE-VERIFY] {stage_name} pagetableéªŒè¯å¤±è´¥: {e}")

# å°†éªŒè¯æ–¹æ³•æ·»åŠ åˆ°InferenceEngineç±»ä¸­
InferenceEngine._verify_pagetable_consistency = lambda self, stage_name, speech_pt, llm_prefill_pt, llm_decode_pt: _verify_pagetable_consistency(self, stage_name, speech_pt, llm_prefill_pt, llm_decode_pt) 