import asyncio
import logging
import time
import uuid
from collections import deque
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple, Union
from threading import Lock, Thread
from enum import Enum

import torch
import numpy as np

logger = logging.getLogger(__name__)

class RequestStage(Enum):
    """Request processing stage"""
    PREFILL = "prefill"
    DECODE = "decode"

@dataclass
class UserSession:
    """
    Maintains state for a user session including all necessary information
    Based on the api.py structure and infinisst.py requirements
    """
    user_id: str
    language_id: str
    session_id: str
    
    # Speech processing state
    source: List[float] = field(default_factory=list)  # Audio samples
    source_finished: bool = False
    source_sample_rate: int = 16000
    src_len: int = 0
    
    # Translation state
    target: List[str] = field(default_factory=list)
    target_ids: List[int] = field(default_factory=list)
    segment_idx: int = 0
    
    # Cache state
    speech_cache: Optional[Any] = None
    past_key_values: Optional[Any] = None
    
    # ğŸ”¥ æ·»åŠ ï¼šBeam searchçŠ¶æ€
    beam_state: Optional[Any] = None
    
    # ğŸ” å†…å­˜ä½¿ç”¨è¿½è¸ª
    memory_usage: Dict[str, int] = field(default_factory=lambda: {
        'speech_pages': 0,
        'llm_prefill_pages': 0, 
        'llm_decode_pages': 0,
        'total_pages': 0,
        'peak_pages': 0,
        'allocation_count': 0
    })
    
    # Session management
    last_activity: float = field(default_factory=time.time)
    created_at: float = field(default_factory=time.time)
    
    # Translation parameters
    latency_multiplier: int = 2
    max_new_tokens: int = 20
    
    def reset(self):
        """Reset session state for new translation"""
        self.source = []
        self.source_finished = False
        self.src_len = 0
        self.target = []
        self.target_ids = []
        self.segment_idx = 0
        self.speech_cache = None
        self.past_key_values = None
        self.beam_state = None  # ğŸ”¥ æ·»åŠ ï¼šé‡ç½®beam_state
        self.last_activity = time.time()
        
        # é‡ç½®å†…å­˜ä½¿ç”¨è¿½è¸ª
        self.memory_usage = {
            'speech_pages': 0,
            'llm_prefill_pages': 0, 
            'llm_decode_pages': 0,
            'total_pages': 0,
            'peak_pages': 0,
            'allocation_count': 0
        }
    
    def update_memory_usage(self, cache_type: str, pages_used: int):
        """æ›´æ–°å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        if cache_type in self.memory_usage:
            self.memory_usage[cache_type] = pages_used
        
        # æ›´æ–°æ€»é¡µé¢æ•°
        total = (self.memory_usage.get('speech_pages', 0) + 
                self.memory_usage.get('llm_prefill_pages', 0) + 
                self.memory_usage.get('llm_decode_pages', 0))
        self.memory_usage['total_pages'] = total
        
        # æ›´æ–°å³°å€¼
        if total > self.memory_usage.get('peak_pages', 0):
            self.memory_usage['peak_pages'] = total
        
        self.memory_usage['allocation_count'] += 1
        
        print(f"ğŸ” [SESSION-MEMORY] {self.session_id} å†…å­˜ä½¿ç”¨:")
        print(f"   - Speech: {self.memory_usage.get('speech_pages', 0)} é¡µ")
        print(f"   - LLM Prefill: {self.memory_usage.get('llm_prefill_pages', 0)} é¡µ")
        print(f"   - LLM Decode: {self.memory_usage.get('llm_decode_pages', 0)} é¡µ")
        print(f"   - æ€»è®¡: {total} é¡µ (å³°å€¼: {self.memory_usage.get('peak_pages', 0)} é¡µ)")
        print(f"   - åˆ†é…æ¬¡æ•°: {self.memory_usage.get('allocation_count', 0)}")
    
    def get_memory_summary(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        return {
            'session_id': self.session_id,
            'user_id': self.user_id,
            'language_id': self.language_id,
            'memory_usage': self.memory_usage.copy(),
            'session_age_seconds': time.time() - self.created_at,
            'inactive_seconds': time.time() - self.last_activity
        }

@dataclass
class InferenceRequest:
    """
    Single inference request 
    """
    request_id: str
    user_id: str
    language_id: str
    session_id: str
    stage: RequestStage
    
    # Input data 
    speech_batch: torch.Tensor  # Speech input tensor
    input_ids: torch.Tensor     # Text input token IDs
    
    # Generation parameters 
    max_new_tokens: int = 20
    beam_size: int = 1
    do_sample: bool = False
    top_p: float = 0.9
    top_k: int = 50
    temperature: float = 1.0
    repetition_penalty: float = 1.0
    
    # State management
    speech_cache: Optional[Any] = None
    past_key_values: Optional[Any] = None
    encoder_input_ids: Optional[torch.Tensor] = None
    segment_idx: int = 0
    translations_list: List[str] = field(default_factory=list)
    session_src_len: int = 0  # ğŸ”¥ æ·»åŠ ï¼šä¼šè¯çš„å·²å¤„ç†éŸ³é¢‘é•¿åº¦
    
    # ğŸ”¥ æ·»åŠ ï¼šBeam searchçŠ¶æ€
    beam_state: Optional[Any] = None
    
    # Metadata
    timestamp: float = field(default_factory=time.time)
    priority: int = 0  # Higher number = higher priority
    
    # Result handling
    result_callback: Optional[callable] = None
    is_processing: bool = False
    is_completed: bool = False
    result: Optional[Dict[str, Any]] = None

class LLMScheduler:
    """
    - Maintains two FCFS queues (PREFILL and DECODE)
    - Prioritizes PREFILL queue over DECODE queue
    - Maximum batch size of 32 requests
    """
    
    def __init__(self, gpu_language_map: Dict[int, str], args=None):
        """
        Initialize scheduler with GPU-to-language mapping
        
        Args:
            gpu_language_map: Dict mapping GPU ID to language pair (e.g., {0: "en-zh", 1: "en-de"})
            args: Additional configuration arguments
        """
        self.gpu_language_map = gpu_language_map  # {gpu_id: language_id}
        self.language_gpu_map = {v: k for k, v in gpu_language_map.items()}  # {language_id: gpu_id}
        
        # Configuration
        self.max_batch_size = getattr(args, 'max_batch_size', 32) if args else 32
        self.batch_timeout = getattr(args, 'batch_timeout', 0.1) if args else 0.1  # seconds
        self.session_timeout = getattr(args, 'session_timeout', 3600) if args else 3600  # 1 hour
        
        # FCFS queues - separate queues for each GPU/language
        self.prefill_queues: Dict[int, deque] = {gpu_id: deque() for gpu_id in gpu_language_map.keys()}
        self.decode_queues: Dict[int, deque] = {gpu_id: deque() for gpu_id in gpu_language_map.keys()}
        
        # User session management
        self.user_sessions: Dict[str, Dict[str, UserSession]] = {}  # {language_id: {user_id: session}}
        
        # Thread safety
        self.queue_lock = Lock()
        self.session_lock = Lock()
        
        # Processing state
        self.is_running = False
        self.processing_threads: Dict[int, Thread] = {}  # One thread per GPU
        
        # Statistics
        self.stats = {
            'total_requests': 0,
            'completed_requests': 0,
            'active_sessions': 0,
            'queue_sizes': {gpu_id: {'prefill': 0, 'decode': 0} for gpu_id in gpu_language_map.keys()}
        }
        
        logger.info(f"LLMScheduler initialized with GPU mapping: {gpu_language_map}")
        logger.info(f"Max batch size: {self.max_batch_size}")
    
    def start(self):
        """Start the scheduler processing loops for all GPUs"""
        if self.is_running:
            logger.warning("Scheduler is already running")
            return
        
        self.is_running = True
        
        # Start one processing thread per GPU
        for gpu_id in self.gpu_language_map.keys():
            thread = Thread(target=self._processing_loop, args=(gpu_id,), daemon=True)
            thread.start()
            self.processing_threads[gpu_id] = thread
            logger.info(f"Started processing thread for GPU {gpu_id} (language: {self.gpu_language_map[gpu_id]})")
        
        logger.info("Scheduler started")
    
    def stop(self):
        """Stop all scheduler processing loops"""
        self.is_running = False
        
        # Wait for all threads to finish
        for gpu_id, thread in self.processing_threads.items():
            thread.join(timeout=5.0)
            logger.info(f"Stopped processing thread for GPU {gpu_id}")
        
        self.processing_threads.clear()
        logger.info("Scheduler stopped")
    
    def get_or_create_session(self, user_id: str, language_id: str) -> UserSession:
        """Get existing session or create new one"""
        with self.session_lock:
            if language_id not in self.user_sessions:
                self.user_sessions[language_id] = {}
            
            if user_id not in self.user_sessions[language_id]:
                session_id = f"{user_id}_{language_id}_{int(time.time())}"
                session = UserSession(
                    user_id=user_id,
                    language_id=language_id,
                    session_id=session_id
                )
                self.user_sessions[language_id][user_id] = session
                self.stats['active_sessions'] += 1
                logger.info(f"Created new session {session_id} for user {user_id}, language {language_id}")
            else:
                session = self.user_sessions[language_id][user_id]
                session.last_activity = time.time()
            
            return self.user_sessions[language_id][user_id]
    
    def submit_request(self, 
                      user_id: str,
                      language_id: str,
                      speech_data: Union[torch.Tensor, np.ndarray, List[float]],
                      stage: RequestStage = RequestStage.PREFILL,
                      is_final: bool = False,
                      max_new_tokens: int = 20,
                      result_callback: Optional[callable] = None) -> str:
        """
        Submit a request to the appropriate queue based on language and stage
        
        Args:
            user_id: Unique identifier for the user
            language_id: Language pair identifier (e.g., "en-zh")
            speech_data: Audio data (will be converted to tensor)
            stage: PREFILL or DECODE stage
            is_final: Whether this is the final segment
            max_new_tokens: Maximum tokens to generate
            result_callback: Callback function for results
            
        Returns:
            request_id: Unique identifier for this request
        """
        # Validate language support
        if language_id not in self.language_gpu_map:
            raise ValueError(f"Unsupported language pair: {language_id}. Supported: {list(self.language_gpu_map.keys())}")
        
        gpu_id = self.language_gpu_map[language_id]
        
        # Get or create user session
        session = self.get_or_create_session(user_id, language_id)
        
        # Update session with new speech data - éªŒè¯ä½†ä¸åšæ»‘åŠ¨çª—å£
        if isinstance(speech_data, (list, np.ndarray)):
            speech_data = torch.tensor(speech_data, dtype=torch.float32)
        elif not isinstance(speech_data, torch.Tensor):
            raise ValueError("speech_data must be list, numpy array, or torch tensor")
        
        # æ£€æŸ¥éŸ³é¢‘æ•°æ®é•¿åº¦
        audio_length = speech_data.numel() if speech_data.dim() > 0 else 0
        print(f"ğŸ” [SCHEDULER] Audio data length: {audio_length}, shape: {speech_data.shape}")
        print(f"ğŸ” [SCHEDULER] Audio stats: min={speech_data.min().item() if audio_length > 0 else 0:.6f}, max={speech_data.max().item() if audio_length > 0 else 0:.6f}")
        
        # å¦‚æœéŸ³é¢‘æ•°æ®ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè®°å½•è­¦å‘Šä½†ä¸å¡«å……
        MIN_AUDIO_LENGTH = 160  # 0.01ç§’ @ 16kHzï¼Œæ›´å®½æ¾çš„é˜ˆå€¼
        if audio_length == 0:
            print(f"âš ï¸ [SCHEDULER] Received empty audio data for user {user_id}, skipping request")
            raise ValueError("Empty audio data received")
        elif audio_length < MIN_AUDIO_LENGTH:
            print(f"âš ï¸ [SCHEDULER] Audio data too short ({audio_length} samples), but processing anyway")
        
        # Update session state - ç®€åŒ–ç‰ˆï¼Œç§»é™¤æ»‘åŠ¨çª—å£
        new_audio_data = speech_data.tolist() if speech_data.dim() == 1 else speech_data.flatten().tolist()
        session.source.extend(new_audio_data)
        session.source_finished = is_final
        session.last_activity = time.time()
        
        print(f"ğŸ” [SCHEDULER] Session source now has {len(session.source)} total samples ({len(session.source)/16000:.1f}s)")
        print(f"ğŸ” [SCHEDULER] Session src_len (already processed): {session.src_len} samples")
        print(f"ğŸ” [SCHEDULER] New samples to process: {len(session.source) - session.src_len}")
        
        # ğŸ” ä¼°ç®—é¡µé¢ä½¿ç”¨é‡ï¼ˆä»…ç”¨äºè¯Šæ–­ï¼‰
        audio_duration_s = len(session.source) / 16000
        estimated_speech_pages = max(1, int(audio_duration_s / 2))  # ä¼°ç®—ï¼šæ¯2ç§’éœ€è¦1ä¸ªspeeché¡µé¢
        estimated_llm_pages = len(session.target) * 2  # ä¼°ç®—ï¼šæ¯ä¸ªç¿»è¯‘æ®µè½éœ€è¦2ä¸ªLLMé¡µé¢
        total_estimated_pages = estimated_speech_pages + estimated_llm_pages
        
        print(f"ğŸ“Š [SCHEDULER] ä¼°ç®—é¡µé¢ä½¿ç”¨: Speech={estimated_speech_pages}, LLM={estimated_llm_pages}, æ€»è®¡={total_estimated_pages}")
        print(f"ğŸ“Š [SCHEDULER] å½“å‰ç¿»è¯‘æ®µè½æ•°: {len(session.target)}")
        print(f"ğŸ“Š [SCHEDULER] Engine cacheç®¡ç†: ä¾èµ–æ¨¡å‹max_cache_sizeè¿›è¡Œè‡ªåŠ¨æ»‘åŠ¨çª—å£")
        
        # ğŸ”¥ ä¿®æ”¹è®¾è®¡ï¼šä¼ é€’å®Œæ•´çš„éŸ³é¢‘å†å²ï¼Œè®©æ¨ç†å¼•æ“å¤„ç†å¢é‡é€»è¾‘
        # è¿™æ ·æ¨¡å‹çš„ _prepare_speech æ–¹æ³•èƒ½æ­£ç¡®ä½¿ç”¨ src_len è¿›è¡Œå¢é‡å¤„ç†
        full_audio_data = session.source
        speech_batch_for_processing = torch.tensor(full_audio_data, dtype=torch.float32)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ•°æ®éœ€è¦å¤„ç†
        new_samples_count = len(session.source) - session.src_len
        print(f"ğŸ” [SCHEDULER] Passing full audio history: {len(full_audio_data)} samples")
        print(f"ğŸ” [SCHEDULER] New samples in this batch: {new_samples_count}")
        
        if new_samples_count <= 0:
            print(f"âš ï¸ [SCHEDULER] No new audio data to process for user {user_id}")
            raise ValueError("No new audio data to process")
        elif new_samples_count < MIN_AUDIO_LENGTH:
            print(f"âš ï¸ [SCHEDULER] New audio data too short ({new_samples_count} samples), but processing anyway")
        
        # Prepare input data 
        request_id = str(uuid.uuid4())
        
        # ğŸ”¥ ç®€åŒ–ï¼šscheduleråªæä¾›placeholderï¼Œè®©inference engineè°ƒç”¨modelçš„_prepare_inputså¤„ç†
        # è¿™æ ·ä¿æŒäº†ä¸åŸå§‹infinisst_faster.policy()å®Œå…¨ä¸€è‡´çš„è¡Œä¸º
        input_ids = torch.tensor([[1]], dtype=torch.long)  # ç®€å•çš„placeholder
        
        print(f"ğŸ”§ [SCHEDULER] ä½¿ç”¨placeholder input_idsï¼Œinference engineå°†è°ƒç”¨model._prepare_inputs")
        
        # ä¿®å¤speech batchçš„ç»´åº¦å¤„ç† - ä½¿ç”¨å®Œæ•´çš„éŸ³é¢‘å†å²
        if speech_batch_for_processing.dim() == 1:
            speech_batch = speech_batch_for_processing.unsqueeze(0)  # [seq_len] -> [1, seq_len]
        else:
            speech_batch = speech_batch_for_processing
        
        request = InferenceRequest(
            request_id=request_id,
            user_id=user_id,
            language_id=language_id,
            session_id=session.session_id,
            stage=stage,
            speech_batch=speech_batch,
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            speech_cache=session.speech_cache,
            past_key_values=session.past_key_values,
            result_callback=result_callback,
            # ğŸ”¥ ä¼ é€’ä¼šè¯çŠ¶æ€ä¿¡æ¯
            segment_idx=session.segment_idx,
            translations_list=session.target,
            session_src_len=session.src_len,
            beam_state=session.beam_state
        )
        
        print(f"ğŸ” [SCHEDULER] Created request with session_src_len={session.src_len}")
        
        # Add to appropriate queue
        with self.queue_lock:
            if stage == RequestStage.PREFILL:
                self.prefill_queues[gpu_id].append(request)
                self.stats['queue_sizes'][gpu_id]['prefill'] += 1
            else:
                self.decode_queues[gpu_id].append(request)
                self.stats['queue_sizes'][gpu_id]['decode'] += 1
            
            self.stats['total_requests'] += 1
        
        logger.debug(f"Submitted {stage.value} request {request_id} for user {user_id}, language {language_id}, GPU {gpu_id}")
        return request_id
    
    def _processing_loop(self, gpu_id: int):
        """
        Main processing loop for a specific GPU
        Implements the scheduling policy: PREFILL queue has priority over DECODE queue
        """
        language_id = self.gpu_language_map[gpu_id]
        logger.info(f"Starting processing loop for GPU {gpu_id} (language: {language_id})")
        
        # ğŸ”¥ æ·»åŠ ï¼šå¡ä½æ£€æµ‹è®¡æ—¶å™¨
        last_diagnosis_time = time.time()
        diagnosis_interval = 60  # æ¯60ç§’è¯Šæ–­ä¸€æ¬¡
        
        while self.is_running:
            try:
                # Get batch of requests following the priority rule
                batch = self._get_request_batch(gpu_id)
                
                if not batch:
                    time.sleep(0.001)  
                    # ğŸ”¥ æ·»åŠ ï¼šåœ¨ç©ºé—²æ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦è¯Šæ–­
                    current_time = time.time()
                    if current_time - last_diagnosis_time > diagnosis_interval:
                        self._auto_diagnose_stuck_sessions(gpu_id)
                        last_diagnosis_time = current_time
                    continue
                
                # Process the batch
                self._process_batch(batch, gpu_id)
                
                # Clean up old sessions periodically
                if time.time() % 60 < 1:  # Every minute
                    self._cleanup_sessions()
                
                # ğŸ”¥ æ·»åŠ ï¼šå®šæœŸè¯Šæ–­æ£€æŸ¥
                current_time = time.time()
                if current_time - last_diagnosis_time > diagnosis_interval:
                    self._auto_diagnose_stuck_sessions(gpu_id)
                    last_diagnosis_time = current_time
                
            except Exception as e:
                logger.error(f"Error in processing loop for GPU {gpu_id}: {e}")
                time.sleep(0.1)  # Brief pause on error
        
        logger.info(f"Processing loop stopped for GPU {gpu_id}")
    
    def _get_request_batch(self, gpu_id: int) -> List[InferenceRequest]:
        """
        Get a HOMOGENEOUS batch of requests (either all PREFILL or all DECODE)
        
        Scheduling Policy:
        1. If PREFILL queue has requests: Create pure PREFILL batch (up to 32 requests)
        2. If PREFILL queue is empty: Create pure DECODE batch (up to 32 requests)
        3. NEVER mix PREFILL and DECODE in the same batch
        """
        batch = []
        
        with self.queue_lock:
            prefill_queue = self.prefill_queues[gpu_id]
            decode_queue = self.decode_queues[gpu_id]
            
            # Priority 1: Create PREFILL batch
            if prefill_queue:
                while len(batch) < self.max_batch_size and prefill_queue:
                    try:
                        request = prefill_queue.popleft()
                        batch.append(request)
                        self.stats['queue_sizes'][gpu_id]['prefill'] -= 1
                    except IndexError:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºå¾ªç¯
                        print(f"âš ï¸ [SCHEDULER] Prefill queue empty during pop for GPU {gpu_id}")
                        break
                # decouple PD
                if batch:
                    assert all(req.stage == RequestStage.PREFILL for req in batch)
                    logger.debug(f"Created PREFILL batch of size {len(batch)} for GPU {gpu_id}")
            
            # Priority 2: Create  DECODE batch ( if no PREFILL requests)
            elif decode_queue:
                while len(batch) < self.max_batch_size and decode_queue:
                    try:
                        request = decode_queue.popleft()
                        batch.append(request)
                        self.stats['queue_sizes'][gpu_id]['decode'] -= 1
                    except IndexError:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºå¾ªç¯
                        print(f"âš ï¸ [SCHEDULER] Decode queue empty during pop for GPU {gpu_id}")
                        break
                
                if batch:
                    assert all(req.stage == RequestStage.DECODE for req in batch)
                    logger.debug(f"Created DECODE batch of size {len(batch)} for GPU {gpu_id}")
        
        return batch
    
    def _process_batch(self, batch: List[InferenceRequest], gpu_id: int):
        """
        Process a batch of requests using inference engine only (no simulation)
        """
        if not batch:
            return
        
        language_id = self.gpu_language_map[gpu_id]
        logger.debug(f"Processing batch of {len(batch)} requests on GPU {gpu_id} for language {language_id}")
        
        # Mark requests as processing
        for request in batch:
            request.is_processing = True
        
        try:
            # ğŸ”¥ åªä½¿ç”¨çœŸå®æ¨ç†å¼•æ“ï¼Œä¸å†ä½¿ç”¨æ¨¡æ‹Ÿæ¨ç†
            if hasattr(self, 'inference_engine') and self.inference_engine:
                try:
                    # ğŸ” å¤„ç†å‰è®°å½•é¡µé¢æ± çŠ¶æ€
                    print(f"ğŸ“Š [SCHEDULER] GPU {gpu_id} å¼€å§‹å¤„ç† {len(batch)} ä¸ªè¯·æ±‚")
                    for i, req in enumerate(batch):
                        audio_len = req.speech_batch.shape[-1] if hasattr(req.speech_batch, 'shape') else len(req.speech_batch)
                        print(f"   - Request {i+1}: {audio_len} samples, stage={req.stage.value}")
                    
                    results = self.inference_engine.process_batch(gpu_id, batch)
                    
                    # ğŸ” å¤„ç†åè®°å½•ç»“æœ
                    print(f"ğŸ“Š [SCHEDULER] GPU {gpu_id} å®Œæˆå¤„ç†ï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
                    
                    # å¤„ç†æ¨ç†ç»“æœ
                    for i, request in enumerate(batch):
                        if i < len(results):
                            result = results[i]
                            success = result.get('success', False)
                            error = result.get('error', 'None')
                            print(f"   - Request {i+1} ç»“æœ: success={success}, error={error}")
                            self._update_session_with_result(request, result)
                            logger.debug(f"Request {request.request_id} completed with inference engine")
                        else:
                            # å¤„ç†ç¼ºå¤±çš„ç»“æœ
                            print(f"   - Request {i+1} ç¼ºå¤±ç»“æœ")
                            self._handle_failed_request(request, "Missing inference result")
                            
                except Exception as e:
                    logger.error(f"Inference engine failed for GPU {gpu_id}: {e}")
                    
                    # ğŸ”¥ æ™ºèƒ½é”™è¯¯å¤„ç†ï¼šæ ¹æ®é”™è¯¯ç±»å‹å†³å®šå¤„ç†ç­–ç•¥
                    if "é¡µé¢æ± è€—å°½" in str(e) or "page" in str(e).lower() or "memory" in str(e).lower():
                        logger.warning(f"GPUå†…å­˜ä¸è¶³ï¼Œå°†è¯·æ±‚é‡æ–°æ’é˜Ÿç­‰å¾…...")
                        
                        # å°†è¯·æ±‚é‡æ–°æ”¾å›é˜Ÿåˆ—ç­‰å¾…
                        self._requeue_requests_for_memory_wait(batch, gpu_id)
                        
                        # å°è¯•æ¸…ç†ä¸æ´»è·ƒçš„ä¼šè¯
                        self._emergency_cleanup_sessions()
                        
                    else:
                        # å…¶ä»–é”™è¯¯ï¼šæ ‡è®°æ‰€æœ‰è¯·æ±‚å¤±è´¥
                        for request in batch:
                            self._handle_failed_request(request, f"Inference engine error: {str(e)}")
            else:
                # æ²¡æœ‰æ¨ç†å¼•æ“å¯ç”¨
                logger.error(f"No inference engine available for GPU {gpu_id}")
                for request in batch:
                    self._handle_failed_request(request, "Inference engine not available")
                
        except Exception as e:
            logger.error(f"Batch processing failed on GPU {gpu_id}: {e}")
            # å¤„ç†æ‰€æœ‰è¯·æ±‚çš„é”™è¯¯
            for request in batch:
                self._handle_failed_request(request, f"Batch processing failed: {str(e)}")
    
    def _requeue_requests_for_memory_wait(self, batch: List[InferenceRequest], gpu_id: int):
        """å°†å†…å­˜ä¸è¶³çš„è¯·æ±‚é‡æ–°æ”¾å›é˜Ÿåˆ—ç­‰å¾…"""
        with self.queue_lock:
            for request in batch:
                # é‡ç½®è¯·æ±‚çŠ¶æ€
                request.is_processing = False
                request.is_completed = False
                
                # æ·»åŠ é‡è¯•æ ‡è®°
                if not hasattr(request, 'retry_count'):
                    request.retry_count = 0
                request.retry_count += 1
                
                # é™åˆ¶é‡è¯•æ¬¡æ•°ï¼Œé¿å…æ— é™é‡è¯•
                max_retries = 3
                if request.retry_count <= max_retries:
                    # é‡æ–°æ”¾å›å¯¹åº”çš„é˜Ÿåˆ—
                    if request.stage == RequestStage.PREFILL:
                        self.prefill_queues[gpu_id].appendleft(request)  # æ”¾åˆ°é˜Ÿåˆ—å‰é¢ï¼Œä¼˜å…ˆå¤„ç†
                        self.stats['queue_sizes'][gpu_id]['prefill'] += 1
                        logger.info(f"Request {request.request_id} requeued for memory wait (retry {request.retry_count}/{max_retries})")
                    else:
                        self.decode_queues[gpu_id].appendleft(request)
                        self.stats['queue_sizes'][gpu_id]['decode'] += 1
                        logger.info(f"Request {request.request_id} requeued for memory wait (retry {request.retry_count}/{max_retries})")
                else:
                    # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°å¤±è´¥
                    logger.error(f"Request {request.request_id} exceeded max retries ({max_retries}) due to memory issues")
                    self._handle_failed_request(request, f"GPU memory exhausted after {max_retries} retries")
    
    def _update_session_with_result(self, request: InferenceRequest, result: Dict[str, Any]):
        """ä½¿ç”¨æ¨ç†ç»“æœæ›´æ–°ç”¨æˆ·ä¼šè¯ - ORCAé£æ ¼åˆ†æ­¥å¤„ç†"""
        try:
            # æ›´æ–°ç”¨æˆ·ä¼šè¯
            session = self.user_sessions[request.language_id][request.user_id]
            
            if result.get('success', False):
                # ğŸ”¥ ORCAé£æ ¼ï¼šæ ¹æ®å¤„ç†é˜¶æ®µæ›´æ–°çŠ¶æ€
                prefill_finished = result.get('prefill_finished', False)
                decode_finished = result.get('decode_finished', False)
                
                if prefill_finished and not hasattr(request, '_prefill_done'):
                    # Prefillé˜¶æ®µåˆšå®Œæˆ
                    print(f"ğŸ” [ORCA-SCHEDULER] Request {request.request_id} prefillå®Œæˆ")
                    request._prefill_done = True
                    
                    # å°†requestçŠ¶æ€åˆ‡æ¢åˆ°DECODE
                    request.stage = RequestStage.DECODE
                    
                    # Prefillé˜¶æ®µé€šå¸¸ä¸ç”Ÿæˆæœ€ç»ˆæ–‡æœ¬ï¼Œåªæ˜¯å‡†å¤‡beamçŠ¶æ€
                    generated_text = result.get('generated_text', '')
                    if generated_text:
                        print(f"ğŸ” [ORCA-SCHEDULER] Prefillç”Ÿæˆåˆå§‹æ–‡æœ¬: '{generated_text}'")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°sessionå’Œrequestçš„ç¼“å­˜çŠ¶æ€
                    if 'speech_cache' in result:
                        session.speech_cache = result['speech_cache']
                        request.speech_cache = result['speech_cache']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] æ›´æ–°speech_cacheå¼•ç”¨")
                    
                    if 'past_key_values' in result:
                        session.past_key_values = result['past_key_values']
                        request.past_key_values = result['past_key_values']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] æ›´æ–°past_key_valueså¼•ç”¨")
                        
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿å­˜beam_state
                    if hasattr(request, 'beam_state') and request.beam_state is not None:
                        session.beam_state = request.beam_state
                        print(f"ğŸ” [ORCA-CACHE] ä¿å­˜beam_stateåˆ°session")
                        
                    # ğŸ”¥ å…³é”®ï¼šå°†requesté‡æ–°æ”¾å›DECODEé˜Ÿåˆ—ç»§ç»­å¤„ç†
                    with self.queue_lock:
                        gpu_id = self.language_gpu_map[request.language_id]
                        self.decode_queues[gpu_id].append(request)
                        self.stats['queue_sizes'][gpu_id]['decode'] += 1
                        print(f"ğŸ”„ [ORCA-SCHEDULER] Request {request.request_id} å·²æ”¾å›DECODEé˜Ÿåˆ— (cacheå·²æ›´æ–°)")
                
                elif request.stage == RequestStage.DECODE:
                    # Decodeé˜¶æ®µ - ç”Ÿæˆäº†æ–°çš„token
                    generated_text = result.get('generated_text', '')
                    generated_tokens = result.get('generated_tokens', [])
                    finished = result.get('finished', False)
                    
                    print(f"ğŸ” [ORCA-SCHEDULER] Decode step: '{generated_text}', finished={finished}")
                    
                    # ğŸ”¥ ä¿®å¤ï¼šç´¯ç§¯å¼æ›´æ–°ç¿»è¯‘å†å²
                    if generated_text:
                        # è·å–å½“å‰å®Œæ•´ç¿»è¯‘å†å²
                        is_chinese_translation = "Chinese" in request.language_id or "zh" in request.language_id.lower()
                        
                        if is_chinese_translation:
                            current_full_text = ''.join(session.target)
                        else:
                            current_full_text = ' '.join(session.target)
                        
                        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŸºäºsrc_lenåˆ¤æ–­æ˜¯å¦ä¸ºæ–°éŸ³é¢‘ç‰‡æ®µ
                        if generated_text.strip() != current_full_text.strip():
                            # æ£€æŸ¥æ˜¯å¦å¤„ç†äº†æ–°çš„éŸ³é¢‘æ•°æ®
                            if not hasattr(session, 'last_processed_src_len'):
                                session.last_processed_src_len = 0
                            
                            current_src_len = request.session_src_len
                            is_new_audio_segment = current_src_len > session.last_processed_src_len
                            
                            if not session.target:
                                # ç¬¬ä¸€ä¸ªç¿»è¯‘ç‰‡æ®µ
                                session.target = [generated_text]
                                session.last_processed_src_len = current_src_len
                                print(f"ğŸ” [ORCA-SCHEDULER] å¼€å§‹æ–°ç¿»è¯‘: '{generated_text}' (src_len: {current_src_len})")
                            elif is_new_audio_segment:
                                # æ–°çš„éŸ³é¢‘ç‰‡æ®µï¼Œæ·»åŠ æ–°çš„ç¿»è¯‘æ®µè½
                                session.target.append(generated_text)
                                session.last_processed_src_len = current_src_len
                                print(f"ğŸ” [ORCA-SCHEDULER] æ–°éŸ³é¢‘ç‰‡æ®µç¿»è¯‘: '{generated_text}' (src_len: {session.last_processed_src_len} -> {current_src_len})")
                                print(f"ğŸ” [ORCA-SCHEDULER] ç¿»è¯‘å†å²å…± {len(session.target)} ä¸ªæ®µè½")
                            else:
                                # åŒä¸€éŸ³é¢‘ç‰‡æ®µçš„ç¿»è¯‘æ‰©å±•ï¼Œæ›¿æ¢æœ€åä¸€ä¸ªç¿»è¯‘
                                session.target[-1] = generated_text
                                print(f"ğŸ” [ORCA-SCHEDULER] æ‰©å±•å½“å‰ç¿»è¯‘: '{generated_text}' (åŒä¸€éŸ³é¢‘ç‰‡æ®µ, src_len: {current_src_len})")
                                print(f"ğŸ” [ORCA-SCHEDULER] ç¿»è¯‘å†å²å…± {len(session.target)} ä¸ªæ®µè½")
                            
                            # è®¡ç®—å‘é€ç»™å‰ç«¯çš„å®Œæ•´ç¿»è¯‘
                            if is_chinese_translation:
                                new_full_text = ''.join(session.target)
                            else:
                                new_full_text = ' '.join(session.target)
                            
                            # è®¡ç®—æ–°å¢çš„å†…å®¹ï¼ˆç›¸å¯¹äºä¸Šæ¬¡å‘é€çš„ï¼‰
                            if current_full_text:
                                new_segment = new_full_text.replace(current_full_text, "").strip()
                            else:
                                new_segment = new_full_text.strip()
                            
                            result['new_segment'] = new_segment
                            result['segment_count'] = len(session.target)
                            result['full_translation'] = new_full_text
                        else:
                            print(f"ğŸ” [ORCA-SCHEDULER] ç¿»è¯‘æœªå˜åŒ–ï¼Œè·³è¿‡æ›´æ–°")
                            result['new_segment'] = ""
                            result['segment_count'] = len(session.target)
                            # è¿”å›å½“å‰å®Œæ•´ç¿»è¯‘å†å²
                            if is_chinese_translation:
                                result['full_translation'] = ''.join(session.target)
                            else:
                                result['full_translation'] = ' '.join(session.target)
                    
                    # æ›´æ–°tokenåºåˆ—
                    if generated_tokens:
                        session.target_ids = generated_tokens.copy()  # å®Œå…¨æ›¿æ¢
                        print(f"ğŸ” [ORCA-SCHEDULER] æ›´æ–°tokenåºåˆ—: {len(session.target_ids)} tokens")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°sessionå’Œrequestçš„ç¼“å­˜çŠ¶æ€
                    if 'speech_cache' in result:
                        session.speech_cache = result['speech_cache']
                        request.speech_cache = result['speech_cache']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] Decodeé˜¶æ®µæ›´æ–°speech_cacheå¼•ç”¨")
                    
                    if 'past_key_values' in result:
                        session.past_key_values = result['past_key_values']
                        request.past_key_values = result['past_key_values']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] Decodeé˜¶æ®µæ›´æ–°past_key_valueså¼•ç”¨")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿å­˜beam_state
                    if hasattr(request, 'beam_state') and request.beam_state is not None:
                        session.beam_state = request.beam_state
                        print(f"ğŸ” [ORCA-CACHE] ä¿å­˜beam_stateåˆ°session")
                        
                    # ğŸ”¥ å…³é”®ï¼šå¦‚æœè¿˜æ²¡å®Œæˆï¼Œç»§ç»­æ”¾å›DECODEé˜Ÿåˆ—
                    if not finished and not decode_finished:
                        with self.queue_lock:
                            gpu_id = self.language_gpu_map[request.language_id]
                            self.decode_queues[gpu_id].append(request)
                            self.stats['queue_sizes'][gpu_id]['decode'] += 1
                            print(f"ğŸ”„ [ORCA-SCHEDULER] Request {request.request_id} ç»§ç»­DECODEï¼Œå·²é‡æ–°å…¥é˜Ÿ (cacheå·²æ›´æ–°)")
                    else:
                        print(f"âœ… [ORCA-SCHEDULER] Request {request.request_id} ç¿»è¯‘å®Œæˆ")
                        # æ›´æ–° src_len åˆ°å½“å‰ session.source çš„é•¿åº¦
                        session.src_len = len(session.source)
                        print(f"ğŸ” [ORCA-SCHEDULER] Final src_len updated to {session.src_len}")
            
            session.last_activity = time.time()
            
            # ğŸ”¥ å…³é”®ï¼šåªåœ¨çœŸæ­£å®Œæˆæ—¶æ‰æ ‡è®°requestå®Œæˆå’Œè°ƒç”¨å›è°ƒ
            finished = result.get('finished', False) or result.get('decode_finished', False)
            
            if finished:
                # æ ‡è®°è¯·æ±‚å®Œæˆ
                request.result = result
                request.is_completed = True
                request.is_processing = False
                
                # è°ƒç”¨å›è°ƒå‡½æ•°
                if request.result_callback:
                    try:
                        request.result_callback(result)
                    except Exception as e:
                        logger.error(f"Error in result callback for request {request.request_id}: {e}")
                
                self.stats['completed_requests'] += 1
                print(f"ğŸ“¤ [ORCA-SCHEDULER] å‘é€æœ€ç»ˆç»“æœåˆ°å®¢æˆ·ç«¯: '{result.get('generated_text', '')}'")
            else:
                # ä¸­é—´æ­¥éª¤ï¼Œä¸è°ƒç”¨å›è°ƒï¼Œç»§ç»­å¤„ç†
                print(f"ğŸ”„ [ORCA-SCHEDULER] ä¸­é—´æ­¥éª¤å®Œæˆï¼Œç»§ç»­å¤„ç†...")
            
        except Exception as e:
            logger.error(f"Error updating session for request {request.request_id}: {e}")
            self._handle_failed_request(request, f"Session update failed: {str(e)}")
    
    def _handle_failed_request(self, request: InferenceRequest, error_msg: str):
        """å¤„ç†å¤±è´¥çš„è¯·æ±‚"""
        error_result = {
            'request_id': request.request_id,
            'success': False,
            'error': error_msg,
            'generated_text': '',
            'generated_tokens': [],
            'stage': request.stage.value
        }
        
        request.result = error_result
        request.is_completed = True
        request.is_processing = False
        
        if request.result_callback:
            try:
                request.result_callback(error_result)
            except Exception as e:
                logger.error(f"Error in error callback for request {request.request_id}: {e}")
        
        self.stats['failed_requests'] = self.stats.get('failed_requests', 0) + 1
    
    def set_inference_engine(self, inference_engine):
        """è®¾ç½®æ¨ç†å¼•æ“"""
        self.inference_engine = inference_engine
        logger.info("æ¨ç†å¼•æ“å·²è®¾ç½®åˆ°è°ƒåº¦å™¨")
    
    def _cleanup_sessions(self):
        """Clean up old/inactive sessions"""
        current_time = time.time()
        sessions_to_remove = []
        
        with self.session_lock:
            for language_id, user_sessions in self.user_sessions.items():
                for user_id, session in list(user_sessions.items()):
                    if current_time - session.last_activity > self.session_timeout:
                        sessions_to_remove.append((language_id, user_id))
            
            # Remove expired sessions
            for language_id, user_id in sessions_to_remove:
                if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                    del self.user_sessions[language_id][user_id]
                    self.stats['active_sessions'] -= 1
                    logger.info(f"Cleaned up expired session for user {user_id}, language {language_id}")
    
    def get_session_info(self, user_id: str, language_id: str) -> Optional[Dict[str, Any]]:
        """Get information about a user session"""
        with self.session_lock:
            if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                session = self.user_sessions[language_id][user_id]
                return {
                    'session_id': session.session_id,
                    'user_id': session.user_id,
                    'language_id': session.language_id,
                    'source_length': len(session.source),
                    'source_finished': session.source_finished,
                    'target_segments': len(session.target),
                    'segment_idx': session.segment_idx,
                    'last_activity': session.last_activity,
                    'created_at': session.created_at
                }
        return None
    
    def reset_session(self, user_id: str, language_id: str) -> bool:
        """Reset a user session"""
        with self.session_lock:
            if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                session = self.user_sessions[language_id][user_id]
                session.reset()
                logger.info(f"Reset session for user {user_id}, language {language_id}")
                return True
        return False
    
    def get_queue_stats(self) -> Dict[str, Any]:
        """Get current queue and system statistics"""
        with self.queue_lock:
            current_stats = self.stats.copy()
            current_stats['gpu_language_map'] = self.gpu_language_map.copy()
            current_stats['timestamp'] = time.time()
            
            # ğŸ”¥ æ·»åŠ ï¼šè¯¦ç»†çš„é˜Ÿåˆ—è¯Šæ–­ä¿¡æ¯
            current_stats['detailed_queue_info'] = {}
            for gpu_id in self.gpu_language_map.keys():
                prefill_count = len(self.prefill_queues[gpu_id])
                decode_count = len(self.decode_queues[gpu_id])
                
                current_stats['detailed_queue_info'][gpu_id] = {
                    'language': self.gpu_language_map[gpu_id],
                    'prefill_queue_size': prefill_count,
                    'decode_queue_size': decode_count,
                    'total_queue_size': prefill_count + decode_count
                }
            
            # ğŸ”¥ æ·»åŠ ï¼šæ´»è·ƒsessionçš„æœ€åæ´»åŠ¨æ—¶é—´æ£€æŸ¥
            current_time = time.time()
            inactive_sessions = []
            
            with self.session_lock:
                for language_id, user_sessions in self.user_sessions.items():
                    for user_id, session in user_sessions.items():
                        inactive_time = current_time - session.last_activity
                        if inactive_time > 60:  # è¶…è¿‡1åˆ†é’Ÿä¸æ´»è·ƒ
                            inactive_sessions.append({
                                'session_id': session.session_id,
                                'user_id': user_id,
                                'language_id': language_id,
                                'inactive_seconds': inactive_time,
                                'source_length': len(session.source),
                                'target_segments': len(session.target)
                            })
            
            current_stats['inactive_sessions'] = inactive_sessions
            current_stats['inactive_session_count'] = len(inactive_sessions)
            
            return current_stats
    
    def diagnose_stuck_sessions(self) -> Dict[str, Any]:
        """ğŸ” è¯Šæ–­å¯èƒ½å¡ä½çš„session"""
        current_time = time.time()
        stuck_sessions = []
        
        with self.session_lock:
            for language_id, user_sessions in self.user_sessions.items():
                for user_id, session in user_sessions.items():
                    inactive_time = current_time - session.last_activity
                    
                    # æ£€æŸ¥æ˜¯å¦å¯èƒ½å¡ä½ï¼šè¶…è¿‡30ç§’ä¸æ´»è·ƒä¸”æœ‰éŸ³é¢‘æ•°æ®
                    if inactive_time > 30 and len(session.source) > 0:
                        gpu_id = self.language_gpu_map.get(language_id)
                        
                        with self.queue_lock:
                            prefill_queue_size = len(self.prefill_queues.get(gpu_id, []))
                            decode_queue_size = len(self.decode_queues.get(gpu_id, []))
                        
                        stuck_info = {
                            'session_id': session.session_id,
                            'user_id': user_id,
                            'language_id': language_id,
                            'gpu_id': gpu_id,
                            'inactive_seconds': inactive_time,
                            'source_length_samples': len(session.source),
                            'source_length_seconds': len(session.source) / 16000,
                            'target_segments': len(session.target),
                            'src_len_processed': session.src_len,
                            'unprocessed_samples': len(session.source) - session.src_len,
                            'prefill_queue_size': prefill_queue_size,
                            'decode_queue_size': decode_queue_size,
                            'total_queue_size': prefill_queue_size + decode_queue_size,
                            'has_speech_cache': session.speech_cache is not None,
                            'has_past_key_values': session.past_key_values is not None,
                            'has_beam_state': session.beam_state is not None
                        }
                        
                        stuck_sessions.append(stuck_info)
        
        diagnosis = {
            'timestamp': current_time,
            'stuck_sessions': stuck_sessions,
            'stuck_session_count': len(stuck_sessions),
            'analysis': []
        }
        
        # åˆ†æåŸå› 
        for session in stuck_sessions:
            analysis = []
            
            if session['unprocessed_samples'] > 0:
                analysis.append(f"æœ‰ {session['unprocessed_samples']} ä¸ªæ ·æœ¬æœªå¤„ç†")
            
            if session['total_queue_size'] == 0:
                analysis.append("é˜Ÿåˆ—ä¸ºç©º - å¯èƒ½æ²¡æœ‰æ–°è¯·æ±‚æäº¤")
            elif session['total_queue_size'] > 10:
                analysis.append(f"é˜Ÿåˆ—ç§¯å‹ä¸¥é‡ ({session['total_queue_size']} ä¸ªè¯·æ±‚)")
            
            if not session['has_speech_cache']:
                analysis.append("ç¼ºå°‘speech_cache")
            
            if not session['has_past_key_values']:
                analysis.append("ç¼ºå°‘past_key_values")
            
            session['possible_causes'] = analysis
        
        return diagnosis
    
    def get_supported_languages(self) -> List[str]:
        """Get list of supported language pairs"""
        return list(self.language_gpu_map.keys())
    
    def _emergency_cleanup_sessions(self):
        """ç´§æ€¥æ¸…ç†ä¸æ´»è·ƒçš„ä¼šè¯ä»¥é‡Šæ”¾GPUå†…å­˜"""
        current_time = time.time()
        cleaned_sessions = 0
        total_freed_pages = 0
        
        # ç´§æ€¥æ¸…ç†é˜ˆå€¼ï¼š5åˆ†é’Ÿä¸æ´»è·ƒ
        emergency_timeout = 300  # 5 minutes
        
        with self.session_lock:
            sessions_to_remove = []
            
            for language_id, user_sessions in self.user_sessions.items():
                for user_id, session in list(user_sessions.items()):
                    inactive_time = current_time - session.last_activity
                    if inactive_time > emergency_timeout:
                        sessions_to_remove.append((language_id, user_id, session))
            
            # ç§»é™¤è¶…æ—¶çš„ä¼šè¯
            for language_id, user_id, session in sessions_to_remove:
                try:
                    memory_summary = session.get_memory_summary()
                    freed_pages = memory_summary['memory_usage']['total_pages']
                    total_freed_pages += freed_pages
                    
                    # ğŸ”¥ å…³é”®ï¼šè°ƒç”¨æ¨ç†å¼•æ“æ¸…ç†KV cacheé¡µé¢
                    self._cleanup_session_pages(session)
                    
                    if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                        del self.user_sessions[language_id][user_id]
                        self.stats['active_sessions'] -= 1
                        cleaned_sessions += 1
                        
                        logger.info(f"ğŸ§¹ ç´§æ€¥æ¸…ç†ä¼šè¯ {session.session_id}ï¼Œé‡Šæ”¾ {freed_pages} é¡µå†…å­˜")
                        logger.info(f"   - ä¸æ´»è·ƒæ—¶é—´: {inactive_time:.1f}s")
                        logger.info(f"   - ç”¨æˆ·: {user_id}, è¯­è¨€: {language_id}")
                        
                except Exception as e:
                    logger.error(f"Error during emergency cleanup of session {session.session_id}: {e}")
        
        if cleaned_sessions > 0:
            logger.info(f"ğŸ§¹ ç´§æ€¥æ¸…ç†å®Œæˆï¼šæ¸…ç†äº† {cleaned_sessions} ä¸ªä¼šè¯ï¼Œé‡Šæ”¾ {total_freed_pages} é¡µå†…å­˜")
            
            # ğŸ”¥ æœ€åæ‰‹æ®µï¼šå¦‚æœè¿˜æ˜¯å†…å­˜ä¸è¶³ï¼Œå¼ºåˆ¶é‡ç½®æ‰€æœ‰é¡µé¢è¡¨
            if total_freed_pages < 100:  # å¦‚æœé‡Šæ”¾çš„é¡µé¢å¤ªå°‘
                logger.warning("ğŸš¨ é‡Šæ”¾çš„é¡µé¢ä¸è¶³ï¼Œæ‰§è¡Œå¼ºåˆ¶é¡µé¢è¡¨é‡ç½®")
                self._force_reset_all_pagetables()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # æ¸…ç†GPUç¼“å­˜
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("ğŸ§¹ æ¸…ç†GPUç¼“å­˜å®Œæˆ")
            except Exception as e:
                logger.error(f"Error clearing GPU cache: {e}")
        else:
            logger.warning("ğŸ§¹ ç´§æ€¥æ¸…ç†ï¼šæ²¡æœ‰æ‰¾åˆ°å¯æ¸…ç†çš„ä¼šè¯")
            
            # ğŸ”¥ å³ä½¿æ²¡æœ‰ä¼šè¯å¯æ¸…ç†ï¼Œä¹Ÿå¯èƒ½éœ€è¦é‡ç½®é¡µé¢è¡¨
            logger.warning("ğŸš¨ æ²¡æœ‰ä¼šè¯å¯æ¸…ç†ä½†å†…å­˜ä¸è¶³ï¼Œæ‰§è¡Œå¼ºåˆ¶é¡µé¢è¡¨é‡ç½®")
            self._force_reset_all_pagetables()
    
    def _cleanup_session_pages(self, session: UserSession):
        """æ¸…ç†å•ä¸ªsessionçš„KV cacheé¡µé¢"""
        try:
            if hasattr(self, 'inference_engine') and self.inference_engine:
                # å¯¼å…¥torch
                import torch
                
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„InferenceRequestç”¨äºæ¸…ç†
                cleanup_request = InferenceRequest(
                    request_id=f"cleanup_{session.session_id}",
                    user_id=session.user_id,
                    language_id=session.language_id,
                    session_id=session.session_id,
                    stage=RequestStage.PREFILL,
                    speech_batch=torch.empty(0),
                    input_ids=torch.empty(0, dtype=torch.long),
                    speech_cache=session.speech_cache,
                    past_key_values=session.past_key_values
                    # ç§»é™¤äº† is_final å‚æ•°ï¼Œå› ä¸º InferenceRequest æ²¡æœ‰è¿™ä¸ªå‚æ•°
                )
                
                # ä»æ¨ç†å¼•æ“è·å–å¯¹åº”GPUçš„å¼•æ“å®ä¾‹
                gpu_id = self.language_gpu_map.get(session.language_id)
                if gpu_id is not None:
                    engine = self.inference_engine.get_engine(gpu_id)
                    if engine:
                        logger.info(f"ğŸ§¹ è°ƒç”¨æ¨ç†å¼•æ“æ¸…ç†session {session.session_id} çš„KV cacheé¡µé¢")
                        engine._cleanup_session_kv_cache(cleanup_request)
                        
                        # æ›´æ–°sessionçš„å†…å­˜ä½¿ç”¨ç»Ÿè®¡ä¸º0
                        session.memory_usage = {
                            'speech_pages': 0,
                            'llm_prefill_pages': 0, 
                            'llm_decode_pages': 0,
                            'total_pages': 0,
                            'peak_pages': session.memory_usage.get('peak_pages', 0),
                            'allocation_count': session.memory_usage.get('allocation_count', 0)
                        }
                        
                        logger.info(f"âœ… Session {session.session_id} KV cacheé¡µé¢æ¸…ç†å®Œæˆ")
                    else:
                        logger.warning(f"âš ï¸ æ— æ³•è·å–GPU {gpu_id} çš„æ¨ç†å¼•æ“")
                else:
                    logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°è¯­è¨€ {session.language_id} å¯¹åº”çš„GPU")
            else:
                logger.warning("âš ï¸ æ¨ç†å¼•æ“ä¸å¯ç”¨ï¼Œè·³è¿‡KV cacheé¡µé¢æ¸…ç†")
                
        except Exception as e:
            logger.error(f"æ¸…ç†session {session.session_id} é¡µé¢æ—¶å‡ºé”™: {e}")
    
    def _partial_page_cleanup(self, session: UserSession, request: InferenceRequest):
        """éƒ¨åˆ†é¡µé¢æ¸…ç†ï¼šé‡Šæ”¾ä¸€äº›ä¸å†éœ€è¦çš„é¡µé¢ï¼Œä½†ä¿æŒsessionæ´»è·ƒ"""
        try:
            # ğŸ”¥ ä¿®å¤ï¼šä¸è¦æ¨¡æ‹Ÿé¡µé¢æ¸…ç†ï¼Œé¿å…çŠ¶æ€ä¸ä¸€è‡´
            # åªè®°å½•éœ€è¦æ¸…ç†ï¼Œä¸å®é™…ä¿®æ”¹å†…å­˜ä½¿ç”¨ç»Ÿè®¡
            
            # ğŸ”¥ ç­–ç•¥1ï¼šåªåœ¨éŸ³é¢‘å†å²çœŸçš„è¿‡é•¿æ—¶æ‰è¿›è¡Œæ¸…ç†
            SPEECH_CLEANUP_THRESHOLD_SECONDS = 60  # æé«˜åˆ°60ç§’é˜ˆå€¼
            speech_samples_threshold = SPEECH_CLEANUP_THRESHOLD_SECONDS * session.source_sample_rate
            
            if len(session.source) > speech_samples_threshold:
                logger.info(f"ğŸ§¹ [PARTIAL-CLEANUP] Session {session.session_id} éŸ³é¢‘å†å²è¿‡é•¿ ({len(session.source)/16000:.1f}s)ï¼Œè®°å½•éœ€è¦æ¸…ç†")
                
                # ğŸ”¥ ä¿®å¤ï¼šåªè°ƒç”¨çœŸå®çš„æ¨ç†å¼•æ“æ¸…ç†ï¼Œä¸æ¨¡æ‹Ÿ
                if hasattr(self, 'inference_engine') and self.inference_engine:
                    gpu_id = self.language_gpu_map.get(session.language_id)
                    if gpu_id is not None:
                        engine = self.inference_engine.get_engine(gpu_id)
                        if engine and hasattr(engine, '_partial_cleanup_speech_cache'):
                            # åªæ¸…ç†æ—©æœŸçš„speech cacheï¼Œä¸ä¿®æ”¹sessionç»Ÿè®¡
                            cleanup_ratio = 0.1  # å‡å°‘åˆ°10%
                            engine._partial_cleanup_speech_cache(request, cleanup_ratio)
                            logger.info(f"ğŸ§¹ [PARTIAL-CLEANUP] è°ƒç”¨å¼•æ“æ¸…ç†äº† {cleanup_ratio*100}% çš„speech cacheé¡µé¢")
                        else:
                            logger.info(f"ğŸ§¹ [PARTIAL-CLEANUP] æ¨ç†å¼•æ“ä¸æ”¯æŒéƒ¨åˆ†æ¸…ç†ï¼Œè·³è¿‡")
                    else:
                        logger.warning(f"âš ï¸ [PARTIAL-CLEANUP] æ— æ³•æ‰¾åˆ°GPU ID for language {session.language_id}")
                else:
                    logger.warning(f"âš ï¸ [PARTIAL-CLEANUP] æ¨ç†å¼•æ“ä¸å¯ç”¨")
            
            # ğŸ”¥ ä¿®å¤ï¼šå®Œå…¨ç§»é™¤æ¨¡æ‹Ÿçš„é¡µé¢ç»Ÿè®¡ä¿®æ”¹
            # ä¸å†ä¿®æ”¹ session.memory_usageï¼Œé¿å…çŠ¶æ€ä¸ä¸€è‡´
            logger.debug(f"ğŸ” [PARTIAL-CLEANUP] Session {session.session_id} å½“å‰å†…å­˜ä½¿ç”¨ä¿æŒä¸å˜: {session.memory_usage.get('total_pages', 0)} é¡µ")
                
        except Exception as e:
            logger.error(f"éƒ¨åˆ†é¡µé¢æ¸…ç†æ—¶å‡ºé”™: {e}")
    
    def _force_reset_all_pagetables(self):
        """å¼ºåˆ¶é‡ç½®æ‰€æœ‰GPUçš„é¡µé¢è¡¨ï¼ˆæœ€åæ‰‹æ®µï¼‰"""
        try:
            if hasattr(self, 'inference_engine') and self.inference_engine:
                logger.warning("ğŸš¨ æ‰§è¡Œå¼ºåˆ¶é¡µé¢è¡¨é‡ç½®")
                
                for gpu_id in self.gpu_language_map.keys():
                    engine = self.inference_engine.get_engine(gpu_id)
                    if engine:
                        logger.warning(f"ğŸš¨ å¼ºåˆ¶é‡ç½®GPU {gpu_id} çš„æ‰€æœ‰é¡µé¢è¡¨")
                        engine.force_cleanup_all_sessions()
                    else:
                        logger.error(f"âŒ æ— æ³•è·å–GPU {gpu_id} çš„æ¨ç†å¼•æ“è¿›è¡Œé‡ç½®")
                
                logger.info("âœ… å¼ºåˆ¶é¡µé¢è¡¨é‡ç½®å®Œæˆ")
            else:
                logger.error("âŒ æ¨ç†å¼•æ“ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œå¼ºåˆ¶é‡ç½®")
                
        except Exception as e:
            logger.error(f"å¼ºåˆ¶é‡ç½®é¡µé¢è¡¨æ—¶å‡ºé”™: {e}")
    
    def cleanup_session(self, user_id: str, language_id: str) -> bool:
        """æ‰‹åŠ¨æ¸…ç†æŒ‡å®šçš„ç”¨æˆ·ä¼šè¯"""
        try:
            with self.session_lock:
                if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                    session = self.user_sessions[language_id][user_id]
                    
                    logger.info(f"ğŸ§¹ æ‰‹åŠ¨æ¸…ç†ä¼šè¯: {session.session_id}")
                    
                    # æ¸…ç†KV cacheé¡µé¢
                    self._cleanup_session_pages(session)
                    
                    # ä»ä¼šè¯å­—å…¸ä¸­ç§»é™¤
                    del self.user_sessions[language_id][user_id]
                    self.stats['active_sessions'] -= 1
                    
                    logger.info(f"âœ… ä¼šè¯ {session.session_id} æ¸…ç†å®Œæˆ")
                    return True
                else:
                    logger.warning(f"âš ï¸ ä¼šè¯ä¸å­˜åœ¨: user_id={user_id}, language_id={language_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"æ‰‹åŠ¨æ¸…ç†ä¼šè¯æ—¶å‡ºé”™: {e}")
            return False
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ä¼šè¯çš„å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        memory_stats = {
            'total_sessions': 0,
            'total_pages_used': 0,
            'sessions_by_language': {},
            'top_memory_users': [],
            'memory_distribution': {
                'speech_pages': 0,
                'llm_prefill_pages': 0,
                'llm_decode_pages': 0
            }
        }
        
        all_sessions = []
        
        with self.session_lock:
            for language_id, user_sessions in self.user_sessions.items():
                language_stats = {
                    'session_count': len(user_sessions),
                    'total_pages': 0,
                    'sessions': []
                }
                
                for user_id, session in user_sessions.items():
                    session_summary = session.get_memory_summary()
                    all_sessions.append(session_summary)
                    language_stats['sessions'].append(session_summary)
                    
                    pages = session_summary['memory_usage']['total_pages']
                    language_stats['total_pages'] += pages
                    memory_stats['total_pages_used'] += pages
                    
                    # ç´¯è®¡å„ç±»å‹é¡µé¢ä½¿ç”¨
                    memory_stats['memory_distribution']['speech_pages'] += session_summary['memory_usage'].get('speech_pages', 0)
                    memory_stats['memory_distribution']['llm_prefill_pages'] += session_summary['memory_usage'].get('llm_prefill_pages', 0)
                    memory_stats['memory_distribution']['llm_decode_pages'] += session_summary['memory_usage'].get('llm_decode_pages', 0)
                
                memory_stats['sessions_by_language'][language_id] = language_stats
                memory_stats['total_sessions'] += language_stats['session_count']
        
        # æ‰¾å‡ºå†…å­˜ä½¿ç”¨æœ€å¤šçš„ä¼šè¯
        memory_stats['top_memory_users'] = sorted(
            all_sessions, 
            key=lambda x: x['memory_usage']['total_pages'], 
            reverse=True
        )[:10]  # å‰10ä¸ª 
    
    def _auto_diagnose_stuck_sessions(self, gpu_id: int):
        """ğŸ” è‡ªåŠ¨è¯Šæ–­å½“å‰GPUçš„å¡ä½session"""
        try:
            language_id = self.gpu_language_map[gpu_id]
            diagnosis = self.diagnose_stuck_sessions()
            
            # è¿‡æ»¤åªçœ‹å½“å‰GPUçš„session
            gpu_stuck_sessions = [
                session for session in diagnosis['stuck_sessions'] 
                if session['gpu_id'] == gpu_id
            ]
            
            if gpu_stuck_sessions:
                logger.warning(f"ğŸš¨ [AUTO-DIAGNOSIS] GPU {gpu_id} ({language_id}) å‘ç° {len(gpu_stuck_sessions)} ä¸ªå¯èƒ½å¡ä½çš„session:")
                
                for session in gpu_stuck_sessions:
                    logger.warning(f"   - Session {session['session_id'][:8]}...")
                    logger.warning(f"     ç”¨æˆ·: {session['user_id']}")
                    logger.warning(f"     ä¸æ´»è·ƒ: {session['inactive_seconds']:.1f}s")
                    logger.warning(f"     æœªå¤„ç†éŸ³é¢‘: {session['unprocessed_samples']} æ ·æœ¬")
                    logger.warning(f"     é˜Ÿåˆ—çŠ¶æ€: P{session['prefill_queue_size']} + D{session['decode_queue_size']}")
                    
                    for cause in session['possible_causes']:
                        logger.warning(f"     å¯èƒ½åŸå› : {cause}")
                
                # ğŸ”¥ æ·»åŠ ï¼šè‡ªåŠ¨ä¿®å¤å°è¯•
                self._attempt_auto_fix_stuck_sessions(gpu_stuck_sessions, gpu_id)
            else:
                logger.debug(f"âœ… [AUTO-DIAGNOSIS] GPU {gpu_id} ({language_id}) æ‰€æœ‰sessionæ­£å¸¸è¿è¡Œ")
                
        except Exception as e:
            logger.error(f"è‡ªåŠ¨è¯Šæ–­æ—¶å‡ºé”™: {e}")
    
    def _attempt_auto_fix_stuck_sessions(self, stuck_sessions: List[Dict], gpu_id: int):
        """ğŸ”§ å°è¯•è‡ªåŠ¨ä¿®å¤å¡ä½çš„session"""
        for session_info in stuck_sessions:
            try:
                session_id = session_info['session_id']
                user_id = session_info['user_id']
                language_id = session_info['language_id']
                
                logger.info(f"ğŸ”§ [AUTO-FIX] å°è¯•ä¿®å¤å¡ä½çš„session {session_id[:8]}...")
                
                # ä¿®å¤ç­–ç•¥1ï¼šå¦‚æœæœ‰æœªå¤„ç†çš„éŸ³é¢‘æ•°æ®ï¼Œå°è¯•é‡æ–°æäº¤è¯·æ±‚
                if session_info['unprocessed_samples'] > 0:
                    logger.info(f"ğŸ”§ [AUTO-FIX] æ£€æµ‹åˆ°æœªå¤„ç†éŸ³é¢‘ï¼Œå°è¯•é‡æ–°ç”Ÿæˆè¯·æ±‚...")
                    
                    # è·å–sessionå¯¹è±¡
                    with self.session_lock:
                        if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                            session = self.user_sessions[language_id][user_id]
                            
                            # åˆ›å»ºä¸€ä¸ªæ–°çš„prefillè¯·æ±‚æ¥å¤„ç†æœªå¤„ç†çš„éŸ³é¢‘
                            try:
                                import torch
                                
                                # è®¡ç®—éœ€è¦å¤„ç†çš„éŸ³é¢‘ç‰‡æ®µ
                                unprocessed_audio = session.source[session.src_len:]
                                if len(unprocessed_audio) > 160:  # è‡³å°‘0.01ç§’çš„éŸ³é¢‘
                                    audio_tensor = torch.tensor(unprocessed_audio, dtype=torch.float32)
                                    
                                    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å¤„ç†è¯·æ±‚
                                    def temp_callback(result):
                                        logger.info(f"ğŸ”§ [AUTO-FIX] è‡ªåŠ¨ä¿®å¤è¯·æ±‚å®Œæˆ: {result.get('success', False)}")
                                    
                                    request_id = self.submit_request(
                                        user_id=user_id,
                                        language_id=language_id,
                                        speech_data=audio_tensor,
                                        stage=RequestStage.PREFILL,
                                        is_final=False,
                                        max_new_tokens=10,
                                        result_callback=temp_callback
                                    )
                                    
                                    logger.info(f"ğŸ”§ [AUTO-FIX] é‡æ–°æäº¤è¯·æ±‚ {request_id} å¤„ç† {len(unprocessed_audio)} ä¸ªæœªå¤„ç†æ ·æœ¬")
                                    
                            except Exception as e:
                                logger.error(f"ğŸ”§ [AUTO-FIX] é‡æ–°æäº¤è¯·æ±‚å¤±è´¥: {e}")
                
                # ä¿®å¤ç­–ç•¥2ï¼šå¦‚æœé˜Ÿåˆ—ä¸ºç©ºä½†sessionæœ‰æ•°æ®ï¼Œå¯èƒ½æ˜¯å‰ç«¯åœæ­¢å‘é€æ•°æ®
                elif session_info['total_queue_size'] == 0 and session_info['source_length_samples'] > 0:
                    logger.warning(f"ğŸ”§ [AUTO-FIX] Session {session_id[:8]} å¯èƒ½å‰ç«¯åœæ­¢å‘é€æ•°æ®")
                    logger.warning(f"   å»ºè®®æ£€æŸ¥å‰ç«¯WebSocketè¿æ¥çŠ¶æ€")
                
            except Exception as e:
                logger.error(f"ğŸ”§ [AUTO-FIX] ä¿®å¤session {session_info['session_id'][:8]} æ—¶å‡ºé”™: {e}") 