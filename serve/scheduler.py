import asyncio
import logging
import time
import uuid
from collections import deque
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from threading import Lock, Thread
from enum import Enum
import json
import statistics

import torch
import numpy as np

logger = logging.getLogger(__name__)

class RequestStage(Enum):
    """Request processing stage"""
    PREFILL = "prefill"
    DECODE = "decode"

@dataclass
class CharacterDelay:
    """è®°å½•å•ä¸ªå­—ç¬¦çš„å»¶è¿Ÿä¿¡æ¯"""
    char: str
    segment_id: int
    char_index: int
    input_time: float
    output_time: float
    delay: float

@dataclass
class SegmentLog:
    """Simulevalå…¼å®¹çš„segmentæ—¥å¿—"""
    segment_id: int
    src: str
    tgt: str
    tokens: List[str]
    delays: List[float]
    input_start_time: float
    output_time: float
    average_delay: float

class DelayTracker:
    """å­—ç¬¦çº§å»¶è¿Ÿè¿½è¸ªå™¨ï¼Œç”¨äºè®¡ç®—streamLAAL"""
    
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.character_delays: List[CharacterDelay] = []
        self.segment_logs: List[SegmentLog] = []
        self.current_segment_id = 0
        self.current_input_buffer = ""
        self.current_input_start_time = 0.0
        self.char_input_times: List[float] = []  # è®°å½•æ¯ä¸ªè¾“å…¥å­—ç¬¦çš„æ—¶é—´
        
    def record_input_segment(self, text: str, timestamp: float):
        """è®°å½•è¾“å…¥segmentå’Œæ¯ä¸ªå­—ç¬¦çš„è¾“å…¥æ—¶é—´"""
        self.current_input_buffer += text
        # ä¸ºæ–°è¾“å…¥çš„æ¯ä¸ªå­—ç¬¦è®°å½•æ—¶é—´æˆ³
        for char in text:
            self.char_input_times.append(timestamp)
        
        if not self.current_input_start_time:
            self.current_input_start_time = timestamp
    
    def record_output_segment(self, output_text: str, timestamp: float, is_final: bool = False):
        """è®°å½•è¾“å‡ºsegmentå¹¶è®¡ç®—å­—ç¬¦çº§å»¶è¿Ÿ"""
        if not output_text or not self.char_input_times:
            return
            
        # åˆ†æè¾“å‡ºæ–‡æœ¬çš„æ¯ä¸ªå­—ç¬¦
        output_chars = list(output_text)
        delays = []
        tokens = []
        
        # å¯¹äºæ¯ä¸ªè¾“å‡ºå­—ç¬¦ï¼Œè®¡ç®—ä¸å¯¹åº”è¾“å…¥å­—ç¬¦çš„å»¶è¿Ÿ
        input_char_index = 0
        for i, output_char in enumerate(output_chars):
            # æ‰¾åˆ°å¯¹åº”çš„è¾“å…¥å­—ç¬¦æ—¶é—´ï¼ˆç®€åŒ–åŒ¹é…ç­–ç•¥ï¼‰
            if input_char_index < len(self.char_input_times):
                input_time = self.char_input_times[input_char_index]
                delay = timestamp - input_time
                
                char_delay = CharacterDelay(
                    char=output_char,
                    segment_id=self.current_segment_id,
                    char_index=i,
                    input_time=input_time,
                    output_time=timestamp,
                    delay=delay
                )
                
                self.character_delays.append(char_delay)
                delays.append(delay)
                tokens.append(output_char)
                
                input_char_index += 1
        
        # åˆ›å»ºsegmentæ—¥å¿—
        if delays:
            segment_log = SegmentLog(
                segment_id=self.current_segment_id,
                src=self.current_input_buffer[:len(output_chars)],  # å¯¹åº”çš„è¾“å…¥æ–‡æœ¬
                tgt=output_text,
                tokens=tokens,
                delays=delays,
                input_start_time=self.current_input_start_time,
                output_time=timestamp,
                average_delay=statistics.mean(delays)
            )
            
            self.segment_logs.append(segment_log)
            
            logger.info(f"ğŸ¯ [DELAY-TRACKER] Segment {self.current_segment_id}: {len(delays)} chars, avg delay: {segment_log.average_delay:.3f}s")
        
        if is_final:
            self.current_segment_id += 1
            self.current_input_buffer = ""
            self.current_input_start_time = 0.0
            self.char_input_times = []
    
    def calculate_stream_laal(self) -> float:
        """è®¡ç®—streamLAALï¼ˆæ‰€æœ‰å­—ç¬¦å»¶è¿Ÿçš„å¹³å‡å€¼ï¼‰"""
        if not self.character_delays:
            return 0.0
            
        all_delays = [cd.delay for cd in self.character_delays]
        stream_laal = statistics.mean(all_delays)
        
        logger.info(f"ğŸ“Š [STREAM-LAAL] Session {self.session_id}: {stream_laal:.3f}s (from {len(all_delays)} characters)")
        return stream_laal
    
    def export_simuleval_log(self, filepath: str):
        """å¯¼å‡ºsimulevalå…¼å®¹çš„instance.logæ ¼å¼"""
        simuleval_data = []
        
        for segment_log in self.segment_logs:
            entry = {
                "segment_id": segment_log.segment_id,
                "src": segment_log.src,
                "tgt": segment_log.tgt,
                "tokens": segment_log.tokens,
                "delays": segment_log.delays,
                "input_start_time": segment_log.input_start_time,
                "output_time": segment_log.output_time,
                "average_delay": segment_log.average_delay
            }
            simuleval_data.append(entry)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for entry in simuleval_data:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        
        logger.info(f"ğŸ“ [EXPORT] Simuleval log exported to {filepath} ({len(simuleval_data)} segments)")
        return filepath
    
    def get_statistics(self, include_character_details: bool = False) -> Dict[str, Any]:
        """è·å–å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.character_delays:
            return {"stream_laal": 0.0, "total_characters": 0, "segments": 0}
            
        delays = [cd.delay for cd in self.character_delays]
        
        result = {
            "stream_laal": statistics.mean(delays),
            "min_delay": min(delays),
            "max_delay": max(delays),
            "median_delay": statistics.median(delays),
            "std_delay": statistics.stdev(delays) if len(delays) > 1 else 0.0,
            "total_characters": len(delays),
            "segments": len(self.segment_logs),
            "session_id": self.session_id
        }
        
        # å¦‚æœéœ€è¦è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…å«å­—ç¬¦çº§å»¶è¿Ÿæ•°æ®
        if include_character_details:
            result["character_delays"] = [
                {
                    "char": cd.char,
                    "segment_id": cd.segment_id,
                    "char_index": cd.char_index,
                    "input_time": cd.input_time,
                    "output_time": cd.output_time,
                    "delay": cd.delay
                }
                for cd in self.character_delays
            ]
            
            result["segment_logs"] = [
                {
                    "segment_id": sl.segment_id,
                    "src": sl.src,
                    "tgt": sl.tgt,
                    "tokens": sl.tokens,
                    "delays": sl.delays,
                    "input_start_time": sl.input_start_time,
                    "output_time": sl.output_time,
                    "average_delay": sl.average_delay
                }
                for sl in self.segment_logs
            ]
        
        return result

@dataclass
class UserSession:
    """
    Maintains state for a user session including all necessary information
    Based on the api.py structure and infinisst.py requirements
    """
    user_id: str
    language_id: str
    session_id: str
    
    # Speech processing state
    source: List[float] = field(default_factory=list)  # Audio samples
    source_finished: bool = False
    source_sample_rate: int = 16000
    
    # Translation state
    target: List[str] = field(default_factory=list)
    target_ids: List[int] = field(default_factory=list)
    segment_idx: int = 0
    
    # Cache state
    speech_cache: Optional[Any] = None
    past_key_values: Optional[Any] = None
    
    # ğŸ”¥ æ·»åŠ ï¼šBeam searchçŠ¶æ€
    beam_state: Optional[Any] = None
    
    # ğŸ” å†…å­˜ä½¿ç”¨è¿½è¸ª
    memory_usage: Dict[str, int] = field(default_factory=lambda: {
        'speech_pages': 0,
        'llm_prefill_pages': 0, 
        'llm_decode_pages': 0,
        'total_pages': 0,
        'peak_pages': 0,
        'allocation_count': 0
    })
    
    # Session management
    last_activity: float = field(default_factory=time.time)
    created_at: float = field(default_factory=time.time)
    
    # Translation parameters
    latency_multiplier: int = 2
    max_new_tokens: int = 20
    
    # ğŸ”¥ æ–°å¢ï¼šå»¶è¿Ÿè¿½è¸ª
    delay_tracker: Optional[DelayTracker] = None
    evaluation_mode: bool = False  # æ˜¯å¦å¯ç”¨è¯„ä¼°æ¨¡å¼
    
    def reset(self):
        """Reset session state for new translation"""
        self.source = []
        self.source_finished = False
        self.src_len = 0
        self.target = []
        self.target_ids = []
        self.segment_idx = 0
        self.speech_cache = None
        self.past_key_values = None
        self.beam_state = None  # ğŸ”¥ æ·»åŠ ï¼šé‡ç½®beam_state
        self.last_activity = time.time()
        
        # é‡ç½®å†…å­˜ä½¿ç”¨è¿½è¸ª
        self.memory_usage = {
            'speech_pages': 0,
            'llm_prefill_pages': 0, 
            'llm_decode_pages': 0,
            'total_pages': 0,
            'peak_pages': 0,
            'allocation_count': 0
        }
    
    def update_memory_usage(self, cache_type: str, pages_used: int):
        """æ›´æ–°å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        if cache_type in self.memory_usage:
            self.memory_usage[cache_type] = pages_used
        
        # æ›´æ–°æ€»é¡µé¢æ•°
        total = (self.memory_usage.get('speech_pages', 0) + 
                self.memory_usage.get('llm_prefill_pages', 0) + 
                self.memory_usage.get('llm_decode_pages', 0))
        self.memory_usage['total_pages'] = total
        
        # æ›´æ–°å³°å€¼
        if total > self.memory_usage.get('peak_pages', 0):
            self.memory_usage['peak_pages'] = total
        
        self.memory_usage['allocation_count'] += 1
        
        print(f"ğŸ” [SESSION-MEMORY] {self.session_id} å†…å­˜ä½¿ç”¨:")
        print(f"   - Speech: {self.memory_usage.get('speech_pages', 0)} é¡µ")
        print(f"   - LLM Prefill: {self.memory_usage.get('llm_prefill_pages', 0)} é¡µ")
        print(f"   - LLM Decode: {self.memory_usage.get('llm_decode_pages', 0)} é¡µ")
        print(f"   - æ€»è®¡: {total} é¡µ (å³°å€¼: {self.memory_usage.get('peak_pages', 0)} é¡µ)")
        print(f"   - åˆ†é…æ¬¡æ•°: {self.memory_usage.get('allocation_count', 0)}")
    
    def get_memory_summary(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        return {
            'session_id': self.session_id,
            'user_id': self.user_id,
            'language_id': self.language_id,
            'memory_usage': self.memory_usage.copy(),
            'session_age_seconds': time.time() - self.created_at,
            'inactive_seconds': time.time() - self.last_activity
        }
    
    def __post_init__(self):
        # åˆå§‹åŒ–å»¶è¿Ÿè¿½è¸ªå™¨
        if self.evaluation_mode:
            self.delay_tracker = DelayTracker(self.session_id)
    
    def enable_evaluation_mode(self):
        """å¯ç”¨è¯„ä¼°æ¨¡å¼ï¼Œå¼€å§‹è®°å½•å»¶è¿Ÿ"""
        self.evaluation_mode = True
        if not self.delay_tracker:
            self.delay_tracker = DelayTracker(self.session_id)
        logger.info(f"ğŸ¯ [EVAL] Session {self.session_id} evaluation mode enabled")
    
    def record_input(self, text: str, timestamp: Optional[float] = None):
        """è®°å½•è¾“å…¥æ–‡æœ¬ç”¨äºå»¶è¿Ÿè®¡ç®—"""
        if self.delay_tracker:
            if timestamp is None:
                timestamp = time.time()
            self.delay_tracker.record_input_segment(text, timestamp)
    
    def record_output(self, text: str, timestamp: Optional[float] = None, is_final: bool = False):
        """è®°å½•è¾“å‡ºæ–‡æœ¬å¹¶è®¡ç®—å»¶è¿Ÿ"""
        if self.delay_tracker:
            if timestamp is None:
                timestamp = time.time()
            self.delay_tracker.record_output_segment(text, timestamp, is_final)
    
    def get_stream_laal(self) -> float:
        """è·å–å½“å‰sessionçš„streamLAAL"""
        if self.delay_tracker:
            return self.delay_tracker.calculate_stream_laal()
        return 0.0
    
    def export_delays(self, filepath: str) -> str:
        """å¯¼å‡ºå»¶è¿Ÿæ•°æ®"""
        if self.delay_tracker:
            return self.delay_tracker.export_simuleval_log(filepath)
        return ""
    
    def get_delay_statistics(self, include_character_details: bool = False) -> Dict[str, Any]:
        """è·å–å»¶è¿Ÿç»Ÿè®¡"""
        if self.delay_tracker:
            return self.delay_tracker.get_statistics(include_character_details)
        return {"stream_laal": 0.0, "total_characters": 0, "segments": 0}

@dataclass
class InferenceRequest:
    """
    Single inference request 
    """
    request_id: str
    user_id: str
    language_id: str
    session_id: str
    stage: RequestStage
    
    # Input data 
    speech_batch: torch.Tensor  # Speech input tensor
    input_ids: torch.Tensor     # Text input token IDs
    
    # Generation parameters 
    max_new_tokens: int = 20
    beam_size: int = 1
    do_sample: bool = False
    top_p: float = 0.9
    top_k: int = 50
    temperature: float = 1.0
    repetition_penalty: float = 1.0
    
    # State management
    speech_cache: Optional[Any] = None
    past_key_values: Optional[Any] = None
    encoder_input_ids: Optional[torch.Tensor] = None
    segment_idx: int = 0
    translations_list: List[str] = field(default_factory=list)
    
    beam_state: Optional[Any] = None
    
    # Metadata
    timestamp: float = field(default_factory=time.time)
    priority: int = 0  # Higher number = higher priority
    
    # ğŸ”¥ åŠ¨æ€å±æ€§æ”¯æŒ
    retry_count: Optional[int] = None  # é‡è¯•æ¬¡æ•°
    queue_enter_time: Optional[float] = None  # å…¥é˜Ÿæ—¶é—´
    queue_exit_time: Optional[float] = None  # å‡ºé˜Ÿæ—¶é—´
    queue_wait_time: Optional[float] = None  # ç­‰å¾…æ—¶é—´
    process_start_time: Optional[float] = None  # å¤„ç†å¼€å§‹æ—¶é—´
    
    # ğŸ”¥ æ·»åŠ åŠ¨æ€å±æ€§æ”¯æŒ
    def __post_init__(self):
        # å…è®¸åŠ¨æ€æ·»åŠ å±æ€§
        pass
    
    def __setattr__(self, name, value):
        # å…è®¸åŠ¨æ€è®¾ç½®å±æ€§
        super().__setattr__(name, value)
    
    # Result handling
    result_callback: Optional[Callable] = None
    is_processing: bool = False
    is_completed: bool = False
    result: Optional[Dict[str, Any]] = None

class LLMScheduler:
    """
    - Maintains two FCFS queues (PREFILL and DECODE)
    - Prioritizes PREFILL queue over DECODE queue
    - Maximum batch size of 32 requests
    """
    
    def __init__(self, gpu_language_map: Dict[int, str], args=None):
        """
        Initialize scheduler with GPU-to-language mapping
        
        Args:
            gpu_language_map: Dict mapping GPU ID to language pair (e.g., {0: "en-zh", 1: "en-de"})
            args: Additional configuration arguments
        """
        self.gpu_language_map = gpu_language_map  # {gpu_id: language_id}
        self.language_gpu_map = {v: k for k, v in gpu_language_map.items()}  # {language_id: gpu_id}
        
        # Configuration
        self.max_batch_size = getattr(args, 'max_batch_size', 32) if args else 32
        self.batch_timeout = getattr(args, 'batch_timeout', 0.1) if args else 0.1  # seconds
        self.session_timeout = getattr(args, 'session_timeout', 3600) if args else 3600  # 1 hour
        
        # ğŸ”¥ Task 3: Dynamic Scheduling Configuration
        self.use_dynamic_schedule = getattr(args, 'use_dynamic_schedule', False) if args else False
        self.dynamic_wait_threshold = getattr(args, 'dynamic_wait_threshold', 0.05) if args else 0.05  # 50ms
        self.dynamic_batch_min_size = getattr(args, 'dynamic_batch_min_size', 1) if args else 1
        
        # FCFS queues - separate queues for each GPU/language
        self.prefill_queues: Dict[int, deque] = {gpu_id: deque() for gpu_id in gpu_language_map.keys()}
        self.decode_queues: Dict[int, deque] = {gpu_id: deque() for gpu_id in gpu_language_map.keys()}
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šç»†ç²’åº¦é” - æ¯ä¸ªGPUçš„prefillå’Œdecodeé˜Ÿåˆ—åˆ†åˆ«æœ‰ç‹¬ç«‹çš„é”
        self.prefill_locks: Dict[int, Lock] = {gpu_id: Lock() for gpu_id in gpu_language_map.keys()}
        self.decode_locks: Dict[int, Lock] = {gpu_id: Lock() for gpu_id in gpu_language_map.keys()}
        
        # User session management
        self.user_sessions: Dict[str, Dict[str, UserSession]] = {}  # {language_id: {user_id: session}}
        
        # ğŸ”¥ Sessioné”ä¿æŒç‹¬ç«‹ï¼Œå› ä¸ºè·¨GPUè®¿é—®
        self.session_lock = Lock()
        
        # Processing state
        self.is_running = False
        self.processing_threads: Dict[int, Thread] = {}  # One thread per GPU
        
        # Statistics
        self.stats = {
            'total_requests': 0,
            'completed_requests': 0,
            'active_sessions': 0,
            'queue_sizes': {gpu_id: {'prefill': 0, 'decode': 0} for gpu_id in gpu_language_map.keys()},
            # ğŸ”¥ Task 3: Dynamic Scheduling Statistics
            'dynamic_triggers': {
                'timeout_triggers': 0,
                'batch_size_triggers': 0,
                'total_dispatches': 0
            }
        }
        
        # ğŸ”¥ æ–°å¢ï¼šé˜Ÿåˆ—ç›‘æ§ç»Ÿè®¡
        self.queue_stats = {
            gpu_id: {
                'prefill': {
                    'total_processed': 0,
                    'total_wait_time': 0.0,
                    'total_process_time': 0.0,
                    'max_wait_time': 0.0,
                    'max_process_time': 0.0,
                    'avg_wait_time': 0.0,
                    'avg_process_time': 0.0,
                    'current_queue_size': 0,
                    'max_queue_size': 0,
                    'last_process_time': 0.0,
                    'throughput_per_sec': 0.0
                },
                'decode': {
                    'total_processed': 0,
                    'total_wait_time': 0.0,
                    'total_process_time': 0.0,
                    'max_wait_time': 0.0,
                    'max_process_time': 0.0,
                    'avg_wait_time': 0.0,
                    'avg_process_time': 0.0,
                    'current_queue_size': 0,
                    'max_queue_size': 0,
                    'last_process_time': 0.0,
                    'throughput_per_sec': 0.0
                }
            } for gpu_id in gpu_language_map.keys()
        }
        
        logger.info(f"LLMScheduler initialized with GPU mapping: {gpu_language_map}")
        logger.info(f"Max batch size: {self.max_batch_size}")
        logger.info(f"ğŸ”’ Fine-grained locks initialized: {len(self.prefill_locks)} prefill locks, {len(self.decode_locks)} decode locks")
    
    def start(self):
        """Start the scheduler processing loops for all GPUs"""
        if self.is_running:
            logger.warning("Scheduler is already running")
            return
        
        self.is_running = True
        
        # Start one processing thread per GPU
        for gpu_id in self.gpu_language_map.keys():
            thread = Thread(target=self._processing_loop, args=(gpu_id,), daemon=True)
            thread.start()
            self.processing_threads[gpu_id] = thread
            logger.info(f"Started processing thread for GPU {gpu_id} (language: {self.gpu_language_map[gpu_id]})")
        
        logger.info("Scheduler started")
    
    def stop(self):
        """Stop all scheduler processing loops"""
        self.is_running = False
        
        # Wait for all threads to finish
        for gpu_id, thread in self.processing_threads.items():
            thread.join(timeout=5.0)
            logger.info(f"Stopped processing thread for GPU {gpu_id}")
        
        self.processing_threads.clear()
        logger.info("Scheduler stopped")
    
    def get_or_create_session(self, user_id: str, language_id: str, session_id: Optional[str] = None, evaluation_mode: bool = False) -> UserSession:
        """Get existing session or create new one"""
        with self.session_lock:
            if language_id not in self.user_sessions:
                self.user_sessions[language_id] = {}
            
            if user_id not in self.user_sessions[language_id]:
                # ä½¿ç”¨æä¾›çš„session_idï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™ç”Ÿæˆæ–°çš„
                if session_id is None:
                    session_id = f"{user_id}_{language_id}_{int(time.time())}"
                
                session = UserSession(
                    user_id=user_id,
                    language_id=language_id,
                    session_id=session_id,
                    evaluation_mode=evaluation_mode  # ğŸ”¥ ä¼ é€’è¯„ä¼°æ¨¡å¼å‚æ•°
                )
                self.user_sessions[language_id][user_id] = session
                
                # ğŸ”¥ æ— é”ç»Ÿè®¡æ›´æ–°ï¼ˆå®¹å¿çŸ­æš‚ä¸ä¸€è‡´ï¼‰
                self.stats['active_sessions'] += 1
                    
                logger.info(f"Created new session {session_id} for user {user_id}, language {language_id}, evaluation_mode={evaluation_mode}")
            else:
                session = self.user_sessions[language_id][user_id]
                session.last_activity = time.time()
                # ğŸ”¥ å¦‚æœç°æœ‰sessionçš„evaluation_modeä¸è¯·æ±‚ä¸åŒï¼Œæ›´æ–°å®ƒ
                if evaluation_mode and not session.evaluation_mode:
                    session.enable_evaluation_mode()
                    logger.info(f"Enabled evaluation mode for existing session {session.session_id}")
            
            return self.user_sessions[language_id][user_id]
    
    def submit_request(self, 
                      user_id: str,
                      language_id: str,
                      speech_data: Union[torch.Tensor, np.ndarray, List[float]],
                      stage: RequestStage = RequestStage.PREFILL,
                      is_final: bool = False,
                      max_new_tokens: int = 20,
                      result_callback: Optional[Callable] = None,
                      api_session_id: Optional[str] = None,
                      evaluation_mode: bool = False) -> str:
        """
        Submit a request to the appropriate queue based on language and stage
        
        Args:
            user_id: Unique identifier for the user
            language_id: Language pair identifier (e.g., "en-zh")
            speech_data: Audio data (will be converted to tensor)
            stage: PREFILL or DECODE stage
            is_final: Whether this is the final segment
            max_new_tokens: Maximum tokens to generate
            result_callback: Callback function for results
            api_session_id: Session ID from API layer (optional, for consistency)
            
        Returns:
            request_id: Unique identifier for this request
        """
        # Validate language support
        if language_id not in self.language_gpu_map:
            raise ValueError(f"Unsupported language pair: {language_id}. Supported: {list(self.language_gpu_map.keys())}")
        
        gpu_id = self.language_gpu_map[language_id]
        
        # Get or create user session
        session = self.get_or_create_session(user_id, language_id, api_session_id, evaluation_mode)
        
        if isinstance(speech_data, (list, np.ndarray)):
            speech_data = torch.tensor(speech_data, dtype=torch.float32)
        elif not isinstance(speech_data, torch.Tensor):
            raise ValueError("speech_data must be list, numpy array, or torch tensor")
        
        session.source = speech_data.tolist() if speech_data.dim() == 1 else speech_data.flatten().tolist()
        session.source_finished = is_final
        session.last_activity = time.time()
        
        # ğŸ¯ è®°å½•è¾“å…¥å»¶è¿Ÿï¼ˆç”¨äºstreamLAALè®¡ç®—ï¼‰
        input_timestamp = time.time()
        if session.evaluation_mode and session.delay_tracker:
            # ç®€åŒ–ï¼šå°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºæ¨¡æ‹Ÿæ–‡æœ¬ä»¥ä¾¿å»¶è¿Ÿè®¡ç®—
            input_text = f"[Audio segment {len(session.source)} samples]"
            session.record_input(input_text, input_timestamp)
            logger.info(f"ğŸ¯ [DELAY] Recorded input for session {session.session_id}: {len(session.source)} audio samples")

        
        # Prepare input data 
        request_id = str(uuid.uuid4())
        input_ids = torch.tensor([[1]], dtype=torch.long)  # ç®€å•çš„placeholder

        request = InferenceRequest(
            request_id=request_id,
            user_id=user_id,
            language_id=language_id,
            session_id=session.session_id,
            stage=stage,
            speech_batch=session.source,
            input_ids=input_ids,
            max_new_tokens=max_new_tokens,
            speech_cache=session.speech_cache,
            past_key_values=session.past_key_values,
            result_callback=result_callback,
            # ğŸ”¥ ä¼ é€’ä¼šè¯çŠ¶æ€ä¿¡æ¯
            segment_idx=session.segment_idx,
            translations_list=session.target,
            beam_state=session.beam_state
        )
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ç»†ç²’åº¦é”æ·»åŠ åˆ°ç›¸åº”é˜Ÿåˆ—
        queue_enter_time = time.time()
        request.queue_enter_time = queue_enter_time  # åŠ¨æ€è®¾ç½®å±æ€§
        
        print(f"ğŸ”’ [FINE-LOCK] Submitting {stage.value} request to GPU {gpu_id}")
        
        if stage.name == RequestStage.PREFILL.name:
            # ğŸ”¥ åªé”å®šprefillé˜Ÿåˆ—ï¼Œä¸å½±å“decodeé˜Ÿåˆ—
            with self.prefill_locks[gpu_id]:
                self.prefill_queues[gpu_id].append(request)
                current_size = len(self.prefill_queues[gpu_id])
                
                # ğŸ”¥ æ›´æ–°é˜Ÿåˆ—ç»Ÿè®¡ï¼ˆæ— é”ï¼Œå®¹å¿çŸ­æš‚ä¸ä¸€è‡´ï¼‰
                self.stats['queue_sizes'][gpu_id]['prefill'] += 1
                self.queue_stats[gpu_id]['prefill']['current_queue_size'] = current_size
                if current_size > self.queue_stats[gpu_id]['prefill']['max_queue_size']:
                    self.queue_stats[gpu_id]['prefill']['max_queue_size'] = current_size
                    print(f"ğŸ“Š [QUEUE-STATS] GPU {gpu_id} Prefillé˜Ÿåˆ—æ–°å³°å€¼: {current_size}")
                        
                print(f"ğŸ”’ [FINE-LOCK] âœ… Prefill request added to GPU {gpu_id}, queue size: {current_size}")
        else:
            # ğŸ”¥ åªé”å®šdecodeé˜Ÿåˆ—ï¼Œä¸å½±å“prefillé˜Ÿåˆ—
            with self.decode_locks[gpu_id]:
                self.decode_queues[gpu_id].append(request)
                current_size = len(self.decode_queues[gpu_id])
                
                # ğŸ”¥ æ›´æ–°é˜Ÿåˆ—ç»Ÿè®¡ï¼ˆæ— é”ï¼Œå®¹å¿çŸ­æš‚ä¸ä¸€è‡´ï¼‰
                self.stats['queue_sizes'][gpu_id]['decode'] += 1
                self.queue_stats[gpu_id]['decode']['current_queue_size'] = current_size
                if current_size > self.queue_stats[gpu_id]['decode']['max_queue_size']:
                    self.queue_stats[gpu_id]['decode']['max_queue_size'] = current_size
                    print(f"ğŸ“Š [QUEUE-STATS] GPU {gpu_id} Decodeé˜Ÿåˆ—æ–°å³°å€¼: {current_size}")
                        
                print(f"ğŸ”’ [FINE-LOCK] âœ… Decode request added to GPU {gpu_id}, queue size: {current_size}")
        
        # ğŸ”¥ æ›´æ–°æ€»ç»Ÿè®¡ï¼ˆæ— é”ï¼Œå®¹å¿çŸ­æš‚ä¸ä¸€è‡´ï¼‰
        self.stats['total_requests'] += 1
        
        logger.info(f"ğŸ”’ [FINE-LOCK] Submitted {stage.value} request {request_id} for user {user_id}, language {language_id}, GPU {gpu_id}")
        return request_id
    
    def _processing_loop(self, gpu_id: int):
        """
        Main processing loop for a specific GPU
        Implements the scheduling policy: PREFILL queue has priority over DECODE queue
        """
        language_id = self.gpu_language_map[gpu_id]
        logger.info(f"Starting processing loop for GPU {gpu_id} (language: {language_id})")
        
        # ğŸ”¥ æ·»åŠ ï¼šé˜Ÿåˆ—çŠ¶æ€æŠ¥å‘Šè®¡æ—¶å™¨
        last_queue_report_time = time.time()
        queue_report_interval = 30  # æ¯30ç§’æŠ¥å‘Šé˜Ÿåˆ—çŠ¶æ€
        
        while self.is_running:
            try:
                # Get batch of requests following the priority rule
                batch = self._get_request_batch(gpu_id)
                
                if not batch:
                    time.sleep(0.001)  
                    # ğŸ”¥ æ·»åŠ ï¼šåœ¨ç©ºé—²æ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦è¯Šæ–­å’ŒæŠ¥å‘Š
                    current_time = time.time()
                    
                    # ğŸ”¥ æ·»åŠ ï¼šå‘¨æœŸæ€§é˜Ÿåˆ—çŠ¶æ€æŠ¥å‘Š
                    if current_time - last_queue_report_time > queue_report_interval:
                        self._report_queue_performance(gpu_id)
                        last_queue_report_time = current_time
                    continue
                
                # ğŸ”¥ å…³é”®ï¼šprocess_batchä¸éœ€è¦é”ï¼Œå› ä¸ºæ¯ä¸ªGPUå•çº¿ç¨‹å¤„ç†
                self._process_batch(batch, gpu_id)
                
                # Clean up old sessions periodically
                if time.time() % 60 < 1:  # Every minute
                    self._cleanup_sessions()
                
                # ğŸ”¥ æ·»åŠ ï¼šå®šæœŸè¯Šæ–­æ£€æŸ¥
                current_time = time.time()
                
            except Exception as e:
                logger.error(f"Error in processing loop for GPU {gpu_id}: {e}")
                
                # ğŸ”¥ æ·»åŠ ï¼šå‘ç”Ÿé”™è¯¯æ—¶è‡ªåŠ¨æ‰“å°è¯Šæ–­ä¿¡æ¯
                print(f"ğŸš¨ [SCHEDULER-ERROR] GPU {gpu_id} å¤„ç†å¾ªç¯å‘ç”Ÿé”™è¯¯ï¼Œæ‰“å°è¯Šæ–­ä¿¡æ¯:")
                self.print_diagnosis()
                
                time.sleep(0.1)  # Brief pause on error
        
        logger.info(f"Processing loop stopped for GPU {gpu_id}")
    
    def _get_request_batch(self, gpu_id: int) -> List[InferenceRequest]:
        """
        Get a HOMOGENEOUS batch of requests (either all PREFILL or all DECODE)
        
        ğŸ”¥ å…³é”®æ”¹è¿›ï¼šä½¿ç”¨ç»†ç²’åº¦é”ï¼Œå‡å°‘é”ç«äº‰
        ğŸ”¥ Task 3: Added dynamic scheduling support
        
        Scheduling Policy:
        1. If PREFILL queue has requests: Create pure PREFILL batch (up to 32 requests)
        2. If PREFILL queue is empty: Create pure DECODE batch (up to 32 requests)
        3. NEVER mix PREFILL and DECODE in the same batch
        4. Dynamic scheduling: Dispatch based on wait time or batch size thresholds
        """
        batch = []
        current_time = time.time()
        
        # ğŸ”¥ Task 3: Check dynamic scheduling conditions
        should_dispatch = False
        trigger_reason = None
        
        if self.use_dynamic_schedule:
            should_dispatch, trigger_reason = self._check_dynamic_dispatch_conditions(gpu_id, current_time)
            if should_dispatch:
                print(f"ğŸš€ [DYNAMIC-SCHEDULE] GPU {gpu_id} dispatch triggered: {trigger_reason}")
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå…ˆæ£€æŸ¥prefillé˜Ÿåˆ—ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰

        prefill_queue = self.prefill_queues[gpu_id]
        if prefill_queue:
            with self.prefill_locks[gpu_id]:
                print(f"ğŸ”’ [FINE-LOCK] GPU {gpu_id} checking prefill queue... {len(prefill_queue)}")
                # æœ‰prefillè¯·æ±‚ï¼Œåˆ›å»ºprefill batch
                batch_exit_time = time.time()
                while len(batch) < self.max_batch_size and prefill_queue:
                    try:
                        request = prefill_queue.popleft()
                        # ğŸ”¥ è®°å½•å‡ºé˜Ÿæ—¶é—´å’Œç­‰å¾…æ—¶é—´
                        request.queue_exit_time = batch_exit_time
                        if hasattr(request, 'queue_enter_time'):
                            wait_time = batch_exit_time - request.queue_enter_time
                            request.queue_wait_time = wait_time
                            # æ›´æ–°ç»Ÿè®¡
                            stats = self.queue_stats[gpu_id]['prefill']
                            stats['total_wait_time'] += wait_time
                            if wait_time > stats['max_wait_time']:
                                stats['max_wait_time'] = wait_time
                        batch.append(request)
                        
                        # ğŸ”¥ æ›´æ–°é˜Ÿåˆ—å¤§å°ç»Ÿè®¡ï¼ˆå‡ºé˜Ÿæ—¶å‡1ï¼‰
                        self.stats['queue_sizes'][gpu_id]['prefill'] -= 1
                        self.queue_stats[gpu_id]['prefill']['current_queue_size'] = len(prefill_queue)
                    except IndexError:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºå¾ªç¯
                        print(f"âš ï¸ [SCHEDULER] Prefill queue empty during pop for GPU {gpu_id}")
                        break
                
                if batch:
                    assert all(req.stage.name == RequestStage.PREFILL.name for req in batch)
                    # ğŸ”¥ æ›´æ–°é˜Ÿåˆ—å¤§å°ç»Ÿè®¡
                    self.queue_stats[gpu_id]['prefill']['current_queue_size'] = len(prefill_queue)
                    logger.info(f"ğŸ”’ [FINE-LOCK] Created PREFILL batch of size {len(batch)} for GPU {gpu_id}")
                    return batch
        
        decode_queue = self.decode_queues[gpu_id]
        if decode_queue:
            with self.decode_locks[gpu_id]:
                print(f"ğŸ”’ [FINE-LOCK] GPU {gpu_id} checking decode queue... {len(decode_queue)}")
                # æœ‰decodeè¯·æ±‚ï¼Œåˆ›å»ºdecode batch
                batch_exit_time = time.time()
                while len(batch) < self.max_batch_size and decode_queue:
                    try:
                        request = decode_queue.popleft()
                        # ğŸ”¥ è®°å½•å‡ºé˜Ÿæ—¶é—´å’Œç­‰å¾…æ—¶é—´
                        request.queue_exit_time = batch_exit_time
                        if hasattr(request, 'queue_enter_time'):
                            wait_time = batch_exit_time - request.queue_enter_time
                            request.queue_wait_time = wait_time
                            # æ›´æ–°ç»Ÿè®¡
                            stats = self.queue_stats[gpu_id]['decode']
                            stats['total_wait_time'] += wait_time
                            if wait_time > stats['max_wait_time']:
                                stats['max_wait_time'] = wait_time
                        batch.append(request)
                        
                        # ğŸ”¥ æ›´æ–°é˜Ÿåˆ—å¤§å°ç»Ÿè®¡ï¼ˆå‡ºé˜Ÿæ—¶å‡1ï¼‰
                        self.stats['queue_sizes'][gpu_id]['decode'] -= 1
                        self.queue_stats[gpu_id]['decode']['current_queue_size'] = len(decode_queue)
                    except IndexError:
                        # é˜Ÿåˆ—ä¸ºç©ºï¼Œé€€å‡ºå¾ªç¯
                        print(f"âš ï¸ [SCHEDULER] Decode queue empty during pop for GPU {gpu_id}")
                        break
                
                if batch:
                    assert all(req.stage.name == RequestStage.DECODE.name for req in batch)
                    # ğŸ”¥ æ›´æ–°é˜Ÿåˆ—å¤§å°ç»Ÿè®¡
                    self.queue_stats[gpu_id]['decode']['current_queue_size'] = len(decode_queue)
                    logger.info(f"ğŸ”’ [FINE-LOCK] Created DECODE batch of size {len(batch)} for GPU {gpu_id}")
        
        print(f"ğŸ”’ [FINE-LOCK] GPU {gpu_id} no requests available, returning empty batch")
        return batch
    
    def _process_batch(self, batch: List[InferenceRequest], gpu_id: int):
        """
        Process a batch of requests using inference engine only (no simulation)
        """
        if not batch:
            return
        
        language_id = self.gpu_language_map[gpu_id]
        batch_stage = batch[0].stage.value if batch else "unknown"
        
        # ğŸ”¥ å¼€å§‹å¤„ç†æ—¶é—´è®°å½•
        process_start_time = time.time()
        print(f"ğŸ“Š [BATCH-TIMING] GPU {gpu_id} å¼€å§‹å¤„ç† {batch_stage} batch: {len(batch)} ä¸ªè¯·æ±‚")
        
        # è®°å½•æ¯ä¸ªè¯·æ±‚çš„å¤„ç†å¼€å§‹æ—¶é—´å’Œç­‰å¾…æ—¶é—´ç»Ÿè®¡
        for i, request in enumerate(batch):
            request.is_processing = True
            request.process_start_time = process_start_time
            
            # ğŸ”¥ æ‰“å°ç­‰å¾…æ—¶é—´ä¿¡æ¯
            if hasattr(request, 'queue_wait_time') and request.queue_wait_time is not None:
                wait_time_ms = request.queue_wait_time * 1000
                print(f"   - Request {i+1}: é˜Ÿåˆ—ç­‰å¾… {wait_time_ms:.1f}ms")
        
        logger.info(f"Processing batch of {len(batch)} requests on GPU {gpu_id} for language {language_id}")
        
        # ğŸ”¥ æ·»åŠ è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
        print(f"ğŸ” [SCHEDULER-DEBUG] Processing batch:")
        print(f"   - GPU {gpu_id}, Language: {language_id}")
        print(f"   - Batch size: {len(batch)}")
        print(f"   - Stage: {batch_stage}")
        print(f"   - Has inference_engine: {hasattr(self, 'inference_engine')}")
        if hasattr(self, 'inference_engine'):
            print(f"   - Inference_engine is None: {self.inference_engine is None}")
        
        try:

            if hasattr(self, 'inference_engine') and self.inference_engine:
                print(f"ğŸ” [SCHEDULER-DEBUG] æ¨ç†å¼•æ“å¯ç”¨ï¼Œå¼€å§‹å¤„ç†...")
                try:
                    # ğŸ” å¤„ç†å‰è®°å½•é¡µé¢æ± çŠ¶æ€
                    print(f"ğŸ“Š [SCHEDULER] GPU {gpu_id} å¼€å§‹å¤„ç† {len(batch)} ä¸ªè¯·æ±‚")
                    for i, req in enumerate(batch):
                        audio_len = req.speech_batch.shape[-1] if hasattr(req.speech_batch, 'shape') else len(req.speech_batch)
                        print(f"   - Request {i+1}: {audio_len} samples, stage={req.stage.value}")
                    
                    batch_inference_start = time.time()
                    print(f"ğŸ” [SCHEDULER-DEBUG] è°ƒç”¨æ¨ç†å¼•æ“ process_batch...")
                    results = self.inference_engine.process_batch(gpu_id, batch)
                    batch_inference_time = time.time() - batch_inference_start
                    
                    print(f"ğŸ” [SCHEDULER-DEBUG] æ¨ç†å¼•æ“è°ƒç”¨å®Œæˆï¼Œè€—æ—¶: {batch_inference_time*1000:.1f}ms")
                    print(f"ğŸ” [SCHEDULER-DEBUG] è¿”å›ç»“æœæ•°é‡: {len(results) if results else 0}")
                    
                    # ğŸ” å¤„ç†åè®°å½•ç»“æœ
                    print(f"ğŸ“Š [SCHEDULER] GPU {gpu_id} å®Œæˆå¤„ç† [{batch_stage}]: {len(batch)} ä¸ªè¯·æ±‚ â†’ {len(results)} ä¸ªç»“æœ, æ¨ç†è€—æ—¶: {batch_inference_time*1000:.1f}ms")
                    
                    # å¤„ç†æ¨ç†ç»“æœ
                    for i, request in enumerate(batch):
                        if i < len(results):
                            result = results[i]
                            success = result.get('success', False)
                            error = result.get('error', 'None')
                            print(f"   - Request {i+1} ç»“æœ: success={success}, error={error}")
                            if not success:
                                print(f"ğŸ” [SCHEDULER-DEBUG] Request {i+1} å¤±è´¥è¯¦æƒ…: {result}")
                            self._update_session_with_result(request, result)
                            logger.info(f"Request {request.request_id} completed with inference engine")
                        else:
                            # å¤„ç†ç¼ºå¤±çš„ç»“æœ
                            print(f"   - Request {i+1} ç¼ºå¤±ç»“æœ")
                            self._handle_failed_request(request, "Missing inference result")
                    
                except Exception as e:
                    logger.error(f"[ERROR] Inference engine failed for GPU {gpu_id}: {e}")
                    print(f"ğŸ” [SCHEDULER-DEBUG] æ¨ç†å¼•æ“å¼‚å¸¸è¯¦æƒ…:")
                    print(f"   - å¼‚å¸¸ç±»å‹: {type(e).__name__}")
                    print(f"   - å¼‚å¸¸æ¶ˆæ¯: {str(e)}")
                    import traceback
                    print(f"   - å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                
                    # å…¶ä»–é”™è¯¯ï¼šæ ‡è®°æ‰€æœ‰è¯·æ±‚å¤±è´¥
                    for request in batch:
                        self._handle_failed_request(request, f"Inference engine error: {str(e)}")
            else:
                # æ²¡æœ‰æ¨ç†å¼•æ“å¯ç”¨
                print(f"ğŸ” [SCHEDULER-DEBUG] æ¨ç†å¼•æ“ä¸å¯ç”¨!")
                logger.error(f"No inference engine available for GPU {gpu_id}")
                for request in batch:
                    self._handle_failed_request(request, "Inference engine not available")
                
        except Exception as e:
            logger.error(f"Batch processing failed on GPU {gpu_id}: {e}")
            # å¤„ç†æ‰€æœ‰è¯·æ±‚çš„é”™è¯¯
            for request in batch:
                self._handle_failed_request(request, f"Batch processing failed: {str(e)}")
        
        # ğŸ”¥ å¤„ç†å®Œæˆåçš„æ—¶é—´ç»Ÿè®¡
        process_end_time = time.time()
        total_process_time = process_end_time - process_start_time
        
        # æ›´æ–°é˜Ÿåˆ—ç»Ÿè®¡
        stage_stats = self.queue_stats[gpu_id][batch_stage]
        stage_stats['total_processed'] += len(batch)
        stage_stats['total_process_time'] += total_process_time
        stage_stats['last_process_time'] = total_process_time
        
        if total_process_time > stage_stats['max_process_time']:
            stage_stats['max_process_time'] = total_process_time
        
        # è®¡ç®—å¹³å‡å€¼
        if stage_stats['total_processed'] > 0:
            stage_stats['avg_process_time'] = stage_stats['total_process_time'] / stage_stats['total_processed']
            stage_stats['avg_wait_time'] = stage_stats['total_wait_time'] / stage_stats['total_processed']
            
            # è®¡ç®—ååé‡ (requests per second)
            if total_process_time > 0:
                stage_stats['throughput_per_sec'] = len(batch) / total_process_time
        
        # ğŸ”¥ æ‰“å°è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡
        print(f"ğŸ“Š [BATCH-TIMING] GPU {gpu_id} {batch_stage} batchå®Œæˆ:")
        print(f"   - æ‰¹å¤„ç†è€—æ—¶: {total_process_time*1000:.1f}ms")
        print(f"   - å¹³å‡æ¯è¯·æ±‚: {(total_process_time/len(batch))*1000:.1f}ms")
        print(f"   - ååé‡: {len(batch)/total_process_time:.1f} req/s")
        print(f"   - ç´¯è®¡å¤„ç†: {stage_stats['total_processed']} ä¸ª{batch_stage}è¯·æ±‚")
        print(f"   - å¹³å‡å¤„ç†æ—¶é—´: {stage_stats['avg_process_time']*1000:.1f}ms")
        print(f"   - å¹³å‡ç­‰å¾…æ—¶é—´: {stage_stats['avg_wait_time']*1000:.1f}ms")
        print(f"   - æœ€å¤§å¤„ç†æ—¶é—´: {stage_stats['max_process_time']*1000:.1f}ms")
        print(f"   - æœ€å¤§ç­‰å¾…æ—¶é—´: {stage_stats['max_wait_time']*1000:.1f}ms")
    
    
    def _update_session_with_result(self, request: InferenceRequest, result: Dict[str, Any]):
        """ä½¿ç”¨æ¨ç†ç»“æœæ›´æ–°ç”¨æˆ·ä¼šè¯ - ORCAé£æ ¼åˆ†æ­¥å¤„ç†"""
        try:
            # æ›´æ–°ç”¨æˆ·ä¼šè¯
            session = self.user_sessions[request.language_id][request.user_id]
            
            if result.get('success', False):
                # ğŸ”¥ ORCAé£æ ¼ï¼šæ ¹æ®å¤„ç†é˜¶æ®µæ›´æ–°çŠ¶æ€
                prefill_finished = result.get('prefill_finished', False)
                decode_finished = result.get('decode_finished', False)
                
                if prefill_finished and not hasattr(request, '_prefill_done'):
                    # Prefillé˜¶æ®µåˆšå®Œæˆ
                    print(f"ğŸ” [ORCA-SCHEDULER] Request {request.request_id} prefillå®Œæˆ")
                    request._prefill_done = True
                    
                    # å°†requestçŠ¶æ€åˆ‡æ¢åˆ°DECODE
                    request.stage = RequestStage.DECODE
                    
                    # Prefillé˜¶æ®µé€šå¸¸ä¸ç”Ÿæˆæœ€ç»ˆæ–‡æœ¬ï¼Œåªæ˜¯å‡†å¤‡beamçŠ¶æ€
                    generated_text = result.get('generated_text', '')
                    if generated_text:
                        print(f"ğŸ” [ORCA-SCHEDULER] Prefillç”Ÿæˆåˆå§‹æ–‡æœ¬: '{generated_text}'")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°sessionå’Œrequestçš„ç¼“å­˜çŠ¶æ€
                    if 'speech_cache' in result:
                        session.speech_cache = result['speech_cache']
                        request.speech_cache = result['speech_cache']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] æ›´æ–°speech_cacheå¼•ç”¨")
                    
                    if 'past_key_values' in result:
                        session.past_key_values = result['past_key_values']
                        request.past_key_values = result['past_key_values']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] æ›´æ–°past_key_valueså¼•ç”¨")
                        
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿å­˜beam_state
                    if hasattr(request, 'beam_state') and request.beam_state is not None:
                        session.beam_state = request.beam_state
                        print(f"ğŸ” [ORCA-CACHE] ä¿å­˜beam_stateåˆ°session")
                        
                    # ğŸ”¥ å…³é”®ï¼šå°†requesté‡æ–°æ”¾å›DECODEé˜Ÿåˆ—ç»§ç»­å¤„ç†
                    gpu_id = self.language_gpu_map[request.language_id]
                    with self.decode_locks[gpu_id]:
                        self.decode_queues[gpu_id].append(request)
                        self.stats['queue_sizes'][gpu_id]['decode'] += 1
                    print(f"ğŸ”„ [ORCA-SCHEDULER] Request {request.request_id} å·²æ”¾å›DECODEé˜Ÿåˆ— (cacheå·²æ›´æ–°)")
                
                elif request.stage.name == RequestStage.DECODE.name:
                    # Decodeé˜¶æ®µ - ç”Ÿæˆäº†æ–°çš„token
                    generated_text = result.get('generated_text', '')
                    generated_tokens = result.get('generated_tokens', [])
                    finished = result.get('finished', False)
                    
                    print(f"ğŸ” [ORCA-SCHEDULER] Decode step: '{generated_text}', finished={finished}")

                    is_chinese_translation = "Chinese" in request.language_id or "zh" in request.language_id.lower()
                    
                    if finished and generated_text:
                        session.target.append(generated_text)
                        
                        # ğŸ¯ è®°å½•è¾“å‡ºå»¶è¿Ÿï¼ˆç”¨äºstreamLAALè®¡ç®—ï¼‰
                        output_timestamp = time.time()
                        if session.evaluation_mode and session.delay_tracker:
                            session.record_output(generated_text, output_timestamp, is_final=True)
                            logger.info(f"ğŸ¯ [DELAY] Recorded output for session {session.session_id}: {len(generated_text)} chars")
                    
                    if is_chinese_translation:
                        new_full_text = ''.join(session.target)
                    else:
                        new_full_text = ' '.join(session.target)
                    result['full_translation'] = new_full_text

                    
                    # æ›´æ–°tokenåºåˆ—
                    if generated_tokens:
                        session.target_ids = generated_tokens.copy()  # å®Œå…¨æ›¿æ¢
                        print(f"ğŸ” [ORCA-SCHEDULER] æ›´æ–°tokenåºåˆ—: {len(session.target_ids)} tokens")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ›´æ–°sessionå’Œrequestçš„ç¼“å­˜çŠ¶æ€
                    if 'speech_cache' in result:
                        session.speech_cache = result['speech_cache']
                        request.speech_cache = result['speech_cache']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] Decodeé˜¶æ®µæ›´æ–°speech_cacheå¼•ç”¨")
                    
                    if 'past_key_values' in result:
                        session.past_key_values = result['past_key_values']
                        request.past_key_values = result['past_key_values']  # ğŸ”¥ åŒæ­¥æ›´æ–°request
                        print(f"ğŸ” [ORCA-CACHE] Decodeé˜¶æ®µæ›´æ–°past_key_valueså¼•ç”¨")
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¿å­˜beam_state
                    if hasattr(request, 'beam_state') and request.beam_state is not None:
                        session.beam_state = request.beam_state
                        print(f"ğŸ” [ORCA-CACHE] ä¿å­˜beam_stateåˆ°session")
                        
                    # ğŸ”¥ å…³é”®ï¼šå¦‚æœè¿˜æ²¡å®Œæˆï¼Œç»§ç»­æ”¾å›DECODEé˜Ÿåˆ—
                    if not finished and not decode_finished:
                        gpu_id = self.language_gpu_map[request.language_id]
                        with self.decode_locks[gpu_id]:
                            self.decode_queues[gpu_id].append(request)
                            self.stats['queue_sizes'][gpu_id]['decode'] += 1
                        print(f"ğŸ”„ [ORCA-SCHEDULER] Request {request.request_id} ç»§ç»­DECODEï¼Œå·²é‡æ–°å…¥é˜Ÿ (cacheå·²æ›´æ–°)")
                    else:
                        print(f"âœ… [ORCA-SCHEDULER] Request {request.request_id} ç¿»è¯‘å®Œæˆ")
                        # æ›´æ–° src_len åˆ°å½“å‰ session.source çš„é•¿åº¦
                        session.src_len = len(session.source)
                        print(f"ğŸ” [ORCA-SCHEDULER] Final src_len updated to {session.src_len}")
            
            session.last_activity = time.time()
            
            # ğŸ”¥ å…³é”®ï¼šåªåœ¨çœŸæ­£å®Œæˆæ—¶æ‰æ ‡è®°requestå®Œæˆå’Œè°ƒç”¨å›è°ƒ
            finished = result.get('finished', False) or result.get('decode_finished', False)
            
            if finished:
                # æ ‡è®°è¯·æ±‚å®Œæˆ
                request.result = result
                request.is_completed = True
                request.is_processing = False
                
                # è°ƒç”¨å›è°ƒå‡½æ•°
                if request.result_callback:
                    try:
                        request.result_callback(result)
                    except Exception as e:
                        logger.error(f"Error in result callback for request {request.request_id}: {e}")
                
                self.stats['completed_requests'] += 1
                print(f"ğŸ“¤ [ORCA-SCHEDULER] å‘é€æœ€ç»ˆç»“æœåˆ°å®¢æˆ·ç«¯: '{result.get('generated_text', '')}'")
            else:
                # ä¸­é—´æ­¥éª¤ï¼Œä¸è°ƒç”¨å›è°ƒï¼Œç»§ç»­å¤„ç†
                print(f"ğŸ”„ [ORCA-SCHEDULER] ä¸­é—´æ­¥éª¤å®Œæˆï¼Œç»§ç»­å¤„ç†...")
            
        except Exception as e:
            logger.error(f"Error updating session for request {request.request_id}: {e}")
            self._handle_failed_request(request, f"Session update failed: {str(e)}")
    
    def _handle_failed_request(self, request: InferenceRequest, error_msg: str):
        """å¤„ç†å¤±è´¥çš„è¯·æ±‚"""
        error_result = {
            'request_id': request.request_id,
            'success': False,
            'error': error_msg,
            'generated_text': '',
            'generated_tokens': [],
            'stage': request.stage.value
        }
        
        request.result = error_result
        request.is_completed = True
        request.is_processing = False
        
        if request.result_callback:
            try:
                request.result_callback(error_result)
            except Exception as e:
                logger.error(f"Error in error callback for request {request.request_id}: {e}")
        
        self.stats['failed_requests'] = self.stats.get('failed_requests', 0) + 1
    
    def set_inference_engine(self, inference_engine):
        """è®¾ç½®æ¨ç†å¼•æ“"""
        self.inference_engine = inference_engine
        logger.info("æ¨ç†å¼•æ“å·²è®¾ç½®åˆ°è°ƒåº¦å™¨")
    
    def diagnose_scheduler_status(self) -> Dict[str, Any]:
        """ğŸ” è¯Šæ–­è°ƒåº¦å™¨çŠ¶æ€ï¼Œå¸®åŠ©æ’æŸ¥é—®é¢˜"""
        diagnosis = {
            'timestamp': time.time(),
            'scheduler_running': self.is_running,
            'gpu_language_map': self.gpu_language_map,
            'inference_engine_status': {},
            'queue_status': {},
            'session_status': {},
            'thread_status': {}
        }
        
        # æ£€æŸ¥æ¨ç†å¼•æ“çŠ¶æ€
        diagnosis['inference_engine_status'] = {
            'has_inference_engine': hasattr(self, 'inference_engine'),
            'inference_engine_is_none': not hasattr(self, 'inference_engine') or self.inference_engine is None,
            'engine_type': type(self.inference_engine).__name__ if hasattr(self, 'inference_engine') and self.inference_engine else 'None'
        }
        
        if hasattr(self, 'inference_engine') and self.inference_engine:
            try:
                # æ£€æŸ¥å¤šGPUæ¨ç†å¼•æ“çš„çŠ¶æ€
                if hasattr(self.inference_engine, 'engines'):
                    engine_details = {}
                    for gpu_id in self.gpu_language_map.keys():
                        engine = self.inference_engine.get_engine(gpu_id)
                        engine_details[gpu_id] = {
                            'engine_exists': engine is not None,
                            'is_loaded': engine.is_loaded if engine else False,
                            'is_running': engine.is_running if engine else False,
                            'language': self.gpu_language_map[gpu_id]
                        }
                    diagnosis['inference_engine_status']['gpu_engines'] = engine_details
                else:
                    diagnosis['inference_engine_status']['single_engine'] = {
                        'is_loaded': getattr(self.inference_engine, 'is_loaded', False),
                        'is_running': getattr(self.inference_engine, 'is_running', False)
                    }
            except Exception as e:
                diagnosis['inference_engine_status']['engine_check_error'] = str(e)
        
        # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
        for gpu_id in self.gpu_language_map.keys():
            diagnosis['queue_status'][gpu_id] = {
                'language': self.gpu_language_map[gpu_id],
                'prefill_queue_size': len(self.prefill_queues[gpu_id]),
                'decode_queue_size': len(self.decode_queues[gpu_id]),
                'has_prefill_lock': gpu_id in self.prefill_locks,
                'has_decode_lock': gpu_id in self.decode_locks
            }
        
        # æ£€æŸ¥ä¼šè¯çŠ¶æ€
        with self.session_lock:
            diagnosis['session_status'] = {
                'total_languages': len(self.user_sessions),
                'languages': list(self.user_sessions.keys()),
                'total_sessions': sum(len(sessions) for sessions in self.user_sessions.values()),
                'sessions_by_language': {
                    lang: len(sessions) for lang, sessions in self.user_sessions.items()
                }
            }
        
        # æ£€æŸ¥å¤„ç†çº¿ç¨‹çŠ¶æ€
        diagnosis['thread_status'] = {
            'total_threads': len(self.processing_threads),
            'thread_details': {
                gpu_id: {
                    'thread_alive': thread.is_alive(),
                    'thread_name': thread.name
                } for gpu_id, thread in self.processing_threads.items()
            }
        }
        
        return diagnosis
    
    def print_diagnosis(self):
        """ğŸ” æ‰“å°è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯"""
        diagnosis = self.diagnose_scheduler_status()
        
        print("ğŸ” [SCHEDULER-DIAGNOSIS] è°ƒåº¦å™¨çŠ¶æ€è¯Šæ–­:")
        print(f"   ğŸ“Š è°ƒåº¦å™¨è¿è¡ŒçŠ¶æ€: {diagnosis['scheduler_running']}")
        print(f"   ğŸ–¥ï¸  GPUè¯­è¨€æ˜ å°„: {diagnosis['gpu_language_map']}")
        
        # æ¨ç†å¼•æ“çŠ¶æ€
        engine_status = diagnosis['inference_engine_status']
        print(f"   ğŸ¤– æ¨ç†å¼•æ“çŠ¶æ€:")
        print(f"      - æœ‰æ¨ç†å¼•æ“: {engine_status['has_inference_engine']}")
        print(f"      - å¼•æ“ä¸ºç©º: {engine_status['inference_engine_is_none']}")
        print(f"      - å¼•æ“ç±»å‹: {engine_status['engine_type']}")
        
        if 'gpu_engines' in engine_status:
            print(f"      - GPUå¼•æ“è¯¦æƒ…:")
            for gpu_id, details in engine_status['gpu_engines'].items():
                print(f"        GPU {gpu_id} ({details['language']}): å­˜åœ¨={details['engine_exists']}, å·²åŠ è½½={details['is_loaded']}, è¿è¡Œä¸­={details['is_running']}")
        
        # é˜Ÿåˆ—çŠ¶æ€
        print(f"   ğŸ“¥ é˜Ÿåˆ—çŠ¶æ€:")
        for gpu_id, queue_info in diagnosis['queue_status'].items():
            print(f"      GPU {gpu_id} ({queue_info['language']}): Prefill={queue_info['prefill_queue_size']}, Decode={queue_info['decode_queue_size']}")
        
        # ä¼šè¯çŠ¶æ€
        session_status = diagnosis['session_status']
        print(f"   ğŸ‘¥ ä¼šè¯çŠ¶æ€: {session_status['total_sessions']} ä¸ªä¼šè¯ï¼Œ{session_status['total_languages']} ç§è¯­è¨€")
        
        # çº¿ç¨‹çŠ¶æ€
        thread_status = diagnosis['thread_status']
        print(f"   ğŸ§µ å¤„ç†çº¿ç¨‹: {thread_status['total_threads']} ä¸ªçº¿ç¨‹")
        for gpu_id, thread_info in thread_status['thread_details'].items():
            print(f"      GPU {gpu_id}: çº¿ç¨‹å­˜æ´»={thread_info['thread_alive']}")
        
        return diagnosis
    
    def _cleanup_sessions(self):
        """Clean up old/inactive sessions"""
        current_time = time.time()
        sessions_to_remove = []
        
        with self.session_lock:
            for language_id, user_sessions in self.user_sessions.items():
                for user_id, session in list(user_sessions.items()):
                    if current_time - session.last_activity > self.session_timeout:
                        sessions_to_remove.append((language_id, user_id))
            
            # Remove expired sessions
            for language_id, user_id in sessions_to_remove:
                if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                    del self.user_sessions[language_id][user_id]
                    self.stats['active_sessions'] -= 1
                    logger.info(f"Cleaned up expired session for user {user_id}, language {language_id}")
    
    def get_session_info(self, user_id: str, language_id: str) -> Optional[Dict[str, Any]]:
        """Get information about a user session"""
        with self.session_lock:
            if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                session = self.user_sessions[language_id][user_id]
                return {
                    'session_id': session.session_id,
                    'user_id': session.user_id,
                    'language_id': session.language_id,
                    'source_length': len(session.source),
                    'source_finished': session.source_finished,
                    'target_segments': len(session.target),
                    'segment_idx': session.segment_idx,
                    'last_activity': session.last_activity,
                    'created_at': session.created_at
                }
        return None
    
    def reset_session(self, user_id: str, language_id: str) -> bool:
        """Reset a user session"""
        with self.session_lock:
            if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                session = self.user_sessions[language_id][user_id]
                session.reset()
                logger.info(f"Reset session for user {user_id}, language {language_id}")
                return True
        return False
    
    def get_queue_stats(self) -> Dict[str, Any]:
        """Get current queue and system statistics with detailed performance monitoring"""
        # ğŸ”¥ æ— é”è¯»å–åŸºç¡€ç»Ÿè®¡ï¼ˆå®¹å¿çŸ­æš‚ä¸ä¸€è‡´ï¼‰
        current_stats = self.stats.copy()
        current_stats['gpu_language_map'] = self.gpu_language_map.copy()
        current_stats['timestamp'] = time.time()
        
        # ğŸ”¥ æ·»åŠ ï¼šè¯¦ç»†çš„é˜Ÿåˆ—è¯Šæ–­ä¿¡æ¯ï¼ˆæ— é”è¯»å–ï¼‰
        current_stats['detailed_queue_info'] = {}
        for gpu_id in self.gpu_language_map.keys():
            prefill_count = len(self.prefill_queues[gpu_id])
            decode_count = len(self.decode_queues[gpu_id])
            
            current_stats['detailed_queue_info'][gpu_id] = {
                'language': self.gpu_language_map[gpu_id],
                'prefill_queue_size': prefill_count,
                'decode_queue_size': decode_count,
                'total_queue_size': prefill_count + decode_count
            }
        
        # ğŸ”¥ æ–°å¢ï¼šè¯¦ç»†çš„é˜Ÿåˆ—æ€§èƒ½ç»Ÿè®¡ï¼ˆæ— é”è¯»å–ï¼‰
        current_stats['queue_performance'] = {}
        for gpu_id in self.gpu_language_map.keys():
            gpu_stats = {}
            for stage in ['prefill', 'decode']:
                stage_stats = self.queue_stats[gpu_id][stage].copy()
                
                # è½¬æ¢æ—¶é—´å•ä½ä¸ºæ¯«ç§’ä»¥ä¾¿é˜…è¯»
                stage_stats['avg_wait_time_ms'] = stage_stats['avg_wait_time'] * 1000
                stage_stats['avg_process_time_ms'] = stage_stats['avg_process_time'] * 1000
                stage_stats['max_wait_time_ms'] = stage_stats['max_wait_time'] * 1000
                stage_stats['max_process_time_ms'] = stage_stats['max_process_time'] * 1000
                stage_stats['last_process_time_ms'] = stage_stats['last_process_time'] * 1000
                
                gpu_stats[stage] = stage_stats
            
            current_stats['queue_performance'][gpu_id] = gpu_stats
        
        # ğŸ”¥ æ·»åŠ ï¼šæ´»è·ƒsessionçš„æœ€åæ´»åŠ¨æ—¶é—´æ£€æŸ¥ï¼ˆåªåœ¨è®¿é—®sessionsæ—¶ç”¨é”ï¼‰
        current_time = time.time()
        inactive_sessions = []
        
        with self.session_lock:
            for language_id, user_sessions in self.user_sessions.items():
                for user_id, session in user_sessions.items():
                    inactive_time = current_time - session.last_activity
                    if inactive_time > 60:  # è¶…è¿‡1åˆ†é’Ÿä¸æ´»è·ƒ
                        inactive_sessions.append({
                            'session_id': session.session_id,
                            'user_id': user_id,
                            'language_id': language_id,
                            'inactive_seconds': inactive_time,
                            'source_length': len(session.source),
                            'target_segments': len(session.target)
                        })
        
        current_stats['inactive_sessions'] = inactive_sessions
        current_stats['inactive_session_count'] = len(inactive_sessions)
        
        return current_stats
    
    
    
    def get_supported_languages(self) -> List[str]:
        """Get list of supported language pairs"""
        return list(self.language_gpu_map.keys())

    
    def _cleanup_session_pages(self, session: UserSession):
        """æ¸…ç†å•ä¸ªsessionçš„KV cacheé¡µé¢"""
        try:
            if hasattr(self, 'inference_engine') and self.inference_engine:
                # å¯¼å…¥torch
                import torch
                
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„InferenceRequestç”¨äºæ¸…ç†
                cleanup_request = InferenceRequest(
                    request_id=f"cleanup_{session.session_id}",
                    user_id=session.user_id,
                    language_id=session.language_id,
                    session_id=session.session_id,
                    stage=RequestStage.PREFILL,
                    speech_batch=torch.empty(0),
                    input_ids=torch.empty(0, dtype=torch.long),
                    speech_cache=session.speech_cache,
                    past_key_values=session.past_key_values
                    # ç§»é™¤äº† is_final å‚æ•°ï¼Œå› ä¸º InferenceRequest æ²¡æœ‰è¿™ä¸ªå‚æ•°
                )
                
                # ä»æ¨ç†å¼•æ“è·å–å¯¹åº”GPUçš„å¼•æ“å®ä¾‹
                gpu_id = self.language_gpu_map.get(session.language_id)
                if gpu_id is not None:
                    engine = self.inference_engine.get_engine(gpu_id)
                    if engine:
                        logger.info(f"ğŸ§¹ è°ƒç”¨æ¨ç†å¼•æ“æ¸…ç†session {session.session_id} çš„KV cacheé¡µé¢")
                        engine._cleanup_session_kv_cache(cleanup_request)
                        
                        # æ›´æ–°sessionçš„å†…å­˜ä½¿ç”¨ç»Ÿè®¡ä¸º0
                        session.memory_usage = {
                            'speech_pages': 0,
                            'llm_prefill_pages': 0, 
                            'llm_decode_pages': 0,
                            'total_pages': 0,
                            'peak_pages': session.memory_usage.get('peak_pages', 0),
                            'allocation_count': session.memory_usage.get('allocation_count', 0)
                        }
                        
                        logger.info(f"âœ… Session {session.session_id} KV cacheé¡µé¢æ¸…ç†å®Œæˆ")
                    else:
                        logger.warning(f"âš ï¸ æ— æ³•è·å–GPU {gpu_id} çš„æ¨ç†å¼•æ“")
                else:
                    logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°è¯­è¨€ {session.language_id} å¯¹åº”çš„GPU")
            else:
                logger.warning("âš ï¸ æ¨ç†å¼•æ“ä¸å¯ç”¨ï¼Œè·³è¿‡KV cacheé¡µé¢æ¸…ç†")
                
        except Exception as e:
            logger.error(f"æ¸…ç†session {session.session_id} é¡µé¢æ—¶å‡ºé”™: {e}")
    
    def cleanup_session(self, user_id: str, language_id: str) -> bool:
        """æ‰‹åŠ¨æ¸…ç†æŒ‡å®šçš„ç”¨æˆ·ä¼šè¯"""
        try:
            with self.session_lock:
                if language_id in self.user_sessions and user_id in self.user_sessions[language_id]:
                    session = self.user_sessions[language_id][user_id]
                    
                    logger.info(f"ğŸ§¹ æ‰‹åŠ¨æ¸…ç†ä¼šè¯: {session.session_id}")
                    
                    # æ¸…ç†KV cacheé¡µé¢
                    self._cleanup_session_pages(session)
                    
                    # ä»ä¼šè¯å­—å…¸ä¸­ç§»é™¤
                    del self.user_sessions[language_id][user_id]
                    self.stats['active_sessions'] -= 1
                    
                    logger.info(f"âœ… ä¼šè¯ {session.session_id} æ¸…ç†å®Œæˆ")
                    return True
                else:
                    logger.warning(f"âš ï¸ ä¼šè¯ä¸å­˜åœ¨: user_id={user_id}, language_id={language_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"æ‰‹åŠ¨æ¸…ç†ä¼šè¯æ—¶å‡ºé”™: {e}")
            return False
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ä¼šè¯çš„å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        memory_stats = {
            'total_sessions': 0,
            'total_pages_used': 0,
            'sessions_by_language': {},
            'top_memory_users': [],
            'memory_distribution': {
                'speech_pages': 0,
                'llm_prefill_pages': 0,
                'llm_decode_pages': 0
            }
        }
        
        all_sessions = []
        
        with self.session_lock:
            for language_id, user_sessions in self.user_sessions.items():
                language_stats = {
                    'session_count': len(user_sessions),
                    'total_pages': 0,
                    'sessions': []
                }
                
                for user_id, session in user_sessions.items():
                    session_summary = session.get_memory_summary()
                    all_sessions.append(session_summary)
                    language_stats['sessions'].append(session_summary)
                    
                    pages = session_summary['memory_usage']['total_pages']
                    language_stats['total_pages'] += pages
                    memory_stats['total_pages_used'] += pages
                    
                    # ç´¯è®¡å„ç±»å‹é¡µé¢ä½¿ç”¨
                    memory_stats['memory_distribution']['speech_pages'] += session_summary['memory_usage'].get('speech_pages', 0)
                    memory_stats['memory_distribution']['llm_prefill_pages'] += session_summary['memory_usage'].get('llm_prefill_pages', 0)
                    memory_stats['memory_distribution']['llm_decode_pages'] += session_summary['memory_usage'].get('llm_decode_pages', 0)
                
                memory_stats['sessions_by_language'][language_id] = language_stats
                memory_stats['total_sessions'] += language_stats['session_count']
        
        # æ‰¾å‡ºå†…å­˜ä½¿ç”¨æœ€å¤šçš„ä¼šè¯
        memory_stats['top_memory_users'] = sorted(
            all_sessions, 
            key=lambda x: x['memory_usage']['total_pages'], 
            reverse=True
        )[:10]  # å‰10ä¸ª 
        
        return memory_stats
    
    def _report_queue_performance(self, gpu_id: int):
        """å‘¨æœŸæ€§æŠ¥å‘Šé˜Ÿåˆ—æ€§èƒ½ç»Ÿè®¡"""
        try:
            # ğŸ”¥ æ— é”è¯»å–é˜Ÿåˆ—å¤§å°
            prefill_count = len(self.prefill_queues[gpu_id])
            decode_count = len(self.decode_queues[gpu_id])
            
            # åªåœ¨æœ‰æ´»åŠ¨æ—¶æŠ¥å‘Š
            if prefill_count == 0 and decode_count == 0:
                prefill_stats = self.queue_stats[gpu_id]['prefill']
                decode_stats = self.queue_stats[gpu_id]['decode']
                
                # å¦‚æœæ²¡æœ‰å¤„ç†è¿‡ä»»ä½•è¯·æ±‚ï¼Œä¸æŠ¥å‘Š
                if prefill_stats['total_processed'] == 0 and decode_stats['total_processed'] == 0:
                    return
            
            language = self.gpu_language_map[gpu_id]
            print(f"ğŸ“Š [QUEUE-REPORT] GPU {gpu_id} ({language}) é˜Ÿåˆ—çŠ¶æ€:")
            print(f"   ğŸ“¥ å½“å‰é˜Ÿåˆ—: Prefill={prefill_count}, Decode={decode_count}")
            
            # Prefillæ€§èƒ½ç»Ÿè®¡
            prefill_stats = self.queue_stats[gpu_id]['prefill']
            if prefill_stats['total_processed'] > 0:
                print(f"   ğŸ”¥ Prefillæ€§èƒ½:")
                print(f"      - ç´¯è®¡å¤„ç†: {prefill_stats['total_processed']} ä¸ªè¯·æ±‚")
                print(f"      - å¹³å‡ç­‰å¾…: {prefill_stats['avg_wait_time']*1000:.1f}ms")
                print(f"      - å¹³å‡å¤„ç†: {prefill_stats['avg_process_time']*1000:.1f}ms")
                print(f"      - æœ€å¤§ç­‰å¾…: {prefill_stats['max_wait_time']*1000:.1f}ms")
                print(f"      - æœ€å¤§å¤„ç†: {prefill_stats['max_process_time']*1000:.1f}ms")
                print(f"      - å½“å‰é˜Ÿåˆ—å¤§å°: {prefill_stats['current_queue_size']}")
                print(f"      - å³°å€¼é˜Ÿåˆ—å¤§å°: {prefill_stats['max_queue_size']}")
                if prefill_stats.get('throughput_per_sec', 0) > 0:
                    print(f"      - ååé‡: {prefill_stats['throughput_per_sec']:.1f} req/s")
            
            # Decodeæ€§èƒ½ç»Ÿè®¡
            decode_stats = self.queue_stats[gpu_id]['decode']
            if decode_stats['total_processed'] > 0:
                print(f"   ğŸ”„ Decodeæ€§èƒ½:")
                print(f"      - ç´¯è®¡å¤„ç†: {decode_stats['total_processed']} ä¸ªè¯·æ±‚")
                print(f"      - å¹³å‡ç­‰å¾…: {decode_stats['avg_wait_time']*1000:.1f}ms")
                print(f"      - å¹³å‡å¤„ç†: {decode_stats['avg_process_time']*1000:.1f}ms")
                print(f"      - æœ€å¤§ç­‰å¾…: {decode_stats['max_wait_time']*1000:.1f}ms")
                print(f"      - æœ€å¤§å¤„ç†: {decode_stats['max_process_time']*1000:.1f}ms")
                print(f"      - å½“å‰é˜Ÿåˆ—å¤§å°: {decode_stats['current_queue_size']}")
                print(f"      - å³°å€¼é˜Ÿåˆ—å¤§å°: {decode_stats['max_queue_size']}")
                if decode_stats.get('throughput_per_sec', 0) > 0:
                    print(f"      - ååé‡: {decode_stats['throughput_per_sec']:.1f} req/s")
            
            # é˜Ÿåˆ—ç§¯å‹è­¦å‘Š
            if prefill_count > 5:
                print(f"âš ï¸  [QUEUE-WARNING] Prefillé˜Ÿåˆ—ç§¯å‹: {prefill_count} ä¸ªè¯·æ±‚")
            if decode_count > 10:
                print(f"âš ï¸  [QUEUE-WARNING] Decodeé˜Ÿåˆ—ç§¯å‹: {decode_count} ä¸ªè¯·æ±‚")
                
        except Exception as e:
            logger.error(f"Error reporting queue performance for GPU {gpu_id}: {e}") 
    
    # ğŸ”¥ Task 3: Dynamic Scheduling Methods
    
    def _check_dynamic_dispatch_conditions(self, gpu_id: int, current_time: float) -> Tuple[bool, Optional[str]]:
        """
        æ£€æŸ¥åŠ¨æ€è°ƒåº¦çš„è§¦å‘æ¡ä»¶
        
        Returns:
            (should_dispatch, trigger_reason)
        """
        if not self.use_dynamic_schedule:
            return False, None
        
        # æ¡ä»¶1ï¼šæ£€æŸ¥æ‰¹å¤„ç†å¤§å°
        prefill_size = len(self.prefill_queues[gpu_id])
        decode_size = len(self.decode_queues[gpu_id])
        total_size = prefill_size + decode_size
        
        if total_size >= self.max_batch_size:
            return True, "batch_size_threshold"
        
        # æ¡ä»¶2ï¼šæ£€æŸ¥ç­‰å¾…æ—¶é—´
        oldest_wait_time = self._get_oldest_request_wait_time(gpu_id, current_time)
        if oldest_wait_time is not None and oldest_wait_time > self.dynamic_wait_threshold:
            return True, f"wait_time_threshold_{oldest_wait_time:.3f}s"
        
        # æ¡ä»¶3ï¼šæ£€æŸ¥æœ€å°æ‰¹å¤„ç†å¤§å°
        if total_size >= self.dynamic_batch_min_size:
            # å¦‚æœæœ‰æœ€å°æ•°é‡çš„è¯·æ±‚ï¼Œå¯ä»¥è€ƒè™‘dispatch
            return True, f"min_batch_size_{total_size}"
        
        return False, None
    
    def _get_oldest_request_wait_time(self, gpu_id: int, current_time: float) -> Optional[float]:
        """è·å–é˜Ÿåˆ—ä¸­æœ€è€è¯·æ±‚çš„ç­‰å¾…æ—¶é—´"""
        oldest_time = None
        
        # æ£€æŸ¥prefillé˜Ÿåˆ—
        prefill_queue = self.prefill_queues[gpu_id]
        if prefill_queue:
            first_request = prefill_queue[0]
            if hasattr(first_request, 'queue_enter_time') and first_request.queue_enter_time:
                wait_time = current_time - first_request.queue_enter_time
                oldest_time = wait_time
        
        # æ£€æŸ¥decodeé˜Ÿåˆ—
        decode_queue = self.decode_queues[gpu_id]
        if decode_queue:
            first_request = decode_queue[0]
            if hasattr(first_request, 'queue_enter_time') and first_request.queue_enter_time:
                wait_time = current_time - first_request.queue_enter_time
                if oldest_time is None or wait_time > oldest_time:
                    oldest_time = wait_time
        
        return oldest_time
    
    def _log_dynamic_dispatch(self, gpu_id: int, trigger_reason: str, prefill_size: int, decode_size: int):
        """è®°å½•åŠ¨æ€è°ƒåº¦è§¦å‘äº‹ä»¶"""
        timestamp = time.time()
        
        # æ›´æ–°ç»Ÿè®¡
        if "batch_size" in trigger_reason:
            self.stats['dynamic_triggers']['batch_size_triggers'] += 1
        elif "wait_time" in trigger_reason:
            self.stats['dynamic_triggers']['timeout_triggers'] += 1
        
        # è®°å½•è¯¦ç»†æ—¥å¿—
        logger.info(f"ğŸš€ [DYNAMIC-SCHEDULE] GPU {gpu_id} dispatch triggered: {trigger_reason}")
        logger.info(f"   - Timestamp: {timestamp}")
        logger.info(f"   - Prefill queue size: {prefill_size}")
        logger.info(f"   - Decode queue size: {decode_size}")
        logger.info(f"   - Total batch size: {prefill_size + decode_size}")
        logger.info(f"   - Batch size triggers: {self.stats['dynamic_triggers']['batch_size_triggers']}")
        logger.info(f"   - Timeout triggers: {self.stats['dynamic_triggers']['timeout_triggers']}")
        logger.info(f"   - Total dispatches: {self.stats['dynamic_triggers']['total_dispatches']}")
        
        # å¯é€‰ï¼šå†™å…¥ä¸“é—¨çš„è°ƒåº¦æ—¥å¿—æ–‡ä»¶
        try:
            log_entry = {
                "timestamp": timestamp,
                "gpu_id": gpu_id,
                "trigger_reason": trigger_reason,
                "prefill_queue_size": prefill_size,
                "decode_queue_size": decode_size,
                "total_batch_size": prefill_size + decode_size,
                "cumulative_stats": self.stats['dynamic_triggers'].copy()
            }
            
            # å†™å…¥è°ƒåº¦æ—¥å¿—æ–‡ä»¶ (å¯é€‰)
            schedule_log_path = f"dynamic_schedule_gpu_{gpu_id}.log"
            with open(schedule_log_path, 'a') as f:
                f.write(json.dumps(log_entry) + '\n')
                
        except Exception as e:
            logger.warning(f"Failed to write dynamic schedule log: {e}")
    
    def get_dynamic_schedule_stats(self) -> Dict[str, Any]:
        """è·å–åŠ¨æ€è°ƒåº¦ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "enabled": self.use_dynamic_schedule,
            "configuration": {
                "wait_threshold_ms": self.dynamic_wait_threshold * 1000,
                "min_batch_size": self.dynamic_batch_min_size,
                "max_batch_size": self.max_batch_size
            },
            "triggers": self.stats['dynamic_triggers'].copy(),
            "per_gpu_status": {}
        }
        
        # æ·»åŠ æ¯ä¸ªGPUçš„å½“å‰çŠ¶æ€
        current_time = time.time()
        for gpu_id in self.gpu_language_map.keys():
            prefill_size = len(self.prefill_queues[gpu_id])
            decode_size = len(self.decode_queues[gpu_id])
            oldest_wait = self._get_oldest_request_wait_time(gpu_id, current_time)
            
            should_dispatch, trigger_reason = self._check_dynamic_dispatch_conditions(gpu_id, current_time)
            
            stats["per_gpu_status"][gpu_id] = {
                "language": self.gpu_language_map[gpu_id],
                "prefill_queue_size": prefill_size,
                "decode_queue_size": decode_size,
                "total_queue_size": prefill_size + decode_size,
                "oldest_wait_time_ms": oldest_wait * 1000 if oldest_wait else None,
                "should_dispatch": should_dispatch,
                "trigger_reason": trigger_reason
            }
        
        return stats