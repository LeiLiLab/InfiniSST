#!/usr/bin/env python3
"""
Ray-based Serving System for InfiniSST
æ”¯æŒåŠ¨æ€batchè°ƒåº¦çš„åˆ†å¸ƒå¼å®æ—¶ç¿»è¯‘ç³»ç»Ÿ
"""

import ray
import asyncio
import time
import uuid
import logging
import numpy as np
import torch
from typing import Dict, List, Optional, Any, Callable, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
from concurrent.futures import ThreadPoolExecutor
import threading
from collections import defaultdict, deque

# Import existing components
from scheduler import RequestStage, UserSession, InferenceRequest
from inference_engine import InferenceEngine, EngineConfig

logger = logging.getLogger(__name__)

# ===== Ray Configuration =====

@dataclass
class RayServingConfig:
    """RayæœåŠ¡ç³»ç»Ÿé…ç½®"""
    # Rayé›†ç¾¤é…ç½®
    ray_address: Optional[str] = None  # Rayé›†ç¾¤åœ°å€ï¼ŒNoneè¡¨ç¤ºæœ¬åœ°æ¨¡å¼
    num_cpus: Optional[int] = None
    num_gpus: Optional[int] = None
    
    # GPUé…ç½®
    gpu_language_map: Dict[int, str] = field(default_factory=dict)
    
    # æ‰¹å¤„ç†é…ç½®
    max_batch_size: int = 32
    batch_timeout_ms: float = 100.0  # æ‰¹å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    min_batch_size: int = 1
    
    # åŠ¨æ€è°ƒåº¦é…ç½®
    enable_dynamic_batching: bool = True
    load_balance_strategy: str = "least_loaded"  # least_loaded, round_robin, gpu_memory
    
    # ä¼šè¯ç®¡ç†
    session_timeout: int = 3600  # ä¼šè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    cleanup_interval: int = 60  # æ¸…ç†é—´éš”ï¼ˆç§’ï¼‰
    
    # æ€§èƒ½é…ç½®
    max_concurrent_sessions: int = 1000
    prefetch_enabled: bool = True
    async_result_processing: bool = True

# ===== Ray Actors =====

@ray.remote(num_gpus=1)
class ModelActor:
    """
    å•GPUæ¨¡å‹Actorï¼Œè´Ÿè´£åœ¨ç‰¹å®šGPUä¸Šè¿›è¡Œæ¨ç†
    """
    
    def __init__(self, gpu_id: int, language_id: str, model_args: Any, config: EngineConfig):
        self.gpu_id = gpu_id
        self.language_id = language_id
        self.model_args = model_args
        self.config = config
        
        # æ¨ç†å¼•æ“
        self.inference_engine: Optional[InferenceEngine] = None
        
        # æ‰¹å¤„ç†çŠ¶æ€
        self.current_batch: List[InferenceRequest] = []
        self.batch_lock = threading.Lock()
        self.last_batch_time = time.time()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "total_batches": 0,
            "avg_batch_size": 0.0,
            "gpu_utilization": 0.0,
            "memory_usage": 0.0,
            "processing_time": 0.0
        }
        
        logger.info(f"ğŸš€ ModelActoråˆå§‹åŒ–: GPU {gpu_id}, Language: {language_id}")
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            # åˆ›å»ºæ¨ç†å¼•æ“
            self.inference_engine = InferenceEngine(
                model_args=self.model_args,
                config=self.config,
                gpu_id=self.gpu_id,
                language_id=self.language_id
            )
            
            # åŠ è½½æ¨¡å‹
            success = self.inference_engine.load_model()
            if success:
                self.inference_engine.start()
                logger.info(f"âœ… ModelActor GPU {self.gpu_id} æ¨¡å‹åŠ è½½æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ ModelActor GPU {self.gpu_id} æ¨¡å‹åŠ è½½å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ModelActor GPU {self.gpu_id} åˆå§‹åŒ–å¼‚å¸¸: {e}")
            return False
    
    def process_batch(self, requests: List[InferenceRequest]) -> List[Dict[str, Any]]:
        """å¤„ç†æ‰¹å¤„ç†è¯·æ±‚"""
        if not self.inference_engine:
            error_results = []
            for req in requests:
                error_results.append({
                    "request_id": req.request_id,
                    "success": False,
                    "error": "Model not initialized"
                })
            return error_results
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨æ¨ç†å¼•æ“å¤„ç†æ‰¹å¤„ç†
            results = self.inference_engine.process_batch(requests)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["total_requests"] += len(requests)
            self.stats["total_batches"] += 1
            self.stats["avg_batch_size"] = self.stats["total_requests"] / self.stats["total_batches"]
            self.stats["processing_time"] = time.time() - start_time
            
            # æ›´æ–°GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if torch.cuda.is_available():
                memory_info = torch.cuda.memory_stats(self.gpu_id)
                allocated = memory_info.get('allocated_bytes.all.current', 0)
                reserved = memory_info.get('reserved_bytes.all.current', 0)
                self.stats["memory_usage"] = allocated / reserved if reserved > 0 else 0.0
            
            logger.debug(f"ğŸ“Š ModelActor GPU {self.gpu_id} å¤„ç†æ‰¹æ¬¡: {len(requests)} è¯·æ±‚, è€—æ—¶: {self.stats['processing_time']:.3f}s")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ModelActor GPU {self.gpu_id} æ‰¹å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯ç»“æœ
            error_results = []
            for req in requests:
                error_results.append({
                    "request_id": req.request_id,
                    "success": False,
                    "error": str(e)
                })
            return error_results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–Actorç»Ÿè®¡ä¿¡æ¯"""
        return {
            "gpu_id": self.gpu_id,
            "language_id": self.language_id,
            "stats": self.stats.copy(),
            "current_batch_size": len(self.current_batch),
            "engine_stats": self.inference_engine.get_stats() if self.inference_engine else {}
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.inference_engine:
            self.inference_engine.stop()
            logger.info(f"ğŸ§¹ ModelActor GPU {self.gpu_id} èµ„æºæ¸…ç†å®Œæˆ")

@ray.remote
class SessionActor:
    """
    ä¼šè¯Actorï¼Œç®¡ç†å•ä¸ªç”¨æˆ·ä¼šè¯çš„çŠ¶æ€å’Œç”Ÿå‘½å‘¨æœŸ
    """
    
    def __init__(self, session_id: str, user_id: str, language_id: str, config: RayServingConfig):
        self.session_id = session_id
        self.user_id = user_id
        self.language_id = language_id
        self.config = config
        
        # ä¼šè¯çŠ¶æ€
        self.session = UserSession(
            user_id=user_id,
            language_id=language_id,
            session_id=session_id
        )
        
        # è¯·æ±‚å†å²
        self.request_history: List[InferenceRequest] = []
        self.result_history: List[Dict[str, Any]] = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            "requests_processed": 0,
            "avg_latency": 0.0,
            "total_characters": 0,
            "session_start_time": time.time(),
            "last_activity": time.time()
        }
        
        logger.info(f"ğŸ“± SessionActoråˆ›å»º: {session_id} (ç”¨æˆ·: {user_id}, è¯­è¨€: {language_id})")
    
    def update_session_state(self, speech_data: np.ndarray, is_final: bool = False):
        """æ›´æ–°ä¼šè¯çŠ¶æ€"""
        self.session.source.extend(speech_data.tolist())
        if is_final:
            self.session.source_finished = True
        
        self.session.last_activity = time.time()
        self.performance_stats["last_activity"] = time.time()
        
        logger.debug(f"ğŸ“ Session {self.session_id} çŠ¶æ€æ›´æ–°: éŸ³é¢‘é•¿åº¦ {len(self.session.source)}")
    
    def add_translation_result(self, result: Dict[str, Any]):
        """æ·»åŠ ç¿»è¯‘ç»“æœ"""
        if result.get("success", False):
            translation = result.get("translation", "")
            if translation:
                self.session.target.append(translation)
                self.performance_stats["total_characters"] += len(translation)
        
        self.result_history.append(result)
        self.performance_stats["requests_processed"] += 1
        
        # è®¡ç®—å¹³å‡å»¶è¿Ÿ
        if "latency" in result:
            current_avg = self.performance_stats["avg_latency"]
            request_count = self.performance_stats["requests_processed"]
            new_latency = result["latency"]
            self.performance_stats["avg_latency"] = (current_avg * (request_count - 1) + new_latency) / request_count
    
    def get_session_info(self) -> Dict[str, Any]:
        """è·å–ä¼šè¯ä¿¡æ¯"""
        return {
            "session_id": self.session_id,
            "user_id": self.user_id,
            "language_id": self.language_id,
            "session_state": {
                "source_length": len(self.session.source),
                "target_segments": len(self.session.target),
                "source_finished": self.session.source_finished,
                "segment_idx": self.session.segment_idx
            },
            "performance": self.performance_stats.copy(),
            "session_age": time.time() - self.performance_stats["session_start_time"],
            "inactive_time": time.time() - self.performance_stats["last_activity"]
        }
    
    def reset_session(self):
        """é‡ç½®ä¼šè¯çŠ¶æ€"""
        self.session.reset()
        self.request_history.clear()
        self.result_history.clear()
        
        # é‡ç½®æ€§èƒ½ç»Ÿè®¡ï¼ˆä¿ç•™ä¼šè¯çº§åˆ«çš„ç»Ÿè®¡ï¼‰
        self.performance_stats.update({
            "requests_processed": 0,
            "avg_latency": 0.0,
            "total_characters": 0,
            "last_activity": time.time()
        })
        
        logger.info(f"ğŸ”„ Session {self.session_id} å·²é‡ç½®")
    
    def cleanup(self):
        """æ¸…ç†ä¼šè¯èµ„æº"""
        logger.info(f"ğŸ§¹ SessionActor {self.session_id} æ¸…ç†å®Œæˆ")

@ray.remote
class SchedulerActor:
    """
    å…¨å±€è°ƒåº¦å™¨Actorï¼Œè´Ÿè´£è´Ÿè½½å‡è¡¡ã€æ‰¹å¤„ç†ä¼˜åŒ–å’Œèµ„æºç®¡ç†
    """
    
    def __init__(self, config: RayServingConfig, model_actors: Dict[int, ray.ObjectRef]):
        self.config = config
        self.model_actors = model_actors
        
        # è¯·æ±‚é˜Ÿåˆ— - æŒ‰GPUåˆ†ç»„
        self.request_queues: Dict[int, deque] = defaultdict(deque)
        self.queue_locks: Dict[int, threading.Lock] = defaultdict(threading.Lock)
        
        # ä¼šè¯ç®¡ç†
        self.sessions: Dict[str, ray.ObjectRef] = {}  # session_id -> SessionActor
        self.session_gpu_map: Dict[str, int] = {}  # session_id -> gpu_id
        
        # è´Ÿè½½å‡è¡¡çŠ¶æ€
        self.gpu_load: Dict[int, float] = {gpu_id: 0.0 for gpu_id in self.model_actors.keys()}
        self.last_used_gpu: Dict[str, int] = {}  # language_id -> last_used_gpu_id
        
        # åŠ¨æ€æ‰¹å¤„ç†çŠ¶æ€
        self.batch_timers: Dict[int, Optional[asyncio.Task]] = {}
        self.processing_flags: Dict[int, bool] = defaultdict(bool)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.global_stats = {
            "total_requests": 0,
            "total_sessions": 0,
            "active_sessions": 0,
            "avg_queue_time": 0.0,
            "throughput": 0.0,
            "gpu_utilization": {}
        }
        
        # å¯åŠ¨åå°ä»»åŠ¡
        self.cleanup_task = None
        self.monitoring_task = None
        
        logger.info(f"ğŸ¯ SchedulerActoråˆå§‹åŒ–: {len(model_actors)} GPU, æ”¯æŒè¯­è¨€: {list(config.gpu_language_map.values())}")
    
    async def start_background_tasks(self):
        """å¯åŠ¨åå°ä»»åŠ¡"""
        self.cleanup_task = asyncio.create_task(self._cleanup_sessions_periodically())
        self.monitoring_task = asyncio.create_task(self._monitor_system_performance())
        
        # ä¸ºæ¯ä¸ªGPUå¯åŠ¨æ‰¹å¤„ç†ä»»åŠ¡
        for gpu_id in self.model_actors.keys():
            batch_task = asyncio.create_task(self._dynamic_batch_processor(gpu_id))
            self.batch_timers[gpu_id] = batch_task
        
        logger.info("ğŸš€ SchedulerActoråå°ä»»åŠ¡å·²å¯åŠ¨")
    
    async def create_session(self, user_id: str, language_id: str, session_id: Optional[str] = None) -> str:
        """åˆ›å»ºæ–°ä¼šè¯"""
        if session_id is None:
            session_id = f"{user_id}_{language_id}_{int(time.time()*1000)}"
        
        # é€‰æ‹©GPU
        gpu_id = self._select_gpu_for_language(language_id)
        if gpu_id is None:
            raise ValueError(f"ä¸æ”¯æŒçš„è¯­è¨€: {language_id}")
        
        # åˆ›å»ºSessionActor
        session_actor = SessionActor.remote(session_id, user_id, language_id, self.config)
        
        # æ³¨å†Œä¼šè¯
        self.sessions[session_id] = session_actor
        self.session_gpu_map[session_id] = gpu_id
        
        # æ›´æ–°ç»Ÿè®¡
        self.global_stats["total_sessions"] += 1
        self.global_stats["active_sessions"] = len(self.sessions)
        
        logger.info(f"âœ… ä¼šè¯åˆ›å»ºæˆåŠŸ: {session_id} -> GPU {gpu_id}")
        return session_id
    
    async def submit_request(self, 
                           session_id: str,
                           speech_data: np.ndarray,
                           stage: RequestStage = RequestStage.PREFILL,
                           is_final: bool = False,
                           max_new_tokens: int = 20,
                           result_callback: Optional[Callable] = None) -> str:
        """æäº¤æ¨ç†è¯·æ±‚"""
        
        # æ£€æŸ¥ä¼šè¯æ˜¯å¦å­˜åœ¨
        if session_id not in self.sessions:
            raise ValueError(f"ä¼šè¯ä¸å­˜åœ¨: {session_id}")
        
        session_actor = self.sessions[session_id]
        gpu_id = self.session_gpu_map[session_id]
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        await session_actor.update_session_state.remote(speech_data, is_final)
        
        # åˆ›å»ºè¯·æ±‚
        request_id = f"{session_id}_{int(time.time()*1000)}_{uuid.uuid4().hex[:8]}"
        
        # è·å–ä¼šè¯ä¿¡æ¯ç”¨äºåˆ›å»ºè¯·æ±‚
        session_info = await session_actor.get_session_info.remote()
        
        # åˆ›å»ºInferenceRequest
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„InferenceRequestæ„é€ å‡½æ•°è°ƒæ•´å‚æ•°
        request = InferenceRequest(
            request_id=request_id,
            user_id=session_info["user_id"],
            language_id=session_info["language_id"],
            session_id=session_id,
            stage=stage,
            speech_batch=torch.tensor(speech_data, dtype=torch.float32),
            input_ids=torch.tensor([], dtype=torch.long),  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µè®¾ç½®
            max_new_tokens=max_new_tokens,
            result_callback=result_callback
        )
        
        # æ·»åŠ åˆ°å¯¹åº”GPUçš„é˜Ÿåˆ—
        with self.queue_locks[gpu_id]:
            self.request_queues[gpu_id].append(request)
        
        # æ›´æ–°ç»Ÿè®¡
        self.global_stats["total_requests"] += 1
        
        logger.debug(f"ğŸ“¤ è¯·æ±‚æäº¤: {request_id} -> GPU {gpu_id} é˜Ÿåˆ—")
        return request_id
    
    def _select_gpu_for_language(self, language_id: str) -> Optional[int]:
        """ä¸ºè¯­è¨€é€‰æ‹©æœ€ä¼˜GPU"""
        
        # æ‰¾åˆ°æ”¯æŒè¯¥è¯­è¨€çš„GPU
        available_gpus = []
        for gpu_id, supported_lang in self.config.gpu_language_map.items():
            if supported_lang == language_id:
                available_gpus.append(gpu_id)
        
        if not available_gpus:
            return None
        
        # æ ¹æ®è´Ÿè½½å‡è¡¡ç­–ç•¥é€‰æ‹©GPU
        if self.config.load_balance_strategy == "least_loaded":
            return min(available_gpus, key=lambda gpu: self.gpu_load[gpu])
        elif self.config.load_balance_strategy == "round_robin":
            last_gpu = self.last_used_gpu.get(language_id, -1)
            next_gpu_idx = (available_gpus.index(last_gpu) + 1) % len(available_gpus) if last_gpu in available_gpus else 0
            selected_gpu = available_gpus[next_gpu_idx]
            self.last_used_gpu[language_id] = selected_gpu
            return selected_gpu
        else:
            # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨GPU
            return available_gpus[0]
    
    async def _dynamic_batch_processor(self, gpu_id: int):
        """åŠ¨æ€æ‰¹å¤„ç†å¤„ç†å™¨"""
        logger.info(f"ğŸ”„ GPU {gpu_id} åŠ¨æ€æ‰¹å¤„ç†å™¨å¯åŠ¨")
        
        while True:
            try:
                current_time = time.time()
                batch_to_process = []
                
                # æ”¶é›†æ‰¹å¤„ç†è¯·æ±‚
                with self.queue_locks[gpu_id]:
                    queue = self.request_queues[gpu_id]
                    
                    # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¤„ç†æ‰¹æ¬¡
                    should_process = (
                        len(queue) >= self.config.max_batch_size or
                        (len(queue) >= self.config.min_batch_size and 
                         (len(queue) > 0 and current_time - queue[0].timestamp > self.config.batch_timeout_ms / 1000.0))
                    )
                    
                    if should_process and not self.processing_flags[gpu_id]:
                        # ä»é˜Ÿåˆ—ä¸­å–å‡ºè¯·æ±‚
                        batch_size = min(len(queue), self.config.max_batch_size)
                        for _ in range(batch_size):
                            if queue:
                                batch_to_process.append(queue.popleft())
                        
                        if batch_to_process:
                            self.processing_flags[gpu_id] = True
                
                # å¤„ç†æ‰¹æ¬¡
                if batch_to_process:
                    await self._process_batch_on_gpu(gpu_id, batch_to_process)
                    self.processing_flags[gpu_id] = False
                
                # çŸ­æš‚ç­‰å¾…é¿å…å¿™ç­‰å¾…
                await asyncio.sleep(0.01)
                
            except Exception as e:
                logger.error(f"âŒ GPU {gpu_id} æ‰¹å¤„ç†å™¨å¼‚å¸¸: {e}")
                self.processing_flags[gpu_id] = False
                await asyncio.sleep(1)  # é”™è¯¯æ¢å¤
    
    async def _process_batch_on_gpu(self, gpu_id: int, batch: List[InferenceRequest]):
        """åœ¨æŒ‡å®šGPUä¸Šå¤„ç†æ‰¹æ¬¡"""
        start_time = time.time()
        
        try:
            # è·å–ModelActorå¹¶å¤„ç†æ‰¹æ¬¡
            model_actor = self.model_actors[gpu_id]
            results = await model_actor.process_batch.remote(batch)
            
            # å¤„ç†ç»“æœ
            for i, result in enumerate(results):
                request = batch[i]
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€
                if request.session_id in self.sessions:
                    session_actor = self.sessions[request.session_id]
                    await session_actor.add_translation_result.remote(result)
                
                # è°ƒç”¨ç»“æœå›è°ƒ
                if request.result_callback:
                    try:
                        if asyncio.iscoroutinefunction(request.result_callback):
                            await request.result_callback(result)
                        else:
                            request.result_callback(result)
                    except Exception as e:
                        logger.error(f"ç»“æœå›è°ƒå¼‚å¸¸: {e}")
            
            # æ›´æ–°GPUè´Ÿè½½
            processing_time = time.time() - start_time
            self.gpu_load[gpu_id] = processing_time  # ç®€åŒ–çš„è´Ÿè½½æŒ‡æ ‡
            
            logger.debug(f"âœ… GPU {gpu_id} æ‰¹å¤„ç†å®Œæˆ: {len(batch)} è¯·æ±‚, è€—æ—¶: {processing_time:.3f}s")
            
        except Exception as e:
            logger.error(f"âŒ GPU {gpu_id} æ‰¹å¤„ç†å¤±è´¥: {e}")
            
            # å¤„ç†å¤±è´¥çš„è¯·æ±‚
            for request in batch:
                error_result = {
                    "request_id": request.request_id,
                    "success": False,
                    "error": str(e),
                    "latency": time.time() - request.timestamp
                }
                
                if request.result_callback:
                    try:
                        if asyncio.iscoroutinefunction(request.result_callback):
                            await request.result_callback(error_result)
                        else:
                            request.result_callback(error_result)
                    except Exception as callback_error:
                        logger.error(f"é”™è¯¯å›è°ƒå¼‚å¸¸: {callback_error}")
    
    async def _cleanup_sessions_periodically(self):
        """å®šæœŸæ¸…ç†è¿‡æœŸä¼šè¯"""
        while True:
            try:
                current_time = time.time()
                sessions_to_remove = []
                
                # æ£€æŸ¥æ‰€æœ‰ä¼šè¯
                for session_id, session_actor in self.sessions.items():
                    session_info = await session_actor.get_session_info.remote()
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    if session_info["inactive_time"] > self.config.session_timeout:
                        sessions_to_remove.append(session_id)
                
                # æ¸…ç†è¿‡æœŸä¼šè¯
                for session_id in sessions_to_remove:
                    await self._cleanup_session(session_id)
                
                # æ›´æ–°æ´»è·ƒä¼šè¯ç»Ÿè®¡
                self.global_stats["active_sessions"] = len(self.sessions)
                
                if sessions_to_remove:
                    logger.info(f"ğŸ§¹ æ¸…ç†äº† {len(sessions_to_remove)} ä¸ªè¿‡æœŸä¼šè¯")
                
            except Exception as e:
                logger.error(f"ä¼šè¯æ¸…ç†å¼‚å¸¸: {e}")
            
            await asyncio.sleep(self.config.cleanup_interval)
    
    async def _monitor_system_performance(self):
        """ç›‘æ§ç³»ç»Ÿæ€§èƒ½"""
        while True:
            try:
                # æ”¶é›†GPUç»Ÿè®¡ä¿¡æ¯
                gpu_stats = {}
                for gpu_id, model_actor in self.model_actors.items():
                    stats = await model_actor.get_stats.remote()
                    gpu_stats[gpu_id] = stats
                
                self.global_stats["gpu_utilization"] = gpu_stats
                
                # è®¡ç®—ååé‡
                # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„é€»è¾‘æ¥è®¡ç®—çœŸå®ååé‡
                
                logger.debug(f"ğŸ“Š ç³»ç»Ÿæ€§èƒ½ç›‘æ§: æ´»è·ƒä¼šè¯ {self.global_stats['active_sessions']}, GPUåˆ©ç”¨ç‡: {gpu_stats}")
                
            except Exception as e:
                logger.error(f"æ€§èƒ½ç›‘æ§å¼‚å¸¸: {e}")
            
            await asyncio.sleep(30)  # æ¯30ç§’ç›‘æ§ä¸€æ¬¡
    
    async def _cleanup_session(self, session_id: str):
        """æ¸…ç†å•ä¸ªä¼šè¯"""
        try:
            if session_id in self.sessions:
                session_actor = self.sessions[session_id]
                await session_actor.cleanup.remote()
                
                # ä»æ˜ å°„ä¸­ç§»é™¤
                del self.sessions[session_id]
                if session_id in self.session_gpu_map:
                    del self.session_gpu_map[session_id]
                
                logger.info(f"ğŸ§¹ ä¼šè¯æ¸…ç†å®Œæˆ: {session_id}")
                
        except Exception as e:
            logger.error(f"ä¼šè¯æ¸…ç†å¤±è´¥ {session_id}: {e}")
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "global_stats": self.global_stats.copy(),
            "queue_stats": {
                gpu_id: len(queue) for gpu_id, queue in self.request_queues.items()
            },
            "gpu_load": self.gpu_load.copy(),
            "active_sessions": len(self.sessions),
            "config": {
                "max_batch_size": self.config.max_batch_size,
                "batch_timeout_ms": self.config.batch_timeout_ms,
                "enable_dynamic_batching": self.config.enable_dynamic_batching
            }
        }
    
    async def cleanup(self):
        """æ¸…ç†è°ƒåº¦å™¨èµ„æº"""
        # åœæ­¢åå°ä»»åŠ¡
        if self.cleanup_task:
            self.cleanup_task.cancel()
        if self.monitoring_task:
            self.monitoring_task.cancel()
        
        for gpu_id, batch_task in self.batch_timers.items():
            if batch_task:
                batch_task.cancel()
        
        # æ¸…ç†æ‰€æœ‰ä¼šè¯
        for session_id in list(self.sessions.keys()):
            await self._cleanup_session(session_id)
        
        logger.info("ğŸ§¹ SchedulerActoræ¸…ç†å®Œæˆ")

# ===== Ray Serving System =====

class RayServingSystem:
    """
    åŸºäºRayçš„åˆ†å¸ƒå¼æœåŠ¡ç³»ç»Ÿä¸»ç±»
    """
    
    def __init__(self, config: RayServingConfig, model_args_map: Dict[int, Any]):
        self.config = config
        self.model_args_map = model_args_map
        
        # Ray Actors
        self.model_actors: Dict[int, ray.ObjectRef] = {}
        self.scheduler_actor: Optional[ray.ObjectRef] = None
        
        # ç³»ç»ŸçŠ¶æ€
        self.is_initialized = False
        self.is_running = False
        
        logger.info(f"ğŸŒŸ RayServingSystemåˆå§‹åŒ–: {len(config.gpu_language_map)} GPU")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–Rayé›†ç¾¤å’ŒActors"""
        try:
            # åˆå§‹åŒ–Ray
            if not ray.is_initialized():
                if self.config.ray_address:
                    ray.init(address=self.config.ray_address)
                    logger.info(f"ğŸ”— è¿æ¥åˆ°Rayé›†ç¾¤: {self.config.ray_address}")
                else:
                    ray.init(
                        num_cpus=self.config.num_cpus,
                        num_gpus=self.config.num_gpus
                    )
                    logger.info("ğŸ  å¯åŠ¨æœ¬åœ°Rayé›†ç¾¤")
            
            # åˆ›å»ºModelActors
            for gpu_id, language_id in self.config.gpu_language_map.items():
                model_args = self.model_args_map.get(gpu_id, {})
                engine_config = EngineConfig()  # ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è‡ªå®šä¹‰
                
                model_actor = ModelActor.remote(gpu_id, language_id, model_args, engine_config)
                
                # åˆå§‹åŒ–æ¨¡å‹
                init_success = await model_actor.initialize.remote()
                if not init_success:
                    logger.error(f"âŒ ModelActor GPU {gpu_id} åˆå§‹åŒ–å¤±è´¥")
                    return False
                
                self.model_actors[gpu_id] = model_actor
                logger.info(f"âœ… ModelActor GPU {gpu_id} ({language_id}) åˆ›å»ºæˆåŠŸ")
            
            # åˆ›å»ºSchedulerActor
            self.scheduler_actor = SchedulerActor.remote(self.config, self.model_actors)
            await self.scheduler_actor.start_background_tasks.remote()
            
            self.is_initialized = True
            logger.info("ğŸ‰ RayServingSystemåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ RayServingSystemåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def start(self):
        """å¯åŠ¨æœåŠ¡ç³»ç»Ÿ"""
        if not self.is_initialized:
            logger.error("ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize()")
            return False
        
        self.is_running = True
        logger.info("ğŸš€ RayServingSystemå·²å¯åŠ¨")
        return True
    
    async def stop(self):
        """åœæ­¢æœåŠ¡ç³»ç»Ÿ"""
        self.is_running = False
        
        # æ¸…ç†Scheduler
        if self.scheduler_actor:
            await self.scheduler_actor.cleanup.remote()
        
        # æ¸…ç†ModelActors
        for gpu_id, model_actor in self.model_actors.items():
            await model_actor.cleanup.remote()
        
        logger.info("â¹ï¸ RayServingSystemå·²åœæ­¢")
    
    async def create_session(self, user_id: str, language_id: str, session_id: Optional[str] = None) -> str:
        """åˆ›å»ºæ–°çš„ç¿»è¯‘ä¼šè¯"""
        if not self.is_running or not self.scheduler_actor:
            raise RuntimeError("ç³»ç»Ÿæœªè¿è¡Œ")
        
        return await self.scheduler_actor.create_session.remote(user_id, language_id, session_id)
    
    async def submit_translation_request(self,
                                       session_id: str,
                                       speech_data: np.ndarray,
                                       stage: RequestStage = RequestStage.PREFILL,
                                       is_final: bool = False,
                                       max_new_tokens: int = 20,
                                       result_callback: Optional[Callable] = None) -> str:
        """æäº¤ç¿»è¯‘è¯·æ±‚"""
        if not self.is_running or not self.scheduler_actor:
            raise RuntimeError("ç³»ç»Ÿæœªè¿è¡Œ")
        
        return await self.scheduler_actor.submit_request.remote(
            session_id=session_id,
            speech_data=speech_data,
            stage=stage,
            is_final=is_final,
            max_new_tokens=max_new_tokens,
            result_callback=result_callback
        )
    
    async def get_system_stats(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not self.scheduler_actor:
            return {"error": "è°ƒåº¦å™¨æœªåˆå§‹åŒ–"}
        
        return await self.scheduler_actor.get_system_stats.remote()
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        if ray.is_initialized():
            ray.shutdown()

# ===== Factory Functions =====

def create_ray_serving_config(
    gpu_ids: List[int],
    language_pairs: List[str],
    max_batch_size: int = 32,
    batch_timeout_ms: float = 100.0,
    **kwargs
) -> RayServingConfig:
    """åˆ›å»ºRayæœåŠ¡é…ç½®çš„å·¥å‚å‡½æ•°"""
    
    # åˆ›å»ºGPUè¯­è¨€æ˜ å°„
    gpu_language_map = {}
    for i, gpu_id in enumerate(gpu_ids):
        if i < len(language_pairs):
            gpu_language_map[gpu_id] = language_pairs[i]
        else:
            # å¦‚æœGPUå¤šäºè¯­è¨€å¯¹ï¼Œå¾ªç¯åˆ†é…
            gpu_language_map[gpu_id] = language_pairs[i % len(language_pairs)]
    
    return RayServingConfig(
        gpu_language_map=gpu_language_map,
        max_batch_size=max_batch_size,
        batch_timeout_ms=batch_timeout_ms,
        **kwargs
    )

async def create_ray_serving_system(
    gpu_ids: List[int],
    language_pairs: List[str],
    model_args_factory: Callable[[int, str], Any],
    **config_kwargs
) -> RayServingSystem:
    """åˆ›å»ºå¹¶åˆå§‹åŒ–RayæœåŠ¡ç³»ç»Ÿçš„å·¥å‚å‡½æ•°"""
    
    # åˆ›å»ºé…ç½®
    config = create_ray_serving_config(gpu_ids, language_pairs, **config_kwargs)
    
    # åˆ›å»ºæ¨¡å‹å‚æ•°æ˜ å°„
    model_args_map = {}
    for gpu_id, language_id in config.gpu_language_map.items():
        model_args_map[gpu_id] = model_args_factory(gpu_id, language_id)
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    system = RayServingSystem(config, model_args_map)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    init_success = await system.initialize()
    if not init_success:
        raise RuntimeError("RayæœåŠ¡ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
    
    return system

# ===== Example Usage =====

if __name__ == "__main__":
    import asyncio
    
    async def example_usage():
        """ç¤ºä¾‹ç”¨æ³•"""
        
        # å®šä¹‰æ¨¡å‹å‚æ•°å·¥å‚å‡½æ•°
        def model_args_factory(gpu_id: int, language_id: str):
            # æ ¹æ®GPUå’Œè¯­è¨€è¿”å›ç›¸åº”çš„æ¨¡å‹å‚æ•°
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ¨¡å‹å‚æ•°ç»“æ„æ¥å®ç°
            return {
                "model_path": f"/path/to/model/{language_id}",
                "gpu_id": gpu_id,
                "language": language_id
            }
        
        # åˆ›å»ºå¹¶å¯åŠ¨ç³»ç»Ÿ
        system = await create_ray_serving_system(
            gpu_ids=[0, 1],
            language_pairs=["English -> Chinese", "English -> Italian"],
            model_args_factory=model_args_factory,
            max_batch_size=16,
            batch_timeout_ms=100.0
        )
        
        await system.start()
        
        try:
            # åˆ›å»ºä¼šè¯
            session_id = await system.create_session("user123", "English -> Chinese")
            print(f"åˆ›å»ºä¼šè¯: {session_id}")
            
            # æ¨¡æ‹ŸéŸ³é¢‘æ•°æ®
            audio_data = np.random.randn(16000).astype(np.float32)  # 1ç§’éŸ³é¢‘
            
            # ç»“æœå›è°ƒå‡½æ•°
            def result_callback(result):
                print(f"ç¿»è¯‘ç»“æœ: {result}")
            
            # æäº¤ç¿»è¯‘è¯·æ±‚
            request_id = await system.submit_translation_request(
                session_id=session_id,
                speech_data=audio_data,
                result_callback=result_callback
            )
            print(f"æäº¤è¯·æ±‚: {request_id}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å¤„ç†
            await asyncio.sleep(5)
            
            # è·å–ç³»ç»Ÿç»Ÿè®¡
            stats = await system.get_system_stats()
            print(f"ç³»ç»Ÿç»Ÿè®¡: {stats}")
            
        finally:
            await system.stop()
    
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(example_usage()) 