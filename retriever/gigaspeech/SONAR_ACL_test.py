import torch
from torch.utils.data import DataLoader, Dataset
import numpy as np
import json
from tqdm import tqdm
import argparse, os, sys
import faiss
from new_retrieve import Retriever
import soundfile as sf
from chunk_splitter import split_audio_from_path, create_chunked_samples

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sonar.inference_pipelines.speech import SpeechToEmbeddingModelPipeline
from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline

# å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„æ¨¡å‹ç±»
from SONAR_train import ContrastiveSpeechTextModel

# === New imports for offline asset building ===
import math
from pathlib import Path


def l2_normalize_numpy(x: np.ndarray) -> np.ndarray:
    """L2 normalize along last dim for numpy array."""
    norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / norms


def save_offline_assets(
    embeddings: np.ndarray,
    terms: list,
    out_dir: str,
    index_name: str = "glossary_emb.ivfpq.faiss",
    term2idx_name: str = "glossary_term2idx.json",
    terms_txt_name: str = "glossary_terms.txt",
):
    """Save term->idx map and ordered term list for verification."""
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    term2idx = {t: i for i, t in enumerate(terms)}
    with open(os.path.join(out_dir, term2idx_name), "w") as f:
        json.dump(term2idx, f)
    with open(os.path.join(out_dir, terms_txt_name), "w") as f:
        for t in terms:
            f.write(t + "\n")
    print(f"[ASSET] Saved term2idx -> {os.path.join(out_dir, term2idx_name)}")
    print(f"[ASSET] Saved terms.txt -> {os.path.join(out_dir, terms_txt_name)}")


def build_ivfpq_index(
    xb: np.ndarray,
    use_ip: bool = True,
    nlist: int = 4096,
    pq_m: int = 64,
    pq_bits: int = 8,
    train_size: int = 1000000,
    nprobe: int = 16,
):
    """Build an IVF-PQ index in memory and return it.
    - xb should be L2-normalized if use_ip=True (cosine via inner product).
    """
    d = xb.shape[1]
    metric = faiss.METRIC_INNER_PRODUCT if use_ip else faiss.METRIC_L2

    # Coarse quantizer
    quantizer = faiss.IndexFlatIP(d) if metric == faiss.METRIC_INNER_PRODUCT else faiss.IndexFlatL2(d)

    # IVF-PQ
    index = faiss.IndexIVFPQ(quantizer, d, nlist, pq_m, pq_bits, metric)

    # Train
    train_size = min(train_size, xb.shape[0])
    print(f"[FAISS] Training IVF-PQ on {train_size} vectors (nlist={nlist}, m={pq_m}, bits={pq_bits})")
    faiss_idx_train = xb[np.random.choice(xb.shape[0], train_size, replace=False)]
    index.train(faiss_idx_train)

    # Add
    index.nprobe = nprobe
    print(f"[FAISS] Adding {xb.shape[0]} vectors to IVF-PQ (nprobe={nprobe})")
    index.add(xb)
    print(f"[FAISS] IVF-PQ ntotal={index.ntotal}")

    return index


def build_sharded_ivfpq_indices(
    xb: np.ndarray,
    terms: list,
    out_dir: str,
    shard_size: int = 2_000_000,
    use_ip: bool = True,
    nlist: int = 4096,
    pq_m: int = 64,
    pq_bits: int = 8,
    train_size: int = 1_000_000,
    nprobe: int = 16,
):
    """Split embeddings into shards and build multiple IVF-PQ indexes: glossary_shard_XX.faiss.
    Returns list of written index paths.
    """
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    N = xb.shape[0]
    d = xb.shape[1]
    n_shards = math.ceil(N / shard_size)
    paths = []
    print(f"[FAISS] Sharding {N} vectors into {n_shards} shards (sizeâ‰ˆ{shard_size})")
    for s in range(n_shards):
        s0 = s * shard_size
        s1 = min((s + 1) * shard_size, N)
        x_shard = xb[s0:s1]
        print(f"[FAISS] Building shard {s+1}/{n_shards} with {x_shard.shape[0]} vectors")
        index = build_ivfpq_index(
            x_shard, use_ip=use_ip, nlist=nlist, pq_m=pq_m, pq_bits=pq_bits, train_size=min(train_size, x_shard.shape[0]), nprobe=nprobe
        )
        shard_path = os.path.join(out_dir, f"glossary_shard_{s:02d}.faiss")
        faiss.write_index(index, shard_path)
        print(f"[ASSET] Shard written -> {shard_path}")
        del index
    return paths


def load_glossary_terms(glossary_path):
    """åŠ è½½å®Œæ•´çš„æœ¯è¯­è¡¨"""
    print(f"[INFO] Loading glossary from {glossary_path}")
    sys.stdout.flush()
    with open(glossary_path, "r") as f:
        glossary = json.load(f)
    
    # æå–æ‰€æœ‰æœ¯è¯­ï¼Œå¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
    terms = []
    if isinstance(glossary, list):
        for item in glossary:
            if isinstance(item, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å– 'term' æˆ– 'text' å­—æ®µ
                term = item.get('term') or item.get('text') or item.get('word')
                if term:
                    terms.append(term.lower())
            elif isinstance(item, str):
                terms.append(item.lower())
    elif isinstance(glossary, dict):
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–æ‰€æœ‰å€¼
        for key, value in glossary.items():
            if isinstance(value, str):
                terms.append(value.lower())
            elif isinstance(value, dict) and 'term' in value:
                terms.append(value['term'].lower())
    
    # å»é‡å¹¶è¿‡æ»¤
    terms = list(set(term for term in terms if term and len(term.strip()) >= 2))
    print(f"[INFO] Loaded {len(terms)} unique terms from glossary")
    sys.stdout.flush()
    return terms


def is_audio_valid(audio_path, min_duration=0.01, max_duration=30.0):
    """æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
    try:
        if not os.path.exists(audio_path):
            return False, "File does not exist"
        
        data, sr = sf.read(audio_path)
        
        # æ£€æŸ¥åŸºæœ¬å±æ€§
        if len(data) == 0:
            return False, "Empty audio file"
        
        duration = len(data) / sr
        if duration < min_duration:
            return False, f"Too short ({duration:.3f}s < {min_duration}s)"
        
        if duration > max_duration:
            return False, f"Too long ({duration:.3f}s > {max_duration}s)"
        
        # æ£€æŸ¥æ˜¯å¦å…¨é™éŸ³
        if np.allclose(data, 0, atol=1e-6):
            return False, "All silence"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if np.isnan(data).any():
            return False, "Contains NaN values"
        
        if np.isinf(data).any():
            return False, "Contains Inf values"
        
        # æ£€æŸ¥åŠ¨æ€èŒƒå›´
        data_std = np.std(data)
        if data_std < 1e-6:
            return False, f"Very low dynamic range (std={data_std:.2e})"
        
        return True, "Valid"
        
    except Exception as e:
        return False, f"Failed to read: {str(e)}"


def validate_audio_batch(audio_paths, verbose=False):
    """æ‰¹é‡éªŒè¯éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›æœ‰æ•ˆçš„è·¯å¾„åˆ—è¡¨å’Œå¯¹åº”çš„åŸå§‹ç´¢å¼•"""
    valid_paths = []
    valid_indices = []
    invalid_count = 0
    
    for i, path in enumerate(audio_paths):
        is_valid, reason = is_audio_valid(path)
        if is_valid:
            valid_paths.append(path)
            valid_indices.append(i)
        else:
            invalid_count += 1
            if verbose or invalid_count <= 5:  # åªæ‰“å°å‰5ä¸ªæ— æ•ˆæ–‡ä»¶
                print(f"[WARN] Invalid audio {i}: {path} - {reason}")
    
    if invalid_count > 5:
        print(f"[WARN] ... and {invalid_count - 5} more invalid audio files")
    
    return valid_paths, valid_indices


class TermLevelDataset(Dataset):
    def __init__(self, path=None, split="test", train_ratio=0.99, test_path=None):
        if split == "test" and test_path is not None:
            # ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®é›†
            print(f"[INFO] Loading test samples from separate file: {test_path}")
            with open(test_path, "r") as f:
                all_samples = json.load(f)
            # å¯¹äºç‹¬ç«‹æµ‹è¯•é›†ï¼Œä¸éœ€è¦train_ratioåˆ†å‰²ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ ·æœ¬
            use_split_logic = False
        else:
            # ä½¿ç”¨åŸæœ‰çš„åˆ†å‰²é€»è¾‘
            if path is None:
                raise ValueError("path must be provided when not using separate test file")
            print(f"[INFO] Loading term-level chunk samples from {path}")
            with open(path, "r") as f:
                all_samples = json.load(f)
            use_split_logic = True
        
        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ï¼šå¿…é¡»æœ‰éŸ³é¢‘æ–‡ä»¶ã€chunkæ–‡æœ¬å’Œground truth terms
        valid_samples = []
        invalid_audio_count = 0
        
        for i, s in enumerate(all_samples):
            terms = s.get('term_chunk_audio_ground_truth_terms')
            if not (terms and isinstance(terms, list)):
                continue
            # è¿‡æ»¤æœ¯è¯­
            filtered_terms = [
                t for t in terms
                if isinstance(t, str)
                and len(t) >= 3
                and sum(c.isdigit() for c in t) <= len(t) // 2
            ]
            if not filtered_terms:
                continue

            # è¿‡æ»¤å‰åç¼€
            black_words = ['yeah','this ']
            black_suffixes = ['years']
            filtered_terms = [
                t for t in filtered_terms 
                if not any(t.lower().startswith(prefix.lower()) for prefix in black_words)
                and not any(t.lower().endswith(suffix.lower()) for suffix in black_suffixes)
            ]
            
            # æ›¿æ¢åŸåˆ—è¡¨ä¸ºè¿‡æ»¤åçš„æœ¯è¯­
            s = dict(s)  # é¿å…ç›´æ¥ä¿®æ”¹åŸå§‹æ•°æ®
            s['term_chunk_audio_ground_truth_terms'] = filtered_terms
            
            # æ£€æŸ¥åŸºæœ¬æ¡ä»¶
            if not (s.get('term_chunk_text', '').strip() and s.get('term_chunk_audio', '')):
                continue
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æœ‰æ•ˆæ€§
            audio_path = s.get("term_chunk_audio", "")
            is_valid, reason = is_audio_valid(audio_path)
            
            if is_valid:
                valid_samples.append(s)
            else:
                invalid_audio_count += 1
                # åªæ‰“å°å‰10ä¸ªæ— æ•ˆéŸ³é¢‘çš„è¯¦ç»†ä¿¡æ¯
                if invalid_audio_count <= 10:
                    print(f"[WARN] Skipping sample {i}: {audio_path} - {reason}")
        
        if invalid_audio_count > 10:
            print(f"[WARN] ... and {invalid_audio_count - 10} more samples with invalid audio")
            
        print(f"[INFO] Audio validation: {len(valid_samples)} valid, {invalid_audio_count} invalid")
        
        print(f"[INFO] Filtered {len(valid_samples)} valid term-level samples from {len(all_samples)} total samples")
        
        if use_split_logic:
            # æ•°æ®åˆ†å‰²ï¼š99%è®­ç»ƒï¼Œ1%æµ‹è¯•
            import random
            random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
            random.shuffle(valid_samples)
            
            split_idx = int(len(valid_samples) * train_ratio)
            
            if split == "train":
                self.samples = valid_samples[:split_idx]
                print(f"[INFO] Training split: {len(self.samples)} term-level samples")
            elif split == "test":
                self.samples = valid_samples[split_idx:]
                print(f"[INFO] Test split: {len(self.samples)} term-level samples")
            else:
                raise ValueError(f"Invalid split: {split}. Must be 'train' or 'test'")
        else:
            # ç‹¬ç«‹æµ‹è¯•é›†ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
            self.samples = valid_samples
            print(f"[INFO] Using separate test dataset: {len(self.samples)} term-level samples")
        
        print(f"[INFO] Loaded {len(self.samples)} term-level samples for {split} split")

    def __getitem__(self, idx):
        sample = self.samples[idx]
        audio_path = sample["term_chunk_audio"]  # ä½¿ç”¨term chunkéŸ³é¢‘
        chunk_text = sample["term_chunk_text"]   # ä½¿ç”¨term chunkæ–‡æœ¬
        ground_truth_terms = sample.get('term_chunk_audio_ground_truth_terms', [])
        
        return ground_truth_terms, audio_path, chunk_text, True

    def __len__(self):
        return len(self.samples)


def extract_all_used_terms(dataset):
    """æå–æ•°æ®é›†ä¸­æ‰€æœ‰ä½¿ç”¨çš„æœ¯è¯­"""
    used_terms = set()
    processed_samples = 0
    valid_samples = 0
    
    for i, sample in enumerate(dataset):
        if sample is None:
            continue
        processed_samples += 1
        ground_truth_terms, audio_path, chunk_text, has_target = sample
        
        if has_target and ground_truth_terms:
            valid_samples += 1
            for t in ground_truth_terms:
                if isinstance(t, str) and len(t.strip()) > 0:
                    used_terms.add(t.lower())
            
            # è°ƒè¯•å‰å‡ ä¸ªæ ·æœ¬
            if i < 5:
                print(f"[DEBUG] extract_all_used_terms - Sample {i}: ground_truth_terms={ground_truth_terms}, chunk_text='{chunk_text}'")
    
    print(f"[DEBUG] extract_all_used_terms - Processed {processed_samples} samples, {valid_samples} valid samples, {len(used_terms)} unique terms")
    return list(used_terms)


def encode_texts_in_batches(model, texts, batch_size=512, device="cuda", auto_batch_size=True, max_chunk_size=1000000):
    """åˆ†æ‰¹ç¼–ç æ–‡æœ¬ï¼Œæ”¯æŒåŠ¨æ€batch_sizeå’Œåˆ†æ®µå¤„ç†"""
    print(f"[INFO] Text encoding setup:")
    print(f"[INFO] - Model type: {type(model)}")
    print(f"[INFO] - Device count: {torch.cuda.device_count()}")
    print(f"[INFO] - Model device: {next(model.parameters()).device if hasattr(model, 'parameters') else 'N/A'}")
    print(f"[INFO] - Initial batch_size: {batch_size}")
    print(f"[INFO] - Total texts: {len(texts)}")
    sys.stdout.flush()
    
    # å¯¹äºå¤§é‡æ–‡æœ¬ï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†
    if len(texts) > max_chunk_size:
        print(f"[INFO] ğŸ“Š Large dataset detected ({len(texts)} texts)")
        print(f"[INFO] ğŸ”„ Using chunked processing with max_chunk_size={max_chunk_size}")
        sys.stdout.flush()
        
        all_results = []
        num_chunks = (len(texts) + max_chunk_size - 1) // max_chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * max_chunk_size
            end_idx = min(start_idx + max_chunk_size, len(texts))
            chunk_texts = texts[start_idx:end_idx]
            
            print(f"[INFO] ğŸ“¦ Processing chunk {chunk_idx + 1}/{num_chunks} ({len(chunk_texts)} texts)")
            sys.stdout.flush()
            
            # é€’å½’è°ƒç”¨å¤„ç†å•ä¸ªchunkï¼ˆä¸ä¼šå†åˆ†æ®µï¼‰
            chunk_result = encode_texts_in_batches(
                model, chunk_texts, batch_size, device, auto_batch_size, max_chunk_size=float('inf')
            )
            all_results.append(chunk_result)
            
            # åŠæ—¶æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print(f"[INFO] âœ… Chunk {chunk_idx + 1}/{num_chunks} completed, shape: {chunk_result.shape}")
            sys.stdout.flush()
        
        # åˆå¹¶æ‰€æœ‰chunkçš„ç»“æœ
        print(f"[INFO] ğŸ”— Merging {len(all_results)} chunks...")
        sys.stdout.flush()
        final_result = torch.cat(all_results, dim=0)
        
        # æ¸…ç†ä¸­é—´ç»“æœ
        del all_results
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"[INFO] âœ… Chunked processing completed: {final_result.shape}")
        sys.stdout.flush()
        return final_result
    
    # åŠ¨æ€è°ƒæ•´batch_sizeåˆ°æ˜¾å­˜æé™
    if auto_batch_size and torch.cuda.is_available():
        print(f"[INFO] Auto-tuning batch size for optimal GPU memory usage...")
        sys.stdout.flush()
        try_bs = batch_size
        test_texts = texts[:min(try_bs * 4, len(texts))]  # ç”¨å°æ ·æœ¬æµ‹è¯•
        
        while try_bs >= 32:  # æœ€å°batch_size
            try:
                print(f"[DEBUG] Testing batch_size: {try_bs}")
                sys.stdout.flush()
                torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜
                
                with torch.no_grad():
                    test_batch = test_texts[:try_bs] if len(test_texts) >= try_bs else test_texts
                    _ = model.encode_text(test_batch)
                    torch.cuda.empty_cache()
                
                # æˆåŠŸäº†ï¼Œå°è¯•æ›´å¤§çš„batch_sizeï¼ˆä½†ä¸è¶…è¿‡åˆç†ä¸Šé™ï¼‰
                max_reasonable_bs = min(1024, batch_size * 2)  # è®¾ç½®åˆç†ä¸Šé™
                if try_bs < max_reasonable_bs:
                    try_bs = int(try_bs * 1.3)  # æ›´ä¿å®ˆçš„å¢é•¿
                else:
                    break
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) or "out of memory" in str(e):
                    try_bs = max(32, try_bs // 2)
                    print(f"[WARNING] OOM, reducing batch_size to {try_bs}")
                    sys.stdout.flush()
                    torch.cuda.empty_cache()
                else:
                    print(f"[ERROR] Unexpected error during batch size testing: {e}")
                    break
        
        # å†é€€ä¸€æ­¥ï¼Œç¡®ä¿ç¨³å®šï¼ˆæ›´ä¿å®ˆï¼‰ï¼Œå¹¶è®¾ç½®ç»å¯¹ä¸Šé™
        batch_size = max(32, min(1024, int(try_bs * 0.6)))
        print(f"[INFO] âœ… Optimized batch_size: {batch_size} (capped at 1024)")
        sys.stdout.flush()
    
    # æ‰¹é‡ç¼–ç 
    all_embeddings = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    print(f"[INFO] Encoding {len(texts)} texts in {total_batches} batches of size {batch_size}")
    sys.stdout.flush()
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        if batch_num % max(1, total_batches // 10) == 0 or batch_num <= 3:
            print(f"[INFO] Processing text batch {batch_num}/{total_batches}")
            sys.stdout.flush()
        
        with torch.no_grad():
            try:
                emb = model.encode_text(batch).detach().cpu()
                all_embeddings.append(emb)
                
                # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ï¼Œé˜²æ­¢ç´¯ç§¯
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"[ERROR] Failed to encode text batch {batch_num}: {e}")
                sys.stdout.flush()
                # æ¸…ç†æ˜¾å­˜åé‡è¯•
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # å°è¯•æ›´å°çš„batch
                for j in range(0, len(batch), batch_size // 4):
                    mini_batch = batch[j:j + batch_size // 4]
                    try:
                        emb = model.encode_text(mini_batch).detach().cpu()
                        all_embeddings.append(emb)
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except Exception as e2:
                        print(f"[ERROR] Failed mini-batch encoding: {e2}")
                        sys.stdout.flush()
                        continue
    
    if not all_embeddings:
        raise RuntimeError("No texts were successfully encoded")
    
    result = torch.cat(all_embeddings, dim=0)
    print(f"[INFO] âœ… Text encoding completed: {result.shape}")
    sys.stdout.flush()
    return result


def encode_audios_in_batches(model, audio_paths, batch_size=1000, device="cuda", auto_batch_size=True):
    """åˆ†æ‰¹ç¼–ç éŸ³é¢‘ï¼Œæ”¯æŒåŠ¨æ€batch_sizeä¼˜åŒ–"""
    print(f"[INFO] Audio encoding setup:")
    print(f"[INFO] - Model type: {type(model)}")
    print(f"[INFO] - Initial batch_size: {batch_size}")
    sys.stdout.flush()
    
    # åŠ¨æ€è°ƒæ•´audio batch_sizeï¼ˆéŸ³é¢‘ç¼–ç æ›´æ¶ˆè€—æ˜¾å­˜ï¼‰
    if auto_batch_size and torch.cuda.is_available() and len(audio_paths) > batch_size:
        print(f"[INFO] Auto-tuning audio batch size...")
        sys.stdout.flush()
        try_bs = batch_size
        test_paths = audio_paths[:min(try_bs * 2, len(audio_paths))]  # ç”¨å°æ ·æœ¬æµ‹è¯•
        
        while try_bs >= 4:  # éŸ³é¢‘æœ€å°batch_size
            try:
                print(f"[DEBUG] Testing audio batch_size: {try_bs}")
                sys.stdout.flush()
                torch.cuda.empty_cache()
                
                with torch.no_grad():
                    test_batch = test_paths[:try_bs] if len(test_paths) >= try_bs else test_paths
                    _ = model.encode_audio(test_batch)
                    torch.cuda.empty_cache()
                
                # æˆåŠŸäº†ï¼Œå°è¯•ç¨å¤§ä¸€ç‚¹çš„batch_sizeï¼ˆéŸ³é¢‘æ›´ä¿å®ˆï¼‰
                max_reasonable_bs = min(128, batch_size * 2)  # éŸ³é¢‘ä¸Šé™128
                if try_bs < max_reasonable_bs:
                    try_bs = int(try_bs * 1.2)  # æ›´ä¿å®ˆçš„å¢é•¿
                else:
                    break
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) or "out of memory" in str(e):
                    try_bs = max(4, try_bs // 2)
                    print(f"[WARNING] Audio OOM, reducing batch_size to {try_bs}")
                    sys.stdout.flush()
                    torch.cuda.empty_cache()
                else:
                    print(f"[ERROR] Unexpected error during audio batch size testing: {e}")
                    break
        
        batch_size = max(4, min(128, int(try_bs * 0.7)))  # ä¿å®ˆä¸€ç‚¹ï¼Œä¸Šé™128
        print(f"[INFO] âœ… Optimized audio batch_size: {batch_size} (capped at 128)")
        sys.stdout.flush()
    
    # æ‰¹é‡ç¼–ç 
    all_embeddings = []
    total_batches = (len(audio_paths) + batch_size - 1) // batch_size
    print(f"[INFO] Encoding {len(audio_paths)} audio files in {total_batches} batches of size {batch_size}")
    sys.stdout.flush()
    
    for i in range(0, len(audio_paths), batch_size):
        batch_paths = audio_paths[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        if batch_num % max(1, total_batches // 10) == 0 or batch_num <= 3:
            print(f"[INFO] Processing audio batch {batch_num}/{total_batches}")
            sys.stdout.flush()
        
        with torch.no_grad():
            try:
                emb = model.encode_audio(batch_paths).detach().cpu()
                all_embeddings.append(emb)
            except Exception as e:
                print(f"[ERROR] Failed to encode audio batch {batch_num}: {e}")
                sys.stdout.flush()
                print(f"[INFO] Trying single file processing for this batch...")
                sys.stdout.flush()
                # å¦‚æœbatchå¤±è´¥ï¼Œå°è¯•å•ä¸ªå¤„ç†
                for single_path in batch_paths:
                    try:
                        single_emb = model.encode_audio([single_path]).detach().cpu()
                        all_embeddings.append(single_emb)
                    except Exception as e2:
                        print(f"[ERROR] Failed to encode single audio {single_path}: {e2}")
                        sys.stdout.flush()
                        # è·³è¿‡è¿™ä¸ªéŸ³é¢‘æ–‡ä»¶
                        continue
    
    if not all_embeddings:
        raise RuntimeError("No audio files were successfully encoded")
    
    result = torch.cat(all_embeddings, dim=0)
    print(f"[INFO] âœ… Audio encoding completed: {result.shape}")
    sys.stdout.flush()
    return result


def evaluate_topk_recall(model, retriever, dataset, device, top_ks=(5, 10, 20), max_eval=1000, field="term", train_terms=None, show_missed_terms=True, glossary_emb_path=None, relaxed_chunk_eval=False):
    """è¯„ä¼°top-kå¬å›ç‡ï¼Œä½¿ç”¨sample-levelå¹³å‡ï¼ŒåŒæ—¶æ”¶é›†term-levelä¿¡æ¯ç”¨äºåˆ†æ"""
    model.eval()
    
    # ç”¨äºå­˜å‚¨sample-levelå¬å›ç‡
    recall_dict = {k: [] for k in top_ks}
    
    # ç”¨äºå­˜å‚¨æ‰€æœ‰GTæœ¯è¯­å’Œå¯¹åº”çš„æ£€ç´¢ç»“æœï¼ˆç”¨äºåˆ†ææœªå‘½ä¸­æœ¯è¯­ï¼‰
    all_gt_terms_with_retrieval = {k: [] for k in top_ks}  # æ¯ä¸ªå…ƒç´ æ˜¯ (gt_term, is_retrieved, sample_info)
    sample_info_for_debug = []  # ç”¨äºè°ƒè¯•è¾“å‡º

    # === æ„å»ºæˆ–åŠ è½½ç´¢å¼• ===
    if glossary_emb_path and os.path.exists(glossary_emb_path):
        # ç›´æ¥åŠ è½½é¢„æ„å»ºçš„ç´¢å¼•
        print(f'[INFO] Loading pre-built glossary index from {glossary_emb_path}')
        try:
            retriever.index = faiss.read_index(glossary_emb_path)
            print(f'[INFO] Successfully loaded index with {retriever.index.ntotal} vectors')
            
            # ä»ç´¢å¼•ä¸­è·å–termæ•°é‡ä¿¡æ¯ï¼Œç”¨äºç»Ÿè®¡
            index_size = retriever.index.ntotal
            print(f'[INFO] Pre-built index contains {index_size} terms')
            
            # å¦‚æœretriever.term_listä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªå ä½ç¬¦åˆ—è¡¨ç”¨äºè¯„ä¼°
            if not hasattr(retriever, 'term_list') or not retriever.term_list:
                retriever.term_list = [{'term': f'term_{i}'} for i in range(index_size)]
                print(f'[INFO] Created placeholder term list for evaluation')
        except Exception as e:
            print(f'[WARNING] Failed to load pre-built index: {e}, falling back to text encoding')
            glossary_emb_path = None
    
    if not glossary_emb_path or not os.path.exists(glossary_emb_path):
        # éœ€è¦é‡æ–°æ„å»ºç´¢å¼•
        text_terms = [term['term'] for term in retriever.term_list]
        print(f'[DEBUG] Building index with {len(text_terms)} terms')
        print(f'[DEBUG] First 10 terms: {text_terms[:10]}')
        print(f'[DEBUG] Last 10 terms: {text_terms[-10:]}')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤terms
        unique_terms = set(text_terms)
        print(f'[DEBUG] Unique terms: {len(unique_terms)} / {len(text_terms)}')
        if len(unique_terms) != len(text_terms):
            print(f'[WARNING] Found duplicate terms in retriever.term_list!')
        
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        text_emb = encode_texts_in_batches(raw_model, text_terms, device=device)
        
        # æ£€æŸ¥embeddingæ˜¯å¦éƒ½ç›¸åŒ
        if text_emb.shape[0] > 1:
            first_emb = text_emb[0:1]
            similarities = F.cosine_similarity(first_emb, text_emb, dim=1)
            identical_count = (similarities > 0.99).sum().item()
            print(f'[DEBUG] Embeddings identical to first: {identical_count} / {text_emb.shape[0]}')
            if identical_count > text_emb.shape[0] * 0.8:
                print(f'[ERROR] Most embeddings are identical! This will cause retrieval issues.')

        retriever.index.reset()
        retriever.index.add(text_emb)
        print(f'[DEBUG] Index built with {retriever.index.ntotal} vectors')

    print(f"[INFO] Dataset size: {len(dataset)}")
    import random
    random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
    eval_indices = random.sample(range(len(dataset)), min(max_eval, len(dataset)))
    valid_samples = []
    valid_indices = []
    
    for i in eval_indices:
        sample = dataset[i]
        if sample is not None and sample[3] and sample[0]:  # has_target=True and has ground_truth_terms
            valid_samples.append(sample)
            valid_indices.append(i)

    print(f"[INFO] Selected {len(eval_indices)} samples randomly, {len(valid_samples)} are valid for evaluation")
    print(f"[INFO] Filtered out {len(eval_indices) - len(valid_samples)} samples (no ground truth terms or has_target=False)")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯chunked dataset
    is_chunked_dataset = hasattr(dataset, 'get_audio_tensor')
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨relaxed evaluationæ¨¡å¼
    use_relaxed_eval = is_chunked_dataset and hasattr(dataset, 'relaxed_eval') and dataset.relaxed_eval
    if use_relaxed_eval:
        print(f"[INFO] Using relaxed chunk evaluation: all sentence terms assigned to each chunk")
    elif relaxed_chunk_eval:
        print(f"[INFO] Relaxed evaluation requested but dataset doesn't support it")
        use_relaxed_eval = relaxed_chunk_eval  # å…¼å®¹æ‰‹åŠ¨è®¾ç½®
    
    if is_chunked_dataset:
        print(f"[DEBUG] Using chunked dataset with audio tensors...")
        # ä»chunked datasetè·å–éŸ³é¢‘tensors
        audio_tensors = []
        chunk_info_list = []
        
        for i in valid_indices:
            audio_tensor = dataset.get_audio_tensor(i)
            chunk_info = dataset.get_chunk_info(i)
            audio_tensors.append(audio_tensor)
            chunk_info_list.append(chunk_info)
        
        print(f"[DEBUG] Collected {len(audio_tensors)} audio tensors for evaluation...")
        
        # å¯¹äºchunkedæ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šå¤„ç†éŸ³é¢‘ç¼–ç 
        # ä½¿ç”¨éŸ³é¢‘tensorè€Œä¸æ˜¯æ–‡ä»¶è·¯å¾„è¿›è¡Œç¼–ç 
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        
        # åˆ†æ‰¹å¤„ç†éŸ³é¢‘tensors
        all_audio_embs = []
        batch_size = 100  # å¯¹äºtensorsï¼Œä½¿ç”¨è¾ƒå°çš„batch size
        
        for i in range(0, len(audio_tensors), batch_size):
            batch_tensors = audio_tensors[i:i + batch_size]
            
            # å°†tensorsä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œç¼–ç 
            import tempfile
            temp_paths = []
            
            try:
                for j, tensor in enumerate(batch_tensors):
                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                        # ç¡®ä¿tensoræ˜¯æ­£ç¡®çš„æ ¼å¼
                        if tensor.dim() > 1:
                            tensor = tensor.squeeze()
                        
                        # é‡é‡‡æ ·åˆ°16kHzè¿›è¡ŒSONARç¼–ç 
                        if hasattr(torch.nn.functional, 'interpolate'):
                            # ä»48kHzé‡é‡‡æ ·åˆ°16kHz
                            target_length = int(tensor.shape[-1] * 16000 / 48000)
                            tensor_16k = torch.nn.functional.interpolate(
                                tensor.unsqueeze(0).unsqueeze(0), 
                                size=target_length, 
                                mode='linear', 
                                align_corners=False
                            ).squeeze()
                        else:
                            # ç®€å•çš„ä¸‹é‡‡æ ·
                            step = 48000 // 16000
                            tensor_16k = tensor[::step]
                        
                        sf.write(tmp_file.name, tensor_16k.detach().numpy(), 16000)
                        temp_paths.append(tmp_file.name)
                
                # æ‰¹é‡ç¼–ç 
                batch_embs = raw_model.encode_audio(temp_paths).detach().cpu()
                all_audio_embs.append(batch_embs)
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                for temp_path in temp_paths:
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
        
        audio_embs = torch.cat(all_audio_embs, dim=0).detach().numpy()
        
        # ç»Ÿè®¡chunkä¿¡æ¯
        short_chunks = sum(1 for info in chunk_info_list if info['is_short_chunk'])
        print(f"[DEBUG] Audio encoding completed for chunked dataset")
        print(f"[DEBUG] - Total chunks: {len(chunk_info_list)}")
        print(f"[DEBUG] - Short chunks (< 1s): {short_chunks} ({short_chunks/len(chunk_info_list)*100:.1f}%)")
        
    else:
        print(f"[DEBUG] Using traditional dataset with audio paths...")
        # åŸæœ‰çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„å¤„ç†é€»è¾‘
        audio_paths = [sample[1] for sample in valid_samples]  # term_chunk_audio paths
        
        # éªŒè¯éŸ³é¢‘æ–‡ä»¶
        print(f"[DEBUG] Validating {len(audio_paths)} audio files for evaluation...")
        valid_audio_paths, valid_audio_indices = validate_audio_batch(audio_paths, verbose=False)
        
        if len(valid_audio_paths) != len(audio_paths):
            print(f"[WARN] Evaluation: Only {len(valid_audio_paths)}/{len(audio_paths)} audio files are valid")
            # è¿‡æ»¤æ‰æ— æ•ˆçš„æ ·æœ¬
            valid_samples = [valid_samples[i] for i in valid_audio_indices]
            valid_indices = [valid_indices[i] for i in valid_audio_indices]
            audio_paths = valid_audio_paths
        
        if len(audio_paths) == 0:
            print(f"[ERROR] No valid audio files for evaluation!")
            return {k: [] for k in top_ks}
        
        print(f"[DEBUG] Encoding {len(audio_paths)} valid audio files...")
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        audio_embs = encode_audios_in_batches(raw_model, audio_paths, batch_size=1000, device=device).detach().numpy()

    for j, (i, sample) in enumerate(zip(valid_indices, valid_samples)):
        ground_truth_terms, audio_path, chunk_text, has_target = sample
        audio_emb = audio_embs[j:j+1]  # shape: [1, 512]
        gt_terms = [t.lower() for t in ground_truth_terms]  # ä½¿ç”¨term_chunk_audio_ground_truth_terms

        # å¯¹æ¯ä¸ªtop_kè¿›è¡Œæ£€ç´¢
        retrieval_results = {}
        for top_k in top_ks:
            D, I = retriever.index.search(audio_emb, top_k)
            retrieved_terms = [retriever.term_list[idx][field].lower() for idx in I[0]]
            retrieval_results[top_k] = (D[0], I[0], retrieved_terms)
            
            # è®¡ç®—sample-levelå¬å›ç‡
            if use_relaxed_eval:
                # å®½æ¾æ¨¡å¼ï¼šåªè¦æœ‰ä»»ä½•ä¸€ä¸ªæœ¯è¯­å‘½ä¸­å°±ç®—æˆåŠŸ
                has_match = any(gt_term in retrieved_terms for gt_term in gt_terms)
                sample_recall = 1.0 if has_match else 0.0

            else:
                # ä¸¥æ ¼æ¨¡å¼ï¼šæŒ‰æ¯”ä¾‹è®¡ç®—
                matched = sum(gt_term in retrieved_terms for gt_term in gt_terms)
                sample_recall = matched / len(gt_terms) if gt_terms else 0.0

            recall_dict[top_k].append(sample_recall)
            
            # åŒæ—¶æ”¶é›†term-levelä¿¡æ¯ç”¨äºåˆ†ææœªå‘½ä¸­æœ¯è¯­
            for gt_term in gt_terms:
                is_retrieved = gt_term in retrieved_terms
                sample_info = {
                    'sample_idx': i,
                    'audio_path': audio_path,
                    'chunk_text': chunk_text,
                    'all_gt_terms': gt_terms,
                    'retrieved_terms': retrieved_terms  # æ·»åŠ æ£€ç´¢åˆ°çš„å€™é€‰æœ¯è¯­
                }
                all_gt_terms_with_retrieval[top_k].append((gt_term, is_retrieved, sample_info))

        # å­˜å‚¨æ ·æœ¬ä¿¡æ¯ç”¨äºè°ƒè¯•ï¼ˆåªå­˜å‚¨ç¬¬ä¸€ä¸ªtop_kçš„ç»“æœï¼‰
        if j < 3:  # åªä¿å­˜å‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            first_top_k = top_ks[0]
            D, I, retrieved_terms = retrieval_results[first_top_k]
            matched = sum(gt_term in retrieved_terms for gt_term in gt_terms)
            sample_info_for_debug.append({
                'sample_idx': i,
                'audio_path': audio_path,
                'chunk_text': chunk_text,
                'gt_terms': gt_terms,
                'audio_emb': audio_emb,
                'retrieved_indices': I,
                'retrieved_distances': D,
                'retrieved_terms': retrieved_terms,
                'matched_count': matched,
                'total_gt_count': len(gt_terms)
            })

    # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆå‰3ä¸ªæ ·æœ¬ï¼‰
    for debug_info in sample_info_for_debug:
        print(f"[DEBUG] Sample {debug_info['sample_idx']}:")
        print(f"[DEBUG] Audio path: {debug_info['audio_path']}")
        print(f"[DEBUG] Chunk text: {debug_info['chunk_text']}")
        print(f"[DEBUG] GT terms: {debug_info['gt_terms']}")
        print(f"[DEBUG] Audio embedding stats: mean={debug_info['audio_emb'].mean():.4f}, std={debug_info['audio_emb'].std():.4f}")
        print(f"[DEBUG] Retrieved indices: {debug_info['retrieved_indices']}")
        print(f"[DEBUG] Retrieved distances: {debug_info['retrieved_distances']}")
        print(f"[DEBUG] Retrieved terms: {debug_info['retrieved_terms']}")
        print(f"[DEBUG] Match count: {debug_info['matched_count']}/{debug_info['total_gt_count']}")
        
        # é¢å¤–æ£€æŸ¥ï¼šçœ‹çœ‹è·ç¦»æœ€è¿‘çš„å‡ ä¸ªterms
        if len(debug_info['retrieved_distances']) > 0:
            print(f"[DEBUG] Closest term distance: {debug_info['retrieved_distances'][0]:.4f}")
            if len(set(debug_info['retrieved_terms'])) == 1:
                print(f"[ERROR] All retrieved terms are identical: '{debug_info['retrieved_terms'][0]}'")
        print(f"[DEBUG] ---")

    # è®¡ç®—sample-levelå’Œterm-levelå¬å›ç‡
    for top_k in top_ks:
        # Sample-levelå¹³å‡å¬å›ç‡
        avg_recall = sum(recall_dict[top_k]) / len(recall_dict[top_k]) if recall_dict[top_k] else 0.0
        eval_mode = " (Relaxed)" if use_relaxed_eval else " (Strict)"
        print(f"[EVAL] Sample-level Average Recall@{top_k}{eval_mode}: {avg_recall:.2%}")
        
        # Term-levelå¾®å¹³å‡å¬å›ç‡
        term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
        total_terms = len(term_retrieval_pairs)
        hit_terms = sum(1 for _, is_retrieved, _ in term_retrieval_pairs if is_retrieved)
        term_micro_avg_recall = hit_terms / total_terms if total_terms > 0 else 0.0
        print(f"[EVAL] Term-level Micro-Average Recall@{top_k}: {term_micro_avg_recall:.2%} ({hit_terms}/{total_terms} terms)")
        
        # è®¡ç®—å·®å¼‚
        diff = avg_recall - term_micro_avg_recall
        if diff > 0:
            print(f"[EVAL] Multi-term sample penalty: -{diff:.2%} (sample-level higher, indicating multi-term samples hurt overall recall)")
        elif diff < 0:
            print(f"[EVAL] Multi-term sample benefit: +{abs(diff):.2%} (term-level higher, indicating multi-term samples help overall recall)")
        else:
            print(f"[EVAL] No difference between sample-level and term-level recall")
        print()
        
    # === ç»Ÿè®¡å’Œæ‰“å°æœªå‘½ä¸­çš„æœ¯è¯­ ===
    if show_missed_terms:
        for top_k in top_ks:
            term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
            missed_terms_info = []
            for gt_term, is_retrieved, sample_info in term_retrieval_pairs:
                if not is_retrieved:
                    missed_terms_info.append((gt_term, sample_info))
            
            print(f"[EVAL] Missed {len(missed_terms_info)} terms for Recall@{top_k}:")
            
            # æŒ‰æœ¯è¯­åˆ†ç»„ç»Ÿè®¡
            missed_terms_count = {}
            for gt_term, sample_info in missed_terms_info:
                if gt_term not in missed_terms_count:
                    missed_terms_count[gt_term] = []
                missed_terms_count[gt_term].append(sample_info)
            
            # æ‰“å°æœªå‘½ä¸­æœ¯è¯­çš„è¯¦ç»†ä¿¡æ¯ï¼ˆé™åˆ¶è¾“å‡ºæ•°é‡ï¼‰
            max_terms_to_show = 20  # æœ€å¤šæ˜¾ç¤º20ä¸ªæœ¯è¯­
            sorted_missed_terms = sorted(missed_terms_count.items(), key=lambda x: len(x[1]), reverse=True)
            
            for i, (missed_term, sample_infos) in enumerate(sorted_missed_terms):
                if i >= max_terms_to_show:
                    remaining_terms = len(sorted_missed_terms) - max_terms_to_show
                    print(f"[EVAL]   ... and {remaining_terms} more missed terms")
                    break
                    
                print(f"[EVAL]   '{missed_term}' (missed {len(sample_infos)} times):")
                
                # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                max_samples_to_show = 3
                for j, sample_info in enumerate(sample_infos):
                    if j >= max_samples_to_show:
                        remaining_samples = len(sample_infos) - max_samples_to_show
                        print(f"[EVAL]     ... and {remaining_samples} more samples")
                        break
                        
                    chunk_text_preview = sample_info['chunk_text'][:100] + '...' if len(sample_info['chunk_text']) > 100 else sample_info['chunk_text']
                    audio_basename = sample_info['audio_path'].split('/')[-1] if sample_info['audio_path'] else 'N/A'
                    print(f"[EVAL]     Sample {sample_info['sample_idx']}: {audio_basename}")
                    print(f"[EVAL]       Text: {chunk_text_preview}")
                    print(f"[EVAL]       All GT terms: {sample_info['all_gt_terms']}")
                    print(f"[EVAL]       Retrieved top-{top_k}: {sample_info['retrieved_terms']}")
            
            print()  # ç©ºè¡Œåˆ†éš”
    else:
        for top_k in top_ks:
            term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
            missed_count = sum(1 for _, is_retrieved, _ in term_retrieval_pairs if not is_retrieved)
            print(f"[EVAL] Missed {missed_count} terms for Recall@{top_k} (details hidden)")
        print()

    # === è®¡ç®— seen/unseen recall (both sample-level and term-level) ===
    if train_terms is not None:
        for top_k in top_ks:
            # åªæœ‰è®­ç»ƒé›†ä¸­çš„æœ¯è¯­æ‰ç®—seen
            seen_terms_set = set(t.lower() for t in train_terms)
            
            # Sample-level seen/unseenåˆ†æ
            seen_recalls, unseen_recalls = [], []
            for recall_val, sample in zip(recall_dict[top_k], valid_samples):
                gt_terms = [t.lower() for t in sample[0]]
                # ä¿®æ­£é€»è¾‘ï¼šå¦‚æœæ ·æœ¬ä¸­æœ‰ä»»ä½•æœ¯è¯­åœ¨è®­ç»ƒé›†ä¸­ï¼Œåˆ™è¯¥æ ·æœ¬å½’ç±»ä¸ºseen
                # è¿™æ ·å¯ä»¥æ›´å¥½åœ°åŒºåˆ†seenå’Œunseenæ ·æœ¬ï¼Œé¿å…è¿‡äºä¸¥æ ¼çš„åˆ†ç±»
                if any(gt in seen_terms_set for gt in gt_terms):
                    seen_recalls.append(recall_val)
                else:
                    unseen_recalls.append(recall_val)

            avg_seen = sum(seen_recalls) / len(seen_recalls) if seen_recalls else 0.0
            avg_unseen = sum(unseen_recalls) / len(unseen_recalls) if unseen_recalls else 0.0
            total_samples = len(seen_recalls) + len(unseen_recalls)
            print(f"[EVAL] Sample-level - Seen Recall@{top_k}: {avg_seen:.2%} ({len(seen_recalls)}/{total_samples} samples), Unseen Recall@{top_k}: {avg_unseen:.2%} ({len(unseen_recalls)}/{total_samples} samples)")
            
            # Term-level seen/unseenåˆ†æ
            term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
            seen_hits, seen_total = 0, 0
            unseen_hits, unseen_total = 0, 0
            
            for gt_term, is_retrieved, sample_info in term_retrieval_pairs:
                if gt_term in seen_terms_set:
                    seen_total += 1
                    if is_retrieved:
                        seen_hits += 1
                else:
                    unseen_total += 1
                    if is_retrieved:
                        unseen_hits += 1
            
            term_seen_recall = seen_hits / seen_total if seen_total > 0 else 0.0
            term_unseen_recall = unseen_hits / unseen_total if unseen_total > 0 else 0.0
            total_terms = seen_total + unseen_total
            unseen_percentage = unseen_total / total_terms * 100 if total_terms > 0 else 0.0
            
            print(f"[EVAL] Term-level - Seen Recall@{top_k}: {term_seen_recall:.2%} ({seen_hits}/{seen_total} terms), Unseen Recall@{top_k}: {term_unseen_recall:.2%} ({unseen_hits}/{unseen_total} terms)")
            print(f"[EVAL] Unseen Term Percentage: {unseen_percentage:.1f}%")
            print()
    else:
        print(f"[WARN] train_terms not provided, skipping seen/unseen analysis")

    model.train()
    return recall_dict


def load_model(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"[INFO] Loading model from {model_path}")
    sys.stdout.flush()
    
    # ç¡®ä¿deviceæ˜¯torch.deviceå¯¹è±¡
    if isinstance(device, str):
        device = torch.device(device)
    
    # åˆå§‹åŒ–ç¼–ç å™¨
    try:
        speech_encoder = SpeechToEmbeddingModelPipeline(
            encoder="sonar_speech_encoder_eng", device=device
        )
    except Exception as e:
        print(f"[ERROR] Failed to initialize speech encoder: {e}")
        sys.stdout.flush()
        print(f"[INFO] Trying alternative initialization...")
        sys.stdout.flush()
        # å°è¯•ä¸ä¼ é€’deviceå‚æ•°
        speech_encoder = SpeechToEmbeddingModelPipeline(
            encoder="sonar_speech_encoder_eng"
        )
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if hasattr(speech_encoder, 'model'):
            speech_encoder.model = speech_encoder.model.to(device)

    try:
        text_encoder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            device=device,
            dtype=torch.float32,
        )
    except Exception as e:
        print(f"[ERROR] Failed to initialize text encoder: {e}")
        sys.stdout.flush()
        print(f"[INFO] Trying alternative initialization...")
        sys.stdout.flush()
        # å°è¯•ä¸ä¼ é€’deviceå‚æ•°
        text_encoder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            dtype=torch.float32,
        )
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if hasattr(text_encoder, 'model'):
            text_encoder.model = text_encoder.model.to(device)

    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œå› ä¸ºç»“æ„éœ€è¦åŒ¹é…ï¼‰
    model = ContrastiveSpeechTextModel(
        speech_encoder, text_encoder, 
        unfreeze_layers=10  # è¿™ä¸ªå‚æ•°ä¸å½±å“æ¨ç†ï¼Œåªå½±å“è®­ç»ƒæ—¶çš„å‚æ•°å†»ç»“
    ).to(device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„å‚æ•°
    state_dict = torch.load(model_path, map_location=device, weights_only=False)
    
    # å¤„ç† DataParallel çš„æƒ…å†µ
    if list(state_dict.keys())[0].startswith('module.'):
        # ç§»é™¤ 'module.' å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            new_state_dict[k[7:]] = v  # ç§»é™¤ 'module.' (7ä¸ªå­—ç¬¦)
        state_dict = new_state_dict
    
    model.load_state_dict(state_dict)
    model.eval()
    print(f"[INFO] Model loaded successfully")
    sys.stdout.flush()
    
    # è‡ªåŠ¨å¤šGPUåŒ…è£…
    if torch.cuda.device_count() > 1:
        print(f"[INFO] ğŸš€ Detected {torch.cuda.device_count()} GPUs, wrapping with DataParallel")
        available_gpus = list(range(torch.cuda.device_count()))
        model = torch.nn.DataParallel(model, device_ids=available_gpus)
        print(f"[INFO] âœ… DataParallel enabled on GPUs: {available_gpus}")
        sys.stdout.flush()
    else:
        print(f"[INFO] Single GPU mode: {device}")
        sys.stdout.flush()
    
    return model


def load_acl_terminology(glossary_csv_path):
    """ä»ACLæœ¯è¯­è¯æ±‡è¡¨ä¸­åŠ è½½è‹±æ–‡æœ¯è¯­"""
    print(f"[INFO] Loading ACL terminology from {glossary_csv_path}")
    sys.stdout.flush()
    
    terms = []
    with open(glossary_csv_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        if len(lines) > 1:
            # è·³è¿‡headerè¡Œï¼Œç¬¬ä¸€åˆ—æ˜¯è‹±æ–‡æœ¯è¯­
            for line in lines[1:]:
                parts = line.strip().split(',')
                if len(parts) > 0 and parts[0]:
                    english_term = parts[0].strip().lower()
                    if len(english_term) >= 2:
                        terms.append(english_term)
    
    # å»é‡
    terms = list(set(terms))
    print(f"[INFO] Loaded {len(terms)} unique English terms from ACL glossary")
    sys.stdout.flush()
    return terms


def parse_acl_tagged_text(tagged_text_path):
    """è§£æACLæ ‡æ³¨çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæå–æœ¯è¯­"""
    print(f"[INFO] Parsing ACL tagged text from {tagged_text_path}")
    sys.stdout.flush()
    
    terms = set()
    with open(tagged_text_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            # æå–æ–¹æ‹¬å·ä¸­çš„æœ¯è¯­ [term]
            import re
            tagged_terms = re.findall(r'\[([^\]]+)\]', line)
            for term in tagged_terms:
                term = term.strip().lower()
                if len(term) >= 2:
                    terms.add(term)
    
    terms_list = list(terms)
    print(f"[INFO] Extracted {len(terms_list)} unique terms from {tagged_text_path}")
    sys.stdout.flush()
    return terms_list


class ACLDataset(Dataset):
    """ACLæ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½ACL segmentedéŸ³é¢‘å’Œå¯¹åº”çš„æœ¯è¯­"""
    
    def __init__(self, acl_root_dir, split="dev", segmentation="gold"):
        """
        Args:
            acl_root_dir: ACLæ•°æ®é›†æ ¹ç›®å½• (å¦‚ data/acl-6060/2/acl_6060)
            split: "dev" æˆ– "eval"
            segmentation: "gold" æˆ– "shas"
        """
        self.acl_root_dir = acl_root_dir
        self.split = split
        self.segmentation = segmentation
        
        # æ„å»ºè·¯å¾„
        self.audio_dir = os.path.join(acl_root_dir, split, "segmented_wavs", segmentation)
        self.text_dir = os.path.join(acl_root_dir, split, "text", "tagged_terminology")
        
        print(f"[INFO] Loading ACL {split} dataset with {segmentation} segmentation")
        print(f"[INFO] Audio dir: {self.audio_dir}")
        print(f"[INFO] Text dir: {self.text_dir}")
        
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
        self.audio_files = []
        if os.path.exists(self.audio_dir):
            for f in os.listdir(self.audio_dir):
                if f.endswith('.wav'):
                    self.audio_files.append(f)
        
        self.audio_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
        
        # åŠ è½½å¯¹åº”çš„æœ¯è¯­æ ‡æ³¨æ–‡ä»¶ (ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬)
        tagged_file = os.path.join(self.text_dir, f"ACL.6060.{split}.tagged.en-xx.en.txt")
        self.tagged_terms = []
        self.sentence_to_terms = {}  # å¥å­IDåˆ°æœ¯è¯­çš„æ˜ å°„
        
        if os.path.exists(tagged_file):
            with open(tagged_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for i, line in enumerate(lines):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æå–æ–¹æ‹¬å·ä¸­çš„æœ¯è¯­
                    import re
                    tagged_terms = re.findall(r'\[([^\]]+)\]', line)
                    clean_terms = [term.strip().lower() for term in tagged_terms if len(term.strip()) >= 2]
                    
                    # å¥å­IDç›´æ¥ä»è¡Œå·+1å¼€å§‹ï¼ˆå¯¹åº”sent_1.wav, sent_2.wav, ...ï¼‰
                    sent_id = i + 1
                    if clean_terms:
                        self.sentence_to_terms[sent_id] = clean_terms
        
        print(f"[INFO] Loaded {len(self.sentence_to_terms)} sentences with terms from tagged file")
        
        # è¿‡æ»¤å‡ºæœ‰æœ¯è¯­æ ‡æ³¨çš„éŸ³é¢‘æ–‡ä»¶
        self.valid_samples = []
        for audio_file in self.audio_files:
            # ä»æ–‡ä»¶åæå–å¥å­ID (å¦‚ sent_1.wav -> 1, sent_401.wav -> 401)
            import re
            match = re.search(r'sent_(\d+)\.wav', audio_file)
            if match:
                sent_id = int(match.group(1))
                if sent_id in self.sentence_to_terms:
                    audio_path = os.path.join(self.audio_dir, audio_file)
                    terms = self.sentence_to_terms[sent_id]
                    
                    # éªŒè¯éŸ³é¢‘æ–‡ä»¶
                    is_valid, reason = is_audio_valid(audio_path)
                    if is_valid:
                        self.valid_samples.append({
                            'audio_path': audio_path,
                            'terms': terms,
                            'sent_id': sent_id
                        })
                    else:
                        print(f"[WARN] Invalid audio {audio_path}: {reason}")
                else:
                    # å¯¹äºæ²¡æœ‰æœ¯è¯­æ ‡æ³¨çš„å¥å­ï¼Œæˆ‘ä»¬è·³è¿‡
                    pass
        
        print(f"[INFO] ACL {split} dataset: {len(self.valid_samples)} valid samples with terms")
        print(f"[INFO] Sentence ID range: {min(s['sent_id'] for s in self.valid_samples)} - {max(s['sent_id'] for s in self.valid_samples)}")
        sys.stdout.flush()
    
    def __len__(self):
        return len(self.valid_samples)
    
    def __getitem__(self, idx):
        sample = self.valid_samples[idx]
        return sample['terms'], sample['audio_path'], f"sent_{sample['sent_id']}", True


class ACLChunkedDataset(Dataset):
    """ACLæ•°æ®é›†çš„chunkedç‰ˆæœ¬ï¼Œå°†sentence-leveléŸ³é¢‘åˆ‡åˆ†æˆ2ç§’å›ºå®šchunk"""
    
    def __init__(self, acl_root_dir, split="dev", segmentation="gold", 
                 chunk_duration=2.0, target_sr=48000, overlap=0.0, 
                 min_chunk_duration=1.0, term_filtering_method="position",
                 save_chunks=False, chunk_save_dir="/mnt/gemini/data/jiaxuanluo/acl_chunks",
                 relaxed_eval=False):
        """
        Args:
            acl_root_dir: ACLæ•°æ®é›†æ ¹ç›®å½• (å¦‚ data/acl-6060/2/acl_6060)
            split: "dev" æˆ– "eval"
            segmentation: "gold" æˆ– "shas"
            chunk_duration: æ¯ä¸ªchunkçš„æ—¶é•¿ï¼ˆç§’ï¼‰
            target_sr: ç›®æ ‡é‡‡æ ·ç‡
            overlap: chunkä¹‹é—´çš„é‡å ï¼ˆç§’ï¼‰
            min_chunk_duration: æœ€å°chunkæ—¶é•¿ï¼Œä½äºæ­¤å€¼çš„chunkä¼šè¢«æ ‡è®°
            term_filtering_method: æœ¯è¯­è¿‡æ»¤æ–¹æ³• ("position", "asr", "none")
                - "position": åŸºäºæœ¯è¯­åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®è¿›è¡Œåˆ†é…
                - "asr": ä½¿ç”¨ASRè½¬å½•è¿›è¡Œæœ¯è¯­åŒ¹é…ï¼ˆéœ€è¦é¢å¤–å®ç°ï¼‰
                - "none": ä¸è¿›è¡Œè¿‡æ»¤ï¼Œæ‰€æœ‰chunkåŒ…å«æ‰€æœ‰æœ¯è¯­ï¼ˆåŸå§‹æ–¹æ³•ï¼‰
            save_chunks: æ˜¯å¦ä¿å­˜chunkæ•°æ®åˆ°æ–‡ä»¶
            chunk_save_dir: chunkæ•°æ®ä¿å­˜ç›®å½•
            relaxed_eval: æ˜¯å¦ä½¿ç”¨å®½æ¾è¯„ä¼°æ¨¡å¼ï¼ˆæ‰€æœ‰sentenceæœ¯è¯­åˆ†é…ç»™æ¯ä¸ªchunkï¼‰
        """
        self.acl_root_dir = acl_root_dir
        self.split = split
        self.segmentation = segmentation
        self.chunk_duration = chunk_duration
        self.target_sr = target_sr
        self.overlap = overlap
        self.min_chunk_duration = min_chunk_duration
        self.term_filtering_method = term_filtering_method
        self.save_chunks = save_chunks
        self.chunk_save_dir = chunk_save_dir
        self.relaxed_eval = relaxed_eval
        
        # æ„å»ºè·¯å¾„
        self.audio_dir = os.path.join(acl_root_dir, split, "segmented_wavs", segmentation)
        self.text_dir = os.path.join(acl_root_dir, split, "text", "tagged_terminology")
        
        print(f"[INFO] Loading ACL {split} chunked dataset with {segmentation} segmentation")
        print(f"[INFO] Chunk duration: {chunk_duration}s, Min chunk: {min_chunk_duration}s")
        print(f"[INFO] Term filtering method: {term_filtering_method}")
        print(f"[INFO] Relaxed evaluation mode: {relaxed_eval}")
        print(f"[INFO] Audio dir: {self.audio_dir}")
        print(f"[INFO] Text dir: {self.text_dir}")
        
        # é¦–å…ˆåŠ è½½åŸå§‹ACLæ•°æ®
        original_dataset = ACLDataset(acl_root_dir, split, segmentation)
        
        # åŠ è½½åŸå§‹æ–‡æœ¬ï¼ˆä¸å¸¦æ ‡è®°çš„ç‰ˆæœ¬ï¼‰ç”¨äºæœ¯è¯­ä½ç½®åˆ†æ
        self.sentence_to_raw_text = {}
        if term_filtering_method == "position":
            raw_text_file = os.path.join(self.text_dir, f"ACL.6060.{split}.tagged.en-xx.en.txt")
            if os.path.exists(raw_text_file):
                with open(raw_text_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    for i, line in enumerate(lines):
                        sent_id = i + 1
                        # ç§»é™¤æœ¯è¯­æ ‡è®° [term] è·å¾—çº¯æ–‡æœ¬
                        import re
                        raw_text = re.sub(r'\[([^\]]+)\]', r'\1', line.strip())
                        self.sentence_to_raw_text[sent_id] = raw_text.lower()
                print(f"[INFO] Loaded raw text for {len(self.sentence_to_raw_text)} sentences for position-based filtering")
        
        # å¼€å§‹chunkåˆ‡åˆ†
        self.chunked_samples = []
        short_chunks_count = 0
        total_chunks_count = 0
        term_filtering_stats = {
            'original_terms': 0,
            'filtered_terms': 0,
            'chunks_with_terms': 0,
            'chunks_without_terms': 0
        }
        
        print(f"[INFO] Processing {len(original_dataset)} original sentences...")
        
        for idx in range(len(original_dataset)):
            terms, audio_path, sent_text, has_target = original_dataset[idx]
            
            # åŠ è½½å’Œåˆ‡åˆ†éŸ³é¢‘
            try:
                # è·å–éŸ³é¢‘ä¿¡æ¯
                data, sr = sf.read(audio_path)
                original_duration = len(data) / sr
                
                # ä½¿ç”¨chunk_splitterè¿›è¡Œåˆ‡åˆ†
                audio_chunks = split_audio_from_path(
                    audio_path, 
                    chunk_duration=chunk_duration, 
                    target_sr=target_sr, 
                    overlap=overlap
                )
                
                if not audio_chunks:
                    print(f"[WARN] No chunks generated for {audio_path}")
                    continue
                
                # ä¸ºæ¯ä¸ªchunkåˆ›å»ºæ ·æœ¬
                for chunk_idx, chunk_tensor in enumerate(audio_chunks):
                    chunk_samples = chunk_tensor.shape[-1]
                    actual_chunk_duration = chunk_samples / target_sr
                    
                    # æ£€æŸ¥chunkæ˜¯å¦è¿‡çŸ­
                    is_short_chunk = actual_chunk_duration < min_chunk_duration
                    if is_short_chunk:
                        short_chunks_count += 1
                    
                    total_chunks_count += 1
                    
                    # è¿›è¡Œæœ¯è¯­è¿‡æ»¤
                    sent_id = original_dataset.valid_samples[idx]['sent_id']
                    if self.relaxed_eval:
                        # å®½æ¾æ¨¡å¼ï¼šæ‰€æœ‰chunkä½¿ç”¨å®Œæ•´çš„sentenceæœ¯è¯­
                        filtered_terms = terms

                    else:
                        # ä¸¥æ ¼æ¨¡å¼ï¼šæ ¹æ®è®¾å®šçš„æ–¹æ³•è¿›è¡Œè¿‡æ»¤
                        filtered_terms = self._filter_terms_for_chunk(
                            terms, sent_id, chunk_idx, len(audio_chunks), original_duration
                        )

                    
                    # æ›´æ–°ç»Ÿè®¡
                    term_filtering_stats['original_terms'] += len(terms)
                    term_filtering_stats['filtered_terms'] += len(filtered_terms)
                    if len(filtered_terms) > 0:
                        term_filtering_stats['chunks_with_terms'] += 1
                    else:
                        term_filtering_stats['chunks_without_terms'] += 1
                    
                    # åˆ›å»ºchunkæ ·æœ¬ä¿¡æ¯
                    chunk_sample = {
                        'terms': filtered_terms,  # ä½¿ç”¨è¿‡æ»¤åçš„æœ¯è¯­
                        'original_terms': terms,  # ä¿ç•™åŸå§‹æœ¯è¯­ç”¨äºè°ƒè¯•
                        'audio_path': audio_path,  # åŸå§‹éŸ³é¢‘è·¯å¾„ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        'audio_tensor': chunk_tensor,  # chunkéŸ³é¢‘tensor
                        'chunk_id': f"sent_{sent_id}_chunk_{chunk_idx}",
                        'original_sent_id': sent_id,
                        'chunk_index': chunk_idx,
                        'total_chunks': len(audio_chunks),
                        'chunk_duration': actual_chunk_duration,
                        'original_duration': original_duration,
                        'is_short_chunk': is_short_chunk,  # æ ‡è®°çŸ­chunk
                        'is_chunked': True,
                        'has_target': has_target and (len(filtered_terms) > 0 or self.relaxed_eval)  # åŸºäºè¿‡æ»¤åçš„æœ¯è¯­ï¼Œrelaxedæ¨¡å¼ä¿ç•™æ‰€æœ‰chunk
                    }
                    
                    self.chunked_samples.append(chunk_sample)
                    
            except Exception as e:
                print(f"[ERROR] Failed to process {audio_path}: {e}")
                continue
        
        # ç»Ÿè®¡ä¿¡æ¯
        valid_chunks = [s for s in self.chunked_samples if s['has_target']]
        print(f"[INFO] ACL chunked dataset statistics:")
        print(f"[INFO] - Total chunks: {total_chunks_count}")
        print(f"[INFO] - Valid chunks (with terms): {len(valid_chunks)}")
        print(f"[INFO] - Short chunks (< {min_chunk_duration}s): {short_chunks_count} ({short_chunks_count/total_chunks_count*100:.1f}%)")
        print(f"[INFO] - Average chunks per sentence: {total_chunks_count / len(original_dataset):.2f}")
        
        # æœ¯è¯­è¿‡æ»¤ç»Ÿè®¡
        if term_filtering_method != "none" or relaxed_eval:
            total_original = term_filtering_stats['original_terms']
            total_filtered = term_filtering_stats['filtered_terms']
            chunks_with = term_filtering_stats['chunks_with_terms']
            chunks_without = term_filtering_stats['chunks_without_terms']
            
            if relaxed_eval:
                filtering_desc = f"relaxed (all sentence terms)"
            else:
                filtering_desc = term_filtering_method
            
            print(f"[INFO] Term assignment statistics ({filtering_desc}):")
            print(f"[INFO] - Original terms: {total_original}")
            print(f"[INFO] - Assigned terms: {total_filtered} ({total_filtered/total_original*100:.1f}% retained)")
            print(f"[INFO] - Chunks with terms: {chunks_with}")
            print(f"[INFO] - Chunks without terms: {chunks_without} ({chunks_without/total_chunks_count*100:.1f}%)")
        
        # åªä¿ç•™æœ‰æœ¯è¯­çš„chunks
        self.chunked_samples = valid_chunks
        print(f"[INFO] Final chunked dataset size: {len(self.chunked_samples)}")
        
        # ä¿å­˜chunkæ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if save_chunks:
            self._save_chunks_to_files()
        
        sys.stdout.flush()
    
    def __len__(self):
        return len(self.chunked_samples)
    
    def __getitem__(self, idx):
        chunk_sample = self.chunked_samples[idx]
        return (
            chunk_sample['terms'], 
            chunk_sample['chunk_id'],  # ä½¿ç”¨chunk_idä½œä¸ºæ ‡è¯†
            chunk_sample['chunk_id'],  # chunk textæ ‡è¯†
            chunk_sample['has_target']
        )
    
    def get_audio_tensor(self, idx):
        """è·å–chunkçš„éŸ³é¢‘tensor"""
        return self.chunked_samples[idx]['audio_tensor']
    
    def get_chunk_info(self, idx):
        """è·å–chunkçš„è¯¦ç»†ä¿¡æ¯"""
        chunk = self.chunked_samples[idx]
        return {
            'chunk_id': chunk['chunk_id'],
            'original_sent_id': chunk['original_sent_id'],
            'chunk_index': chunk['chunk_index'],
            'total_chunks': chunk['total_chunks'],
            'chunk_duration': chunk['chunk_duration'],
            'original_duration': chunk['original_duration'],
            'is_short_chunk': chunk['is_short_chunk'],
            'terms': chunk['terms'],
            'original_terms': chunk.get('original_terms', chunk['terms'])
        }
    
    def _filter_terms_for_chunk(self, terms, sent_id, chunk_idx, total_chunks, original_duration):
        """
        æ ¹æ®ä¸åŒç­–ç•¥è¿‡æ»¤æœ¯è¯­ï¼Œç¡®ä¿æ¯ä¸ªchunkåªåŒ…å«å®é™…åœ¨å…¶å†…å®¹ä¸­çš„æœ¯è¯­
        
        Args:
            terms: åŸå§‹æœ¯è¯­åˆ—è¡¨
            sent_id: å¥å­ID
            chunk_idx: chunkç´¢å¼•
            total_chunks: æ€»chunkæ•°
            original_duration: åŸå§‹éŸ³é¢‘æ—¶é•¿
            
        Returns:
            è¿‡æ»¤åçš„æœ¯è¯­åˆ—è¡¨
        """
        if self.term_filtering_method == "none":
            # ä¸è¿›è¡Œè¿‡æ»¤ï¼Œè¿”å›æ‰€æœ‰æœ¯è¯­
            return terms
        
        elif self.term_filtering_method == "position":
            # åŸºäºæœ¯è¯­åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®è¿›è¡Œè¿‡æ»¤
            return self._filter_terms_by_position(terms, sent_id, chunk_idx, total_chunks)
        
        elif self.term_filtering_method == "asr":
            # åŸºäºASRçš„æœ¯è¯­è¿‡æ»¤ï¼ˆå¾…å®ç°ï¼‰
            print(f"[WARN] ASR-based term filtering not implemented, falling back to position-based")
            return self._filter_terms_by_position(terms, sent_id, chunk_idx, total_chunks)
        
        else:
            print(f"[WARN] Unknown term filtering method: {self.term_filtering_method}, using 'none'")
            return terms
    
    def _filter_terms_by_position(self, terms, sent_id, chunk_idx, total_chunks):
        """
        åŸºäºæœ¯è¯­åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®è¿›è¡Œchunkçº§åˆ«çš„æœ¯è¯­è¿‡æ»¤
        
        è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼æ–¹æ³•ï¼Œå‡è®¾ï¼š
        1. æœ¯è¯­åœ¨æ–‡æœ¬ä¸­çš„ç›¸å¯¹ä½ç½®å¯¹åº”å…¶åœ¨éŸ³é¢‘ä¸­çš„ç›¸å¯¹ä½ç½®
        2. æ¯ä¸ªchunkè¦†ç›–å¥å­çš„ä¸€ä¸ªè¿ç»­æ—¶é—´æ®µ
        """
        if sent_id not in self.sentence_to_raw_text:
            # å¦‚æœæ²¡æœ‰åŸå§‹æ–‡æœ¬ï¼Œè¿”å›æ‰€æœ‰æœ¯è¯­
            return terms
        
        raw_text = self.sentence_to_raw_text[sent_id]
        text_length = len(raw_text)
        
        if text_length == 0:
            return []
        
        # è®¡ç®—chunkåœ¨æ–‡æœ¬ä¸­çš„è¦†ç›–èŒƒå›´
        chunk_start_ratio = chunk_idx / total_chunks
        chunk_end_ratio = (chunk_idx + 1) / total_chunks
        
        # è½¬æ¢ä¸ºæ–‡æœ¬ä½ç½®
        text_start = int(chunk_start_ratio * text_length)
        text_end = int(chunk_end_ratio * text_length)
        
        # æ‰©å±•è¾¹ç•Œä»¥å¤„ç†æœ¯è¯­è·¨è¾¹ç•Œçš„æƒ…å†µï¼ˆæ·»åŠ 10%çš„ç¼“å†²åŒºï¼‰
        buffer_size = max(10, int(0.1 * (text_end - text_start)))
        text_start = max(0, text_start - buffer_size)
        text_end = min(text_length, text_end + buffer_size)
        
        chunk_text = raw_text[text_start:text_end]
        
        # è¿‡æ»¤æœ¯è¯­ï¼šåªä¿ç•™åœ¨chunkæ–‡æœ¬èŒƒå›´å†…çš„æœ¯è¯­
        filtered_terms = []
        for term in terms:
            term_lower = term.lower()
            # æ£€æŸ¥æœ¯è¯­æ˜¯å¦åœ¨chunkæ–‡æœ¬ä¸­å‡ºç°
            if term_lower in chunk_text:
                # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šç¡®ä¿æ˜¯å®Œæ•´çš„è¯ï¼ˆé¿å…éƒ¨åˆ†åŒ¹é…ï¼‰
                import re
                pattern = r'\b' + re.escape(term_lower) + r'\b'
                if re.search(pattern, chunk_text):
                    filtered_terms.append(term)
        
        return filtered_terms
    
    def _save_chunks_to_files(self):
        """ä¿å­˜chunkæ•°æ®åˆ°æ–‡ä»¶ï¼ŒåŒ…æ‹¬éŸ³é¢‘æ–‡ä»¶å’Œå…ƒæ•°æ®JSON"""
        import os
        from pathlib import Path
        import json
        import soundfile as sf
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = Path(self.chunk_save_dir)
        audio_dir = save_dir / "audio"
        metadata_dir = save_dir / "metadata"
        
        audio_dir.mkdir(parents=True, exist_ok=True)
        metadata_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"[INFO] Saving {len(self.chunked_samples)} chunks to {save_dir}")
        
        # å‡†å¤‡å…ƒæ•°æ®
        chunks_metadata = []
        sample_chunks = []  # ç”¨äºæŠ½æ ·æ£€æŸ¥
        
        for idx, chunk_sample in enumerate(self.chunked_samples):
            chunk_id = chunk_sample['chunk_id']
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_tensor = chunk_sample['audio_tensor']
            if audio_tensor.dim() > 1:
                audio_tensor = audio_tensor.squeeze()
            
            audio_filename = f"{chunk_id}.wav"
            audio_path = audio_dir / audio_filename
            
            # ä¿å­˜ä¸º16kHzçš„wavæ–‡ä»¶ï¼ˆé€‚åˆSONARï¼‰
            audio_16k = self._resample_to_16k(audio_tensor)
            sf.write(str(audio_path), audio_16k.detach().numpy(), 16000)
            
            # å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                'chunk_id': chunk_id,
                'original_sent_id': chunk_sample['original_sent_id'],
                'chunk_index': chunk_sample['chunk_index'],
                'total_chunks': chunk_sample['total_chunks'],
                'chunk_duration': chunk_sample['chunk_duration'],
                'original_duration': chunk_sample['original_duration'],
                'is_short_chunk': chunk_sample['is_short_chunk'],
                'audio_file': audio_filename,
                'audio_path_relative': f"audio/{audio_filename}",
                'terms': chunk_sample['terms'],
                'original_terms': chunk_sample.get('original_terms', chunk_sample['terms']),
                'term_filtering_method': self.term_filtering_method,
                'original_audio_path': chunk_sample['audio_path']
            }
            
            chunks_metadata.append(metadata)
            
            # æ”¶é›†å‰10ä¸ªchunkç”¨äºæŠ½æ ·æ£€æŸ¥
            if idx < 10:
                sample_chunks.append(metadata)
        
        # ä¿å­˜å®Œæ•´å…ƒæ•°æ®
        metadata_file = metadata_dir / f"chunks_{self.split}_{self.segmentation}_{self.term_filtering_method}.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_metadata, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜æŠ½æ ·æ£€æŸ¥æ•°æ®
        sample_file = metadata_dir / f"sample_chunks_{self.split}_{self.segmentation}_{self.term_filtering_method}.json"
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_chunks, f, indent=2, ensure_ascii=False)
        
        # åˆ›å»ºæ£€æŸ¥æŠ¥å‘Š
        self._create_inspection_report(save_dir, chunks_metadata, sample_chunks)
        
        print(f"[INFO] âœ… Chunks saved successfully:")
        print(f"[INFO]   - Audio files: {audio_dir} ({len(chunks_metadata)} files)")
        print(f"[INFO]   - Metadata: {metadata_file}")
        print(f"[INFO]   - Sample data: {sample_file}")
        print(f"[INFO]   - Inspection report: {save_dir}/inspection_report.txt")
    
    def _resample_to_16k(self, audio_tensor):
        """å°†éŸ³é¢‘ä»48kHzé‡é‡‡æ ·åˆ°16kHz"""
        if audio_tensor.shape[0] == 0:
            return torch.zeros(0)
        
        # ç®€å•çš„ä¸‹é‡‡æ ·ï¼šæ¯3ä¸ªé‡‡æ ·ç‚¹å–1ä¸ª (48000/16000 = 3)
        step = 3
        resampled = audio_tensor[::step]
        
        return resampled
    
    def _create_inspection_report(self, save_dir, chunks_metadata, sample_chunks):
        """åˆ›å»ºè¯¦ç»†çš„æ£€æŸ¥æŠ¥å‘Š"""
        report_path = save_dir / "inspection_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("ACL Chunked Dataset Inspection Report\n")
            f.write("=" * 50 + "\n\n")
            
            # åŸºæœ¬ç»Ÿè®¡
            f.write("ğŸ“Š Basic Statistics:\n")
            f.write(f"  - Total chunks: {len(chunks_metadata)}\n")
            f.write(f"  - Split: {self.split}\n")
            f.write(f"  - Segmentation: {self.segmentation}\n")
            f.write(f"  - Chunk duration: {self.chunk_duration}s\n")
            f.write(f"  - Term filtering method: {self.term_filtering_method}\n")
            f.write(f"  - Target sample rate: {self.target_sr}Hz\n\n")
            
            # æœ¯è¯­ç»Ÿè®¡
            all_terms = []
            all_original_terms = []
            short_chunks = 0
            
            for chunk in chunks_metadata:
                all_terms.extend(chunk['terms'])
                all_original_terms.extend(chunk['original_terms'])
                if chunk['is_short_chunk']:
                    short_chunks += 1
            
            f.write("ğŸ” Term Filtering Analysis:\n")
            f.write(f"  - Original terms (total): {len(all_original_terms)}\n")
            f.write(f"  - Filtered terms (total): {len(all_terms)}\n")
            f.write(f"  - Retention rate: {len(all_terms)/len(all_original_terms)*100:.1f}%\n")
            f.write(f"  - Unique original terms: {len(set(all_original_terms))}\n")
            f.write(f"  - Unique filtered terms: {len(set(all_terms))}\n")
            f.write(f"  - Short chunks: {short_chunks} ({short_chunks/len(chunks_metadata)*100:.1f}%)\n\n")
            
            # æŠ½æ ·æ£€æŸ¥
            f.write("ğŸ”¬ Sample Inspection (First 10 chunks):\n")
            f.write("-" * 50 + "\n")
            
            for i, chunk in enumerate(sample_chunks):
                f.write(f"\nChunk {i+1}: {chunk['chunk_id']}\n")
                f.write(f"  ğŸ“ Audio file: {chunk['audio_file']}\n")
                f.write(f"  â±ï¸  Duration: {chunk['chunk_duration']:.2f}s")
                if chunk['is_short_chunk']:
                    f.write(" (SHORT)")
                f.write("\n")
                f.write(f"  ğŸ“ Original sentence: {chunk['original_sent_id']}\n")
                f.write(f"  ğŸ§© Chunk position: {chunk['chunk_index']}/{chunk['total_chunks']-1}\n")
                f.write(f"  ğŸ“š Original terms: {chunk['original_terms']}\n")
                f.write(f"  âœ… Filtered terms: {chunk['terms']}\n")
                
                if chunk['original_terms'] != chunk['terms']:
                    removed = set(chunk['original_terms']) - set(chunk['terms'])
                    f.write(f"  âŒ Removed terms: {list(removed)}\n")
                
                f.write(f"  ğŸµ Original audio: {chunk['original_audio_path']}\n")
            
            f.write("\n" + "=" * 50 + "\n")
            f.write("ğŸ’¡ Usage: Check sample chunks to verify term filtering accuracy\n")
            f.write("ğŸ§ Listen to audio files to confirm terms are actually present\n")
            f.write("ğŸ“ˆ Compare filtered vs original terms to assess filtering quality\n")


def main():
    parser = argparse.ArgumentParser(description="Full evaluation with complete glossary or ACL dataset")
    parser.add_argument('--model_path', type=str, required=True,
                       help="Path to trained model (.pt file)")
    
    # === ACL evaluation mode ===
    parser.add_argument('--acl_mode', action='store_true',
                       help="Enable ACL evaluation mode")
    parser.add_argument('--acl_root_dir', type=str, 
                       default="data/acl-6060/2/acl_6060",
                       help="Path to ACL dataset root directory")
    parser.add_argument('--acl_glossary_path', type=str,
                       default="data/acl-6060/2/intermediate_files/terminology_glossary.csv",
                       help="Path to ACL terminology glossary CSV file")
    parser.add_argument('--acl_test_split', type=str, default="eval", choices=["dev", "eval"],
                       help="ACL split to use for testing")
    parser.add_argument('--acl_index_split', type=str, default="dev", choices=["dev", "eval"],
                       help="ACL split to use for building index (terminology source)")
    parser.add_argument('--acl_segmentation', type=str, default="gold", choices=["gold", "shas"],
                       help="ACL segmentation type to use")
    parser.add_argument('--acl_chunked', action='store_true',
                       help="Use chunked ACL dataset (2-second chunks)")
    parser.add_argument('--chunk_duration', type=float, default=2.0,
                       help="Duration of each chunk in seconds (for chunked mode)")
    parser.add_argument('--min_chunk_duration', type=float, default=1.0,
                       help="Minimum chunk duration to avoid marking as short")
    parser.add_argument('--term_filtering_method', type=str, default="position", 
                       choices=["position", "asr", "none"],
                       help="Term filtering method for chunked mode")
    parser.add_argument('--save_chunks', action='store_true',
                       help="Save chunked audio and metadata to files for inspection")
    parser.add_argument('--chunk_save_dir', type=str, default="/mnt/gemini/data/jiaxuanluo/acl_chunks",
                       help="Directory to save chunk files")
    parser.add_argument('--relaxed_chunk_eval', action='store_true',
                       help="Use relaxed evaluation: assign all sentence terms to chunks, match if any candidate hits")
    
    # === Original evaluation mode ===
    parser.add_argument('--test_samples_path', type=str, 
                       default="data/xl_term_level_chunks_merged.json",
                       help="Path to test samples (for non-ACL mode)")
    parser.add_argument('--glossary_path', type=str, 
                       default="data/terms/glossary_filtered.json",
                       help="Path to complete glossary file (for non-ACL mode)")
    parser.add_argument('--glossary_emb_path', type=str, default=None,
                       help="Path to pre-built glossary embedding index (.faiss file). If provided, will skip text encoding")
    parser.add_argument('--train_samples_path', type=str,
                       default="data/samples/xl/term_level_chunks_single_0_500000.json",
                       help="Path to training samples for seen/unseen analysis (for non-ACL mode)")
    
    parser.add_argument('--max_eval', type=int, default=1000,
                       help="Maximum number of samples to evaluate")
    parser.add_argument('--batch_size', type=int, default=512,
                       help="Initial batch size for text encoding (will be auto-optimized, max 1024)")
    parser.add_argument('--audio_batch_size', type=int, default=1000,
                       help="Initial batch size for audio encoding (will be auto-optimized, max 128)")

    # === Offline asset building args ===
    parser.add_argument('--build_offline_assets', action='store_true',
                       help='If set, build and save offline glossary assets (embeddings + FAISS index) and exit')
    parser.add_argument('--asset_out_dir', type=str, default='data',
                       help='Output directory for offline assets')
    parser.add_argument('--index_type', type=str, default='ivfpq', choices=['ivfpq', 'flat'],
                       help='Index type to build for offline assets')
    parser.add_argument('--use_ip', action='store_true',
                       help='Use inner-product metric (cosine if vectors are L2-normalized). Default false => L2')
    parser.add_argument('--nlist', type=int, default=4096, help='IVF nlist (coarse clusters)')
    parser.add_argument('--pq_m', type=int, default=64, help='PQ m (number of subvectors)')
    parser.add_argument('--pq_bits', type=int, default=8, help='PQ bits per subvector')
    parser.add_argument('--nprobe', type=int, default=16, help='nprobe for IVF search/add sanity')
    parser.add_argument('--shard_size', type=int, default=0,
                       help='If >0, build multiple sharded indices of at most this many vectors per shard')

    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[INFO] Using device: {device}")
    sys.stdout.flush()

    # === åŠ è½½æ¨¡å‹ ===
    device = torch.device(device)  # è½¬æ¢ä¸ºtorch.deviceå¯¹è±¡
    model = load_model(args.model_path, device)

    # === ACL evaluation mode ===
    if args.acl_mode:
        print("\n" + "="*50)
        print("ACL EVALUATION MODE")
        print("="*50)
        sys.stdout.flush()
        
        # 1. ä»ACL glossaryæˆ–dev setæ„å»ºæœ¯è¯­ç´¢å¼•
        print(f"[INFO] Building index from ACL {args.acl_index_split} split")
        
        # æ–¹æ³•1: ä½¿ç”¨ACLæœ¯è¯­è¯æ±‡è¡¨
        if os.path.exists(args.acl_glossary_path):
            print(f"[INFO] Using ACL terminology glossary: {args.acl_glossary_path}")
            index_terms = load_acl_terminology(args.acl_glossary_path)
        else:
            # æ–¹æ³•2: ä»dev setçš„taggedæ–‡æœ¬ä¸­æå–æœ¯è¯­
            print(f"[INFO] Extracting terms from ACL {args.acl_index_split} tagged text")
            tagged_file = os.path.join(args.acl_root_dir, args.acl_index_split, "text", "tagged_terminology", f"ACL.6060.{args.acl_index_split}.tagged.en-xx.en.txt")
            if os.path.exists(tagged_file):
                index_terms = parse_acl_tagged_text(tagged_file)
            else:
                raise FileNotFoundError(f"ACL tagged text file not found: {tagged_file}")
        
        print(f"[INFO] Building index with {len(index_terms)} ACL terms")
        
        # 2. åŠ è½½ACLæµ‹è¯•æ•°æ®é›†
        if args.acl_chunked:
            print(f"[INFO] Using chunked ACL dataset (chunks: {args.chunk_duration}s)")
            test_dataset = ACLChunkedDataset(
                acl_root_dir=args.acl_root_dir,
                split=args.acl_test_split,
                segmentation=args.acl_segmentation,
                chunk_duration=args.chunk_duration,
                target_sr=48000,
                overlap=0.0,
                min_chunk_duration=args.min_chunk_duration,
                term_filtering_method=args.term_filtering_method,
                save_chunks=args.save_chunks,
                chunk_save_dir=args.chunk_save_dir,
                relaxed_eval=args.relaxed_chunk_eval
            )
        else:
            print(f"[INFO] Using sentence-level ACL dataset")
            test_dataset = ACLDataset(
                acl_root_dir=args.acl_root_dir,
                split=args.acl_test_split,
                segmentation=args.acl_segmentation
            )
        
        # 3. åˆå§‹åŒ–æ£€ç´¢å™¨
        retriever = Retriever(enable_fusion=True, device=device)
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        retriever.model = raw_model
        retriever.index = faiss.IndexFlatL2(512)
        retriever.term_list = [{'term': t} for t in index_terms]
        
        # 4. æ‰§è¡ŒACLè¯„ä¼°
        print(f"\n[INFO] ACL Evaluation Setup:")
        print(f"[INFO] - Index split: {args.acl_index_split} ({len(index_terms)} terms)")
        print(f"[INFO] - Test split: {args.acl_test_split} ({len(test_dataset)} samples)")
        print(f"[INFO] - Segmentation: {args.acl_segmentation}")
        sys.stdout.flush()
        
        recall_results = evaluate_topk_recall(
            model, retriever, test_dataset, device,
            top_ks=(1, 5, 10),
            max_eval=min(args.max_eval, len(test_dataset)),
            train_terms=None,  # ACLæ¨¡å¼ä¸‹ä¸åšseen/unseenåˆ†æ
            show_missed_terms=True,
            glossary_emb_path=None,  # ACLæ¨¡å¼ä¸‹ä¸ä½¿ç”¨é¢„æ„å»ºç´¢å¼•
            relaxed_chunk_eval=args.relaxed_chunk_eval
        )
        
        # 5. ä¿å­˜ACLè¯„ä¼°ç»“æœ
        chunked_suffix = '_chunked' if args.acl_chunked else ''
        relaxed_suffix = '_relaxed' if args.relaxed_chunk_eval else ''
        results_path = args.model_path.replace('.pt', f'_acl_{args.acl_test_split}{chunked_suffix}{relaxed_suffix}_eval_results.json')
        eval_summary = {
            'model_path': args.model_path,
            'acl_mode': True,
            'acl_chunked': args.acl_chunked,
            'relaxed_chunk_eval': args.relaxed_chunk_eval,
            'chunk_duration': args.chunk_duration if args.acl_chunked else None,
            'min_chunk_duration': args.min_chunk_duration if args.acl_chunked else None,
            'term_filtering_method': args.term_filtering_method if args.acl_chunked else None,
            'acl_root_dir': args.acl_root_dir,
            'acl_glossary_path': args.acl_glossary_path,
            'acl_test_split': args.acl_test_split,
            'acl_index_split': args.acl_index_split,
            'acl_segmentation': args.acl_segmentation,
            'index_terms_count': len(index_terms),
            'test_samples': len(test_dataset),
            'evaluated_samples': min(args.max_eval, len(test_dataset)),
            'results': {}
        }
        
        for top_k in [1, 5, 10]:
            if top_k in recall_results and recall_results[top_k]:
                avg_recall = sum(recall_results[top_k]) / len(recall_results[top_k])
                eval_summary['results'][f'recall@{top_k}'] = float(avg_recall)
        
        with open(results_path, 'w') as f:
            json.dump(eval_summary, f, indent=2)
        
        print(f"\n[INFO] ACL evaluation results saved to {results_path}")
        print(f"[INFO] ACL evaluation completed!")
        sys.stdout.flush()
        return

    # === Offline asset building path ===
    if args.build_offline_assets:
        print("[ASSET] Building offline glossary assets...")
        sys.stdout.flush()

        # 1) Load glossary
        glossary_terms = load_glossary_terms(args.glossary_path)
        print(f"[ASSET] Total glossary terms: {len(glossary_terms)}")
        sys.stdout.flush()

        # 2) Encode texts using the SAME text encoder as training
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        text_emb_t = encode_texts_in_batches(raw_model, glossary_terms, batch_size=args.batch_size, device=device)
        text_emb = text_emb_t.detach().cpu().numpy()

        # 3) L2 normalize if we plan to use IP (cosine via inner product)
        if args.use_ip:
            text_emb = l2_normalize_numpy(text_emb)
            print("[ASSET] L2-normalized embeddings for IP/cosine metric.")
        else:
            print("[ASSET] Using L2 metric; skipping normalization requirement.")
        sys.stdout.flush()

        # 4) Save maps (term2idx, terms.txt)
        save_offline_assets(text_emb, glossary_terms, args.asset_out_dir)

        # 5) Build FAISS index (ivfpq or flat) â€” single or sharded
        index_paths = []
        if args.index_type == 'flat':
            # Flat index (primarily for debugging)
            d = text_emb.shape[1]
            if args.use_ip:
                index = faiss.IndexFlatIP(d)
            else:
                index = faiss.IndexFlatL2(d)
            index.add(text_emb)
            out_path = os.path.join(args.asset_out_dir, 'glossary_emb.flat.faiss')
            faiss.write_index(index, out_path)
            print(f"[ASSET] Wrote flat index -> {out_path} (ntotal={index.ntotal})")
            del index
            index_paths.append(out_path)
        else:
            # ivfpq path (recommended)
            if args.shard_size and args.shard_size > 0:
                index_paths = build_sharded_ivfpq_indices(
                    text_emb, glossary_terms, args.asset_out_dir,
                    shard_size=args.shard_size, use_ip=args.use_ip,
                    nlist=args.nlist, pq_m=args.pq_m, pq_bits=args.pq_bits, train_size=min(args.nlist * 100, text_emb.shape[0]), nprobe=args.nprobe
                )
                print(f"[ASSET] Built {len(index_paths)} sharded indices under {args.asset_out_dir}")
            else:
                index = build_ivfpq_index(
                    text_emb, use_ip=args.use_ip, nlist=args.nlist, pq_m=args.pq_m, pq_bits=args.pq_bits, train_size=min(args.nlist * 100, text_emb.shape[0]), nprobe=args.nprobe
                )
                out_path = os.path.join(args.asset_out_dir, 'glossary_emb.ivfpq.faiss')
                faiss.write_index(index, out_path)
                print(f"[ASSET] Wrote IVF-PQ index -> {out_path} (ntotal={index.ntotal})")
                del index
                index_paths.append(out_path)

        print("[ASSET] Offline asset building finished.")
        sys.stdout.flush()
        return

    # === åŸæœ‰çš„æ­£å¸¸è¯„ä¼°æ¨¡å¼ ===
    # === åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼Œä¸æŒ‰æ¯”ä¾‹åˆ‡åˆ†ï¼‰ ===
    print(f"[INFO] Loading test dataset from {args.test_samples_path}")
    sys.stdout.flush()
    test_dataset = TermLevelDataset(
        None,
        split="test",
        test_path=args.test_samples_path
    )
    print(f"[INFO] Test dataset size: {len(test_dataset)}")
    sys.stdout.flush()

    # === åŠ è½½è®­ç»ƒæ ·æœ¬ç”¨äº seen/unseen ç»Ÿè®¡ï¼ˆä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰ ===
    print(f"[INFO] Loading training samples for seen/unseen from {args.train_samples_path}")
    sys.stdout.flush()
    train_dataset = TermLevelDataset(
        args.train_samples_path,
        split="train",
        train_ratio=1.0
    )
    train_terms = extract_all_used_terms(train_dataset)
    print(f"[INFO] Training terms collected: {len(train_terms)}")
    sys.stdout.flush()

    # === åŠ è½½å®Œæ•´æœ¯è¯­è¡¨å¹¶åˆå§‹åŒ–æ£€ç´¢å™¨ ===
    if args.glossary_emb_path and os.path.exists(args.glossary_emb_path):
        # ä½¿ç”¨é¢„æ„å»ºç´¢å¼•ï¼Œä¸éœ€è¦åŠ è½½glossary terms
        print(f"[INFO] Using pre-built glossary index: {args.glossary_emb_path}")
        glossary_terms = []  # ç©ºåˆ—è¡¨ï¼Œå› ä¸ºç´¢å¼•ä¸­å·²åŒ…å«æ‰€æœ‰ä¿¡æ¯
    else:
        # éœ€è¦åŠ è½½glossary termsæ¥æ„å»ºç´¢å¼•
        glossary_terms = load_glossary_terms(args.glossary_path)
        print(f"[INFO] Loaded {len(glossary_terms)} terms from glossary")
    
    retriever = Retriever(enable_fusion=True, device=device)
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    retriever.model = raw_model
    retriever.index = faiss.IndexFlatL2(512)
    
    if not args.glossary_emb_path or not os.path.exists(args.glossary_emb_path):
        # åªæœ‰åœ¨æ²¡æœ‰é¢„æ„å»ºç´¢å¼•æ—¶æ‰è®¾ç½®term_list
        retriever.term_list = [{'term': t} for t in glossary_terms]
        print(f"[INFO] Using complete glossary with {len(glossary_terms)} terms")
    else:
        # ä½¿ç”¨é¢„æ„å»ºç´¢å¼•æ—¶ï¼Œterm_listä¼šåœ¨evaluateå‡½æ•°ä¸­ä»ç´¢å¼•è·å–
        print(f"[INFO] Will load term list from pre-built index")
    
    sys.stdout.flush()

    # === ç®€è¦æ•°æ®é›†ç»Ÿè®¡ ===
    print(f"\n[INFO] Dataset statistics:")
    if args.glossary_emb_path and os.path.exists(args.glossary_emb_path):
        # ä½¿ç”¨é¢„æ„å»ºç´¢å¼•æ—¶çš„ç»Ÿè®¡
        print(f"[INFO] - Pre-built index terms: {retriever.index.ntotal}")
    else:
        # ä½¿ç”¨glossaryæ–‡ä»¶æ—¶çš„ç»Ÿè®¡
        print(f"[INFO] - Glossary terms: {len(glossary_terms)}")
    print(f"[INFO] - Training terms (for seen/unseen): {len(train_terms)}")
    sys.stdout.flush()

    # === æ‰§è¡Œå®Œæ•´è¯„ä¼° ===
    print("\n" + "="*50)
    print("FULL EVALUATION WITH COMPLETE GLOSSARY")
    print("="*50)
    sys.stdout.flush()

    recall_results = evaluate_topk_recall(
        model, retriever, test_dataset, device,
        top_ks=(1, 5, 10),
        max_eval=args.max_eval,
        train_terms=train_terms,
        show_missed_terms=True,
        glossary_emb_path=args.glossary_emb_path
    )

    # === ä¿å­˜è¯„ä¼°ç»“æœ ===
    results_path = args.model_path.replace('.pt', '_full_eval_results.json')
    eval_summary = {
        'model_path': args.model_path,
        'glossary_path': args.glossary_path,
        'glossary_emb_path': args.glossary_emb_path,
        'test_samples_path': args.test_samples_path,
        'train_samples_path': args.train_samples_path,
        'total_terms': retriever.index.ntotal if args.glossary_emb_path and os.path.exists(args.glossary_emb_path) else len(glossary_terms),
        'train_terms_count': len(train_terms),
        'test_samples': len(test_dataset),
        'evaluated_samples': min(args.max_eval, len(test_dataset)),
        'results': {}
    }

    for top_k in [1, 5, 10]:
        if top_k in recall_results and recall_results[top_k]:
            avg_recall = sum(recall_results[top_k]) / len(recall_results[top_k])
            eval_summary['results'][f'recall@{top_k}'] = float(avg_recall)

    with open(results_path, 'w') as f:
        json.dump(eval_summary, f, indent=2)

    print(f"\n[INFO] Evaluation results saved to {results_path}")
    print(f"[INFO] Full evaluation completed!")
    sys.stdout.flush()


if __name__ == "__main__":
    main()
