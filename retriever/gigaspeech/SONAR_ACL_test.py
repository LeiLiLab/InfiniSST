import torch
from torch.utils.data import DataLoader, Dataset
import numpy as np
import json
from tqdm import tqdm
import argparse, os, sys
import faiss
from new_retrieve import Retriever
import soundfile as sf

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sonar.inference_pipelines.speech import SpeechToEmbeddingModelPipeline
from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline

# å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„æ¨¡å‹ç±»
from SONAR_train import ContrastiveSpeechTextModel

# === New imports for offline asset building ===
import math
from pathlib import Path


def l2_normalize_numpy(x: np.ndarray) -> np.ndarray:
    """L2 normalize along last dim for numpy array."""
    norms = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12
    return x / norms


def save_offline_assets(
    embeddings: np.ndarray,
    terms: list,
    out_dir: str,
    index_name: str = "glossary_emb.ivfpq.faiss",
    term2idx_name: str = "glossary_term2idx.json",
    terms_txt_name: str = "glossary_terms.txt",
):
    """Save term->idx map and ordered term list for verification."""
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    term2idx = {t: i for i, t in enumerate(terms)}
    with open(os.path.join(out_dir, term2idx_name), "w") as f:
        json.dump(term2idx, f)
    with open(os.path.join(out_dir, terms_txt_name), "w") as f:
        for t in terms:
            f.write(t + "\n")
    print(f"[ASSET] Saved term2idx -> {os.path.join(out_dir, term2idx_name)}")
    print(f"[ASSET] Saved terms.txt -> {os.path.join(out_dir, terms_txt_name)}")


def build_ivfpq_index(
    xb: np.ndarray,
    use_ip: bool = True,
    nlist: int = 4096,
    pq_m: int = 64,
    pq_bits: int = 8,
    train_size: int = 1000000,
    nprobe: int = 16,
):
    """Build an IVF-PQ index in memory and return it.
    - xb should be L2-normalized if use_ip=True (cosine via inner product).
    """
    d = xb.shape[1]
    metric = faiss.METRIC_INNER_PRODUCT if use_ip else faiss.METRIC_L2

    # Coarse quantizer
    quantizer = faiss.IndexFlatIP(d) if metric == faiss.METRIC_INNER_PRODUCT else faiss.IndexFlatL2(d)

    # IVF-PQ
    index = faiss.IndexIVFPQ(quantizer, d, nlist, pq_m, pq_bits, metric)

    # Train
    train_size = min(train_size, xb.shape[0])
    print(f"[FAISS] Training IVF-PQ on {train_size} vectors (nlist={nlist}, m={pq_m}, bits={pq_bits})")
    faiss_idx_train = xb[np.random.choice(xb.shape[0], train_size, replace=False)]
    index.train(faiss_idx_train)

    # Add
    index.nprobe = nprobe
    print(f"[FAISS] Adding {xb.shape[0]} vectors to IVF-PQ (nprobe={nprobe})")
    index.add(xb)
    print(f"[FAISS] IVF-PQ ntotal={index.ntotal}")

    return index


def build_sharded_ivfpq_indices(
    xb: np.ndarray,
    terms: list,
    out_dir: str,
    shard_size: int = 2_000_000,
    use_ip: bool = True,
    nlist: int = 4096,
    pq_m: int = 64,
    pq_bits: int = 8,
    train_size: int = 1_000_000,
    nprobe: int = 16,
):
    """Split embeddings into shards and build multiple IVF-PQ indexes: glossary_shard_XX.faiss.
    Returns list of written index paths.
    """
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    N = xb.shape[0]
    d = xb.shape[1]
    n_shards = math.ceil(N / shard_size)
    paths = []
    print(f"[FAISS] Sharding {N} vectors into {n_shards} shards (sizeâ‰ˆ{shard_size})")
    for s in range(n_shards):
        s0 = s * shard_size
        s1 = min((s + 1) * shard_size, N)
        x_shard = xb[s0:s1]
        print(f"[FAISS] Building shard {s+1}/{n_shards} with {x_shard.shape[0]} vectors")
        index = build_ivfpq_index(
            x_shard, use_ip=use_ip, nlist=nlist, pq_m=pq_m, pq_bits=pq_bits, train_size=min(train_size, x_shard.shape[0]), nprobe=nprobe
        )
        shard_path = os.path.join(out_dir, f"glossary_shard_{s:02d}.faiss")
        faiss.write_index(index, shard_path)
        print(f"[ASSET] Shard written -> {shard_path}")
        del index
    return paths


def load_glossary_terms(glossary_path):
    """åŠ è½½å®Œæ•´çš„æœ¯è¯­è¡¨"""
    print(f"[INFO] Loading glossary from {glossary_path}")
    sys.stdout.flush()
    with open(glossary_path, "r") as f:
        glossary = json.load(f)
    
    # æå–æ‰€æœ‰æœ¯è¯­ï¼Œå¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
    terms = []
    if isinstance(glossary, list):
        for item in glossary:
            if isinstance(item, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å– 'term' æˆ– 'text' å­—æ®µ
                term = item.get('term') or item.get('text') or item.get('word')
                if term:
                    terms.append(term.lower())
            elif isinstance(item, str):
                terms.append(item.lower())
    elif isinstance(glossary, dict):
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–æ‰€æœ‰å€¼
        for key, value in glossary.items():
            if isinstance(value, str):
                terms.append(value.lower())
            elif isinstance(value, dict) and 'term' in value:
                terms.append(value['term'].lower())
    
    # å»é‡å¹¶è¿‡æ»¤
    terms = list(set(term for term in terms if term and len(term.strip()) >= 2))
    print(f"[INFO] Loaded {len(terms)} unique terms from glossary")
    sys.stdout.flush()
    return terms


def is_audio_valid(audio_path, min_duration=0.01, max_duration=30.0):
    """æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
    try:
        if not os.path.exists(audio_path):
            return False, "File does not exist"
        
        data, sr = sf.read(audio_path)
        
        # æ£€æŸ¥åŸºæœ¬å±æ€§
        if len(data) == 0:
            return False, "Empty audio file"
        
        duration = len(data) / sr
        if duration < min_duration:
            return False, f"Too short ({duration:.3f}s < {min_duration}s)"
        
        if duration > max_duration:
            return False, f"Too long ({duration:.3f}s > {max_duration}s)"
        
        # æ£€æŸ¥æ˜¯å¦å…¨é™éŸ³
        if np.allclose(data, 0, atol=1e-6):
            return False, "All silence"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if np.isnan(data).any():
            return False, "Contains NaN values"
        
        if np.isinf(data).any():
            return False, "Contains Inf values"
        
        # æ£€æŸ¥åŠ¨æ€èŒƒå›´
        data_std = np.std(data)
        if data_std < 1e-6:
            return False, f"Very low dynamic range (std={data_std:.2e})"
        
        return True, "Valid"
        
    except Exception as e:
        return False, f"Failed to read: {str(e)}"


def validate_audio_batch(audio_paths, verbose=False):
    """æ‰¹é‡éªŒè¯éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›æœ‰æ•ˆçš„è·¯å¾„åˆ—è¡¨å’Œå¯¹åº”çš„åŸå§‹ç´¢å¼•"""
    valid_paths = []
    valid_indices = []
    invalid_count = 0
    
    for i, path in enumerate(audio_paths):
        is_valid, reason = is_audio_valid(path)
        if is_valid:
            valid_paths.append(path)
            valid_indices.append(i)
        else:
            invalid_count += 1
            if verbose or invalid_count <= 5:  # åªæ‰“å°å‰5ä¸ªæ— æ•ˆæ–‡ä»¶
                print(f"[WARN] Invalid audio {i}: {path} - {reason}")
    
    if invalid_count > 5:
        print(f"[WARN] ... and {invalid_count - 5} more invalid audio files")
    
    return valid_paths, valid_indices


class TermLevelDataset(Dataset):
    def __init__(self, path=None, split="test", train_ratio=0.99, test_path=None):
        if split == "test" and test_path is not None:
            # ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®é›†
            print(f"[INFO] Loading test samples from separate file: {test_path}")
            with open(test_path, "r") as f:
                all_samples = json.load(f)
            # å¯¹äºç‹¬ç«‹æµ‹è¯•é›†ï¼Œä¸éœ€è¦train_ratioåˆ†å‰²ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ ·æœ¬
            use_split_logic = False
        else:
            # ä½¿ç”¨åŸæœ‰çš„åˆ†å‰²é€»è¾‘
            if path is None:
                raise ValueError("path must be provided when not using separate test file")
            print(f"[INFO] Loading term-level chunk samples from {path}")
            with open(path, "r") as f:
                all_samples = json.load(f)
            use_split_logic = True
        
        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ï¼šå¿…é¡»æœ‰éŸ³é¢‘æ–‡ä»¶ã€chunkæ–‡æœ¬å’Œground truth terms
        valid_samples = []
        invalid_audio_count = 0
        
        for i, s in enumerate(all_samples):
            terms = s.get('term_chunk_audio_ground_truth_terms')
            if not (terms and isinstance(terms, list)):
                continue
            # è¿‡æ»¤æœ¯è¯­
            filtered_terms = [
                t for t in terms
                if isinstance(t, str)
                and len(t) >= 3
                and sum(c.isdigit() for c in t) <= len(t) // 2
            ]
            if not filtered_terms:
                continue

            # è¿‡æ»¤å‰åç¼€
            black_words = ['yeah','this ']
            black_suffixes = ['years']
            filtered_terms = [
                t for t in filtered_terms 
                if not any(t.lower().startswith(prefix.lower()) for prefix in black_words)
                and not any(t.lower().endswith(suffix.lower()) for suffix in black_suffixes)
            ]
            
            # æ›¿æ¢åŸåˆ—è¡¨ä¸ºè¿‡æ»¤åçš„æœ¯è¯­
            s = dict(s)  # é¿å…ç›´æ¥ä¿®æ”¹åŸå§‹æ•°æ®
            s['term_chunk_audio_ground_truth_terms'] = filtered_terms
            
            # æ£€æŸ¥åŸºæœ¬æ¡ä»¶
            if not (s.get('term_chunk_text', '').strip() and s.get('term_chunk_audio', '')):
                continue
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æœ‰æ•ˆæ€§
            audio_path = s.get("term_chunk_audio", "")
            is_valid, reason = is_audio_valid(audio_path)
            
            if is_valid:
                valid_samples.append(s)
            else:
                invalid_audio_count += 1
                # åªæ‰“å°å‰10ä¸ªæ— æ•ˆéŸ³é¢‘çš„è¯¦ç»†ä¿¡æ¯
                if invalid_audio_count <= 10:
                    print(f"[WARN] Skipping sample {i}: {audio_path} - {reason}")
        
        if invalid_audio_count > 10:
            print(f"[WARN] ... and {invalid_audio_count - 10} more samples with invalid audio")
            
        print(f"[INFO] Audio validation: {len(valid_samples)} valid, {invalid_audio_count} invalid")
        
        print(f"[INFO] Filtered {len(valid_samples)} valid term-level samples from {len(all_samples)} total samples")
        
        if use_split_logic:
            # æ•°æ®åˆ†å‰²ï¼š99%è®­ç»ƒï¼Œ1%æµ‹è¯•
            import random
            random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
            random.shuffle(valid_samples)
            
            split_idx = int(len(valid_samples) * train_ratio)
            
            if split == "train":
                self.samples = valid_samples[:split_idx]
                print(f"[INFO] Training split: {len(self.samples)} term-level samples")
            elif split == "test":
                self.samples = valid_samples[split_idx:]
                print(f"[INFO] Test split: {len(self.samples)} term-level samples")
            else:
                raise ValueError(f"Invalid split: {split}. Must be 'train' or 'test'")
        else:
            # ç‹¬ç«‹æµ‹è¯•é›†ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
            self.samples = valid_samples
            print(f"[INFO] Using separate test dataset: {len(self.samples)} term-level samples")
        
        print(f"[INFO] Loaded {len(self.samples)} term-level samples for {split} split")

    def __getitem__(self, idx):
        sample = self.samples[idx]
        audio_path = sample["term_chunk_audio"]  # ä½¿ç”¨term chunkéŸ³é¢‘
        chunk_text = sample["term_chunk_text"]   # ä½¿ç”¨term chunkæ–‡æœ¬
        ground_truth_terms = sample.get('term_chunk_audio_ground_truth_terms', [])
        
        return ground_truth_terms, audio_path, chunk_text, True

    def __len__(self):
        return len(self.samples)


def extract_all_used_terms(dataset):
    """æå–æ•°æ®é›†ä¸­æ‰€æœ‰ä½¿ç”¨çš„æœ¯è¯­"""
    used_terms = set()
    processed_samples = 0
    valid_samples = 0
    
    for i, sample in enumerate(dataset):
        if sample is None:
            continue
        processed_samples += 1
        ground_truth_terms, audio_path, chunk_text, has_target = sample
        
        if has_target and ground_truth_terms:
            valid_samples += 1
            for t in ground_truth_terms:
                if isinstance(t, str) and len(t.strip()) > 0:
                    used_terms.add(t.lower())
            
            # è°ƒè¯•å‰å‡ ä¸ªæ ·æœ¬
            if i < 5:
                print(f"[DEBUG] extract_all_used_terms - Sample {i}: ground_truth_terms={ground_truth_terms}, chunk_text='{chunk_text}'")
    
    print(f"[DEBUG] extract_all_used_terms - Processed {processed_samples} samples, {valid_samples} valid samples, {len(used_terms)} unique terms")
    return list(used_terms)


def encode_texts_in_batches(model, texts, batch_size=512, device="cuda", auto_batch_size=True, max_chunk_size=1000000):
    """åˆ†æ‰¹ç¼–ç æ–‡æœ¬ï¼Œæ”¯æŒåŠ¨æ€batch_sizeå’Œåˆ†æ®µå¤„ç†"""
    print(f"[INFO] Text encoding setup:")
    print(f"[INFO] - Model type: {type(model)}")
    print(f"[INFO] - Device count: {torch.cuda.device_count()}")
    print(f"[INFO] - Model device: {next(model.parameters()).device if hasattr(model, 'parameters') else 'N/A'}")
    print(f"[INFO] - Initial batch_size: {batch_size}")
    print(f"[INFO] - Total texts: {len(texts)}")
    sys.stdout.flush()
    
    # å¯¹äºå¤§é‡æ–‡æœ¬ï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†
    if len(texts) > max_chunk_size:
        print(f"[INFO] ğŸ“Š Large dataset detected ({len(texts)} texts)")
        print(f"[INFO] ğŸ”„ Using chunked processing with max_chunk_size={max_chunk_size}")
        sys.stdout.flush()
        
        all_results = []
        num_chunks = (len(texts) + max_chunk_size - 1) // max_chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * max_chunk_size
            end_idx = min(start_idx + max_chunk_size, len(texts))
            chunk_texts = texts[start_idx:end_idx]
            
            print(f"[INFO] ğŸ“¦ Processing chunk {chunk_idx + 1}/{num_chunks} ({len(chunk_texts)} texts)")
            sys.stdout.flush()
            
            # é€’å½’è°ƒç”¨å¤„ç†å•ä¸ªchunkï¼ˆä¸ä¼šå†åˆ†æ®µï¼‰
            chunk_result = encode_texts_in_batches(
                model, chunk_texts, batch_size, device, auto_batch_size, max_chunk_size=float('inf')
            )
            all_results.append(chunk_result)
            
            # åŠæ—¶æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print(f"[INFO] âœ… Chunk {chunk_idx + 1}/{num_chunks} completed, shape: {chunk_result.shape}")
            sys.stdout.flush()
        
        # åˆå¹¶æ‰€æœ‰chunkçš„ç»“æœ
        print(f"[INFO] ğŸ”— Merging {len(all_results)} chunks...")
        sys.stdout.flush()
        final_result = torch.cat(all_results, dim=0)
        
        # æ¸…ç†ä¸­é—´ç»“æœ
        del all_results
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"[INFO] âœ… Chunked processing completed: {final_result.shape}")
        sys.stdout.flush()
        return final_result
    
    # åŠ¨æ€è°ƒæ•´batch_sizeåˆ°æ˜¾å­˜æé™
    if auto_batch_size and torch.cuda.is_available():
        print(f"[INFO] Auto-tuning batch size for optimal GPU memory usage...")
        sys.stdout.flush()
        try_bs = batch_size
        test_texts = texts[:min(try_bs * 4, len(texts))]  # ç”¨å°æ ·æœ¬æµ‹è¯•
        
        while try_bs >= 32:  # æœ€å°batch_size
            try:
                print(f"[DEBUG] Testing batch_size: {try_bs}")
                sys.stdout.flush()
                torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜
                
                with torch.no_grad():
                    test_batch = test_texts[:try_bs] if len(test_texts) >= try_bs else test_texts
                    _ = model.encode_text(test_batch)
                    torch.cuda.empty_cache()
                
                # æˆåŠŸäº†ï¼Œå°è¯•æ›´å¤§çš„batch_sizeï¼ˆä½†ä¸è¶…è¿‡åˆç†ä¸Šé™ï¼‰
                max_reasonable_bs = min(1024, batch_size * 2)  # è®¾ç½®åˆç†ä¸Šé™
                if try_bs < max_reasonable_bs:
                    try_bs = int(try_bs * 1.3)  # æ›´ä¿å®ˆçš„å¢é•¿
                else:
                    break
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) or "out of memory" in str(e):
                    try_bs = max(32, try_bs // 2)
                    print(f"[WARNING] OOM, reducing batch_size to {try_bs}")
                    sys.stdout.flush()
                    torch.cuda.empty_cache()
                else:
                    print(f"[ERROR] Unexpected error during batch size testing: {e}")
                    break
        
        # å†é€€ä¸€æ­¥ï¼Œç¡®ä¿ç¨³å®šï¼ˆæ›´ä¿å®ˆï¼‰ï¼Œå¹¶è®¾ç½®ç»å¯¹ä¸Šé™
        batch_size = max(32, min(1024, int(try_bs * 0.6)))
        print(f"[INFO] âœ… Optimized batch_size: {batch_size} (capped at 1024)")
        sys.stdout.flush()
    
    # æ‰¹é‡ç¼–ç 
    all_embeddings = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    print(f"[INFO] Encoding {len(texts)} texts in {total_batches} batches of size {batch_size}")
    sys.stdout.flush()
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        if batch_num % max(1, total_batches // 10) == 0 or batch_num <= 3:
            print(f"[INFO] Processing text batch {batch_num}/{total_batches}")
            sys.stdout.flush()
        
        with torch.no_grad():
            try:
                emb = model.encode_text(batch).cpu()
                all_embeddings.append(emb)
                
                # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ï¼Œé˜²æ­¢ç´¯ç§¯
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"[ERROR] Failed to encode text batch {batch_num}: {e}")
                sys.stdout.flush()
                # æ¸…ç†æ˜¾å­˜åé‡è¯•
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # å°è¯•æ›´å°çš„batch
                for j in range(0, len(batch), batch_size // 4):
                    mini_batch = batch[j:j + batch_size // 4]
                    try:
                        emb = model.encode_text(mini_batch).cpu()
                        all_embeddings.append(emb)
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except Exception as e2:
                        print(f"[ERROR] Failed mini-batch encoding: {e2}")
                        sys.stdout.flush()
                        continue
    
    if not all_embeddings:
        raise RuntimeError("No texts were successfully encoded")
    
    result = torch.cat(all_embeddings, dim=0)
    print(f"[INFO] âœ… Text encoding completed: {result.shape}")
    sys.stdout.flush()
    return result


def encode_audios_in_batches(model, audio_paths, batch_size=1000, device="cuda", auto_batch_size=True):
    """åˆ†æ‰¹ç¼–ç éŸ³é¢‘ï¼Œæ”¯æŒåŠ¨æ€batch_sizeä¼˜åŒ–"""
    print(f"[INFO] Audio encoding setup:")
    print(f"[INFO] - Model type: {type(model)}")
    print(f"[INFO] - Initial batch_size: {batch_size}")
    sys.stdout.flush()
    
    # åŠ¨æ€è°ƒæ•´audio batch_sizeï¼ˆéŸ³é¢‘ç¼–ç æ›´æ¶ˆè€—æ˜¾å­˜ï¼‰
    if auto_batch_size and torch.cuda.is_available() and len(audio_paths) > batch_size:
        print(f"[INFO] Auto-tuning audio batch size...")
        sys.stdout.flush()
        try_bs = batch_size
        test_paths = audio_paths[:min(try_bs * 2, len(audio_paths))]  # ç”¨å°æ ·æœ¬æµ‹è¯•
        
        while try_bs >= 4:  # éŸ³é¢‘æœ€å°batch_size
            try:
                print(f"[DEBUG] Testing audio batch_size: {try_bs}")
                sys.stdout.flush()
                torch.cuda.empty_cache()
                
                with torch.no_grad():
                    test_batch = test_paths[:try_bs] if len(test_paths) >= try_bs else test_paths
                    _ = model.encode_audio(test_batch)
                    torch.cuda.empty_cache()
                
                # æˆåŠŸäº†ï¼Œå°è¯•ç¨å¤§ä¸€ç‚¹çš„batch_sizeï¼ˆéŸ³é¢‘æ›´ä¿å®ˆï¼‰
                max_reasonable_bs = min(128, batch_size * 2)  # éŸ³é¢‘ä¸Šé™128
                if try_bs < max_reasonable_bs:
                    try_bs = int(try_bs * 1.2)  # æ›´ä¿å®ˆçš„å¢é•¿
                else:
                    break
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) or "out of memory" in str(e):
                    try_bs = max(4, try_bs // 2)
                    print(f"[WARNING] Audio OOM, reducing batch_size to {try_bs}")
                    sys.stdout.flush()
                    torch.cuda.empty_cache()
                else:
                    print(f"[ERROR] Unexpected error during audio batch size testing: {e}")
                    break
        
        batch_size = max(4, min(128, int(try_bs * 0.7)))  # ä¿å®ˆä¸€ç‚¹ï¼Œä¸Šé™128
        print(f"[INFO] âœ… Optimized audio batch_size: {batch_size} (capped at 128)")
        sys.stdout.flush()
    
    # æ‰¹é‡ç¼–ç 
    all_embeddings = []
    total_batches = (len(audio_paths) + batch_size - 1) // batch_size
    print(f"[INFO] Encoding {len(audio_paths)} audio files in {total_batches} batches of size {batch_size}")
    sys.stdout.flush()
    
    for i in range(0, len(audio_paths), batch_size):
        batch_paths = audio_paths[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        if batch_num % max(1, total_batches // 10) == 0 or batch_num <= 3:
            print(f"[INFO] Processing audio batch {batch_num}/{total_batches}")
            sys.stdout.flush()
        
        with torch.no_grad():
            try:
                emb = model.encode_audio(batch_paths).cpu()
                all_embeddings.append(emb)
            except Exception as e:
                print(f"[ERROR] Failed to encode audio batch {batch_num}: {e}")
                sys.stdout.flush()
                print(f"[INFO] Trying single file processing for this batch...")
                sys.stdout.flush()
                # å¦‚æœbatchå¤±è´¥ï¼Œå°è¯•å•ä¸ªå¤„ç†
                for single_path in batch_paths:
                    try:
                        single_emb = model.encode_audio([single_path]).cpu()
                        all_embeddings.append(single_emb)
                    except Exception as e2:
                        print(f"[ERROR] Failed to encode single audio {single_path}: {e2}")
                        sys.stdout.flush()
                        # è·³è¿‡è¿™ä¸ªéŸ³é¢‘æ–‡ä»¶
                        continue
    
    if not all_embeddings:
        raise RuntimeError("No audio files were successfully encoded")
    
    result = torch.cat(all_embeddings, dim=0)
    print(f"[INFO] âœ… Audio encoding completed: {result.shape}")
    sys.stdout.flush()
    return result


def evaluate_topk_recall(model, retriever, dataset, device, top_ks=(5, 10, 20), max_eval=1000, field="term", train_terms=None, show_missed_terms=True, glossary_emb_path=None):
    """è¯„ä¼°top-kå¬å›ç‡ï¼Œä½¿ç”¨sample-levelå¹³å‡ï¼ŒåŒæ—¶æ”¶é›†term-levelä¿¡æ¯ç”¨äºåˆ†æ"""
    model.eval()
    
    # ç”¨äºå­˜å‚¨sample-levelå¬å›ç‡
    recall_dict = {k: [] for k in top_ks}
    
    # ç”¨äºå­˜å‚¨æ‰€æœ‰GTæœ¯è¯­å’Œå¯¹åº”çš„æ£€ç´¢ç»“æœï¼ˆç”¨äºåˆ†ææœªå‘½ä¸­æœ¯è¯­ï¼‰
    all_gt_terms_with_retrieval = {k: [] for k in top_ks}  # æ¯ä¸ªå…ƒç´ æ˜¯ (gt_term, is_retrieved, sample_info)
    sample_info_for_debug = []  # ç”¨äºè°ƒè¯•è¾“å‡º

    # === æ„å»ºæˆ–åŠ è½½ç´¢å¼• ===
    if glossary_emb_path and os.path.exists(glossary_emb_path):
        # ç›´æ¥åŠ è½½é¢„æ„å»ºçš„ç´¢å¼•
        print(f'[INFO] Loading pre-built glossary index from {glossary_emb_path}')
        try:
            retriever.index = faiss.read_index(glossary_emb_path)
            print(f'[INFO] Successfully loaded index with {retriever.index.ntotal} vectors')
            
            # ä»ç´¢å¼•ä¸­è·å–termæ•°é‡ä¿¡æ¯ï¼Œç”¨äºç»Ÿè®¡
            index_size = retriever.index.ntotal
            print(f'[INFO] Pre-built index contains {index_size} terms')
            
            # å¦‚æœretriever.term_listä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªå ä½ç¬¦åˆ—è¡¨ç”¨äºè¯„ä¼°
            if not hasattr(retriever, 'term_list') or not retriever.term_list:
                retriever.term_list = [{'term': f'term_{i}'} for i in range(index_size)]
                print(f'[INFO] Created placeholder term list for evaluation')
        except Exception as e:
            print(f'[WARNING] Failed to load pre-built index: {e}, falling back to text encoding')
            glossary_emb_path = None
    
    if not glossary_emb_path or not os.path.exists(glossary_emb_path):
        # éœ€è¦é‡æ–°æ„å»ºç´¢å¼•
        text_terms = [term['term'] for term in retriever.term_list]
        print(f'[DEBUG] Building index with {len(text_terms)} terms')
        print(f'[DEBUG] First 10 terms: {text_terms[:10]}')
        print(f'[DEBUG] Last 10 terms: {text_terms[-10:]}')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤terms
        unique_terms = set(text_terms)
        print(f'[DEBUG] Unique terms: {len(unique_terms)} / {len(text_terms)}')
        if len(unique_terms) != len(text_terms):
            print(f'[WARNING] Found duplicate terms in retriever.term_list!')
        
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        text_emb = encode_texts_in_batches(raw_model, text_terms, device=device)
        
        # æ£€æŸ¥embeddingæ˜¯å¦éƒ½ç›¸åŒ
        if text_emb.shape[0] > 1:
            first_emb = text_emb[0:1]
            similarities = F.cosine_similarity(first_emb, text_emb, dim=1)
            identical_count = (similarities > 0.99).sum().item()
            print(f'[DEBUG] Embeddings identical to first: {identical_count} / {text_emb.shape[0]}')
            if identical_count > text_emb.shape[0] * 0.8:
                print(f'[ERROR] Most embeddings are identical! This will cause retrieval issues.')

        retriever.index.reset()
        retriever.index.add(text_emb)
        print(f'[DEBUG] Index built with {retriever.index.ntotal} vectors')

    print(f"[INFO] Dataset size: {len(dataset)}")
    import random
    random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
    eval_indices = random.sample(range(len(dataset)), min(max_eval, len(dataset)))
    valid_samples = []
    valid_indices = []
    
    for i in eval_indices:
        sample = dataset[i]
        if sample is not None and sample[3] and sample[0]:  # has_target=True and has ground_truth_terms
            valid_samples.append(sample)
            valid_indices.append(i)

    print(f"[INFO] Selected {len(eval_indices)} samples randomly, {len(valid_samples)} are valid for evaluation")
    print(f"[INFO] Filtered out {len(eval_indices) - len(valid_samples)} samples (no ground truth terms or has_target=False)")
    
    # ä½¿ç”¨term chunkéŸ³é¢‘è¿›è¡Œç¼–ç ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    audio_paths = [sample[1] for sample in valid_samples]  # term_chunk_audio paths
    
    # éªŒè¯éŸ³é¢‘æ–‡ä»¶
    print(f"[DEBUG] Validating {len(audio_paths)} audio files for evaluation...")
    valid_audio_paths, valid_audio_indices = validate_audio_batch(audio_paths, verbose=False)
    
    if len(valid_audio_paths) != len(audio_paths):
        print(f"[WARN] Evaluation: Only {len(valid_audio_paths)}/{len(audio_paths)} audio files are valid")
        # è¿‡æ»¤æ‰æ— æ•ˆçš„æ ·æœ¬
        valid_samples = [valid_samples[i] for i in valid_audio_indices]
        valid_indices = [valid_indices[i] for i in valid_audio_indices]
        audio_paths = valid_audio_paths
    
    if len(audio_paths) == 0:
        print(f"[ERROR] No valid audio files for evaluation!")
        return {k: [] for k in top_ks}
    
    print(f"[DEBUG] Encoding {len(audio_paths)} valid audio files...")
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    audio_embs = encode_audios_in_batches(raw_model, audio_paths, batch_size=1000, device=device).numpy()

    for j, (i, sample) in enumerate(zip(valid_indices, valid_samples)):
        ground_truth_terms, audio_path, chunk_text, has_target = sample
        audio_emb = audio_embs[j:j+1]  # shape: [1, 512]
        gt_terms = [t.lower() for t in ground_truth_terms]  # ä½¿ç”¨term_chunk_audio_ground_truth_terms

        # å¯¹æ¯ä¸ªtop_kè¿›è¡Œæ£€ç´¢
        retrieval_results = {}
        for top_k in top_ks:
            D, I = retriever.index.search(audio_emb, top_k)
            retrieved_terms = [retriever.term_list[idx][field].lower() for idx in I[0]]
            retrieval_results[top_k] = (D[0], I[0], retrieved_terms)
            
            # è®¡ç®—sample-levelå¬å›ç‡
            matched = sum(gt_term in retrieved_terms for gt_term in gt_terms)
            sample_recall = matched / len(gt_terms) if gt_terms else 0.0
            recall_dict[top_k].append(sample_recall)
            
            # åŒæ—¶æ”¶é›†term-levelä¿¡æ¯ç”¨äºåˆ†ææœªå‘½ä¸­æœ¯è¯­
            for gt_term in gt_terms:
                is_retrieved = gt_term in retrieved_terms
                sample_info = {
                    'sample_idx': i,
                    'audio_path': audio_path,
                    'chunk_text': chunk_text,
                    'all_gt_terms': gt_terms,
                    'retrieved_terms': retrieved_terms  # æ·»åŠ æ£€ç´¢åˆ°çš„å€™é€‰æœ¯è¯­
                }
                all_gt_terms_with_retrieval[top_k].append((gt_term, is_retrieved, sample_info))

        # å­˜å‚¨æ ·æœ¬ä¿¡æ¯ç”¨äºè°ƒè¯•ï¼ˆåªå­˜å‚¨ç¬¬ä¸€ä¸ªtop_kçš„ç»“æœï¼‰
        if j < 3:  # åªä¿å­˜å‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            first_top_k = top_ks[0]
            D, I, retrieved_terms = retrieval_results[first_top_k]
            matched = sum(gt_term in retrieved_terms for gt_term in gt_terms)
            sample_info_for_debug.append({
                'sample_idx': i,
                'audio_path': audio_path,
                'chunk_text': chunk_text,
                'gt_terms': gt_terms,
                'audio_emb': audio_emb,
                'retrieved_indices': I,
                'retrieved_distances': D,
                'retrieved_terms': retrieved_terms,
                'matched_count': matched,
                'total_gt_count': len(gt_terms)
            })

    # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆå‰3ä¸ªæ ·æœ¬ï¼‰
    for debug_info in sample_info_for_debug:
        print(f"[DEBUG] Sample {debug_info['sample_idx']}:")
        print(f"[DEBUG] Audio path: {debug_info['audio_path']}")
        print(f"[DEBUG] Chunk text: {debug_info['chunk_text']}")
        print(f"[DEBUG] GT terms: {debug_info['gt_terms']}")
        print(f"[DEBUG] Audio embedding stats: mean={debug_info['audio_emb'].mean():.4f}, std={debug_info['audio_emb'].std():.4f}")
        print(f"[DEBUG] Retrieved indices: {debug_info['retrieved_indices']}")
        print(f"[DEBUG] Retrieved distances: {debug_info['retrieved_distances']}")
        print(f"[DEBUG] Retrieved terms: {debug_info['retrieved_terms']}")
        print(f"[DEBUG] Match count: {debug_info['matched_count']}/{debug_info['total_gt_count']}")
        
        # é¢å¤–æ£€æŸ¥ï¼šçœ‹çœ‹è·ç¦»æœ€è¿‘çš„å‡ ä¸ªterms
        if len(debug_info['retrieved_distances']) > 0:
            print(f"[DEBUG] Closest term distance: {debug_info['retrieved_distances'][0]:.4f}")
            if len(set(debug_info['retrieved_terms'])) == 1:
                print(f"[ERROR] All retrieved terms are identical: '{debug_info['retrieved_terms'][0]}'")
        print(f"[DEBUG] ---")

    # è®¡ç®—sample-levelå’Œterm-levelå¬å›ç‡
    for top_k in top_ks:
        # Sample-levelå¹³å‡å¬å›ç‡
        avg_recall = sum(recall_dict[top_k]) / len(recall_dict[top_k]) if recall_dict[top_k] else 0.0
        print(f"[EVAL] Sample-level Average Recall@{top_k}: {avg_recall:.2%}")
        
        # Term-levelå¾®å¹³å‡å¬å›ç‡
        term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
        total_terms = len(term_retrieval_pairs)
        hit_terms = sum(1 for _, is_retrieved, _ in term_retrieval_pairs if is_retrieved)
        term_micro_avg_recall = hit_terms / total_terms if total_terms > 0 else 0.0
        print(f"[EVAL] Term-level Micro-Average Recall@{top_k}: {term_micro_avg_recall:.2%} ({hit_terms}/{total_terms} terms)")
        
        # è®¡ç®—å·®å¼‚
        diff = avg_recall - term_micro_avg_recall
        if diff > 0:
            print(f"[EVAL] Multi-term sample penalty: -{diff:.2%} (sample-level higher, indicating multi-term samples hurt overall recall)")
        elif diff < 0:
            print(f"[EVAL] Multi-term sample benefit: +{abs(diff):.2%} (term-level higher, indicating multi-term samples help overall recall)")
        else:
            print(f"[EVAL] No difference between sample-level and term-level recall")
        print()
        
    # === ç»Ÿè®¡å’Œæ‰“å°æœªå‘½ä¸­çš„æœ¯è¯­ ===
    if show_missed_terms:
        for top_k in top_ks:
            term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
            missed_terms_info = []
            for gt_term, is_retrieved, sample_info in term_retrieval_pairs:
                if not is_retrieved:
                    missed_terms_info.append((gt_term, sample_info))
            
            print(f"[EVAL] Missed {len(missed_terms_info)} terms for Recall@{top_k}:")
            
            # æŒ‰æœ¯è¯­åˆ†ç»„ç»Ÿè®¡
            missed_terms_count = {}
            for gt_term, sample_info in missed_terms_info:
                if gt_term not in missed_terms_count:
                    missed_terms_count[gt_term] = []
                missed_terms_count[gt_term].append(sample_info)
            
            # æ‰“å°æœªå‘½ä¸­æœ¯è¯­çš„è¯¦ç»†ä¿¡æ¯ï¼ˆé™åˆ¶è¾“å‡ºæ•°é‡ï¼‰
            max_terms_to_show = 20  # æœ€å¤šæ˜¾ç¤º20ä¸ªæœ¯è¯­
            sorted_missed_terms = sorted(missed_terms_count.items(), key=lambda x: len(x[1]), reverse=True)
            
            for i, (missed_term, sample_infos) in enumerate(sorted_missed_terms):
                if i >= max_terms_to_show:
                    remaining_terms = len(sorted_missed_terms) - max_terms_to_show
                    print(f"[EVAL]   ... and {remaining_terms} more missed terms")
                    break
                    
                print(f"[EVAL]   '{missed_term}' (missed {len(sample_infos)} times):")
                
                # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                max_samples_to_show = 3
                for j, sample_info in enumerate(sample_infos):
                    if j >= max_samples_to_show:
                        remaining_samples = len(sample_infos) - max_samples_to_show
                        print(f"[EVAL]     ... and {remaining_samples} more samples")
                        break
                        
                    chunk_text_preview = sample_info['chunk_text'][:100] + '...' if len(sample_info['chunk_text']) > 100 else sample_info['chunk_text']
                    audio_basename = sample_info['audio_path'].split('/')[-1] if sample_info['audio_path'] else 'N/A'
                    print(f"[EVAL]     Sample {sample_info['sample_idx']}: {audio_basename}")
                    print(f"[EVAL]       Text: {chunk_text_preview}")
                    print(f"[EVAL]       All GT terms: {sample_info['all_gt_terms']}")
                    print(f"[EVAL]       Retrieved top-{top_k}: {sample_info['retrieved_terms']}")
            
            print()  # ç©ºè¡Œåˆ†éš”
    else:
        for top_k in top_ks:
            term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
            missed_count = sum(1 for _, is_retrieved, _ in term_retrieval_pairs if not is_retrieved)
            print(f"[EVAL] Missed {missed_count} terms for Recall@{top_k} (details hidden)")
        print()

    # === è®¡ç®— seen/unseen recall (both sample-level and term-level) ===
    if train_terms is not None:
        for top_k in top_ks:
            # åªæœ‰è®­ç»ƒé›†ä¸­çš„æœ¯è¯­æ‰ç®—seen
            seen_terms_set = set(t.lower() for t in train_terms)
            
            # Sample-level seen/unseenåˆ†æ
            seen_recalls, unseen_recalls = [], []
            for recall_val, sample in zip(recall_dict[top_k], valid_samples):
                gt_terms = [t.lower() for t in sample[0]]
                # ä¿®æ­£é€»è¾‘ï¼šå¦‚æœæ ·æœ¬ä¸­æœ‰ä»»ä½•æœ¯è¯­åœ¨è®­ç»ƒé›†ä¸­ï¼Œåˆ™è¯¥æ ·æœ¬å½’ç±»ä¸ºseen
                # è¿™æ ·å¯ä»¥æ›´å¥½åœ°åŒºåˆ†seenå’Œunseenæ ·æœ¬ï¼Œé¿å…è¿‡äºä¸¥æ ¼çš„åˆ†ç±»
                if any(gt in seen_terms_set for gt in gt_terms):
                    seen_recalls.append(recall_val)
                else:
                    unseen_recalls.append(recall_val)

            avg_seen = sum(seen_recalls) / len(seen_recalls) if seen_recalls else 0.0
            avg_unseen = sum(unseen_recalls) / len(unseen_recalls) if unseen_recalls else 0.0
            total_samples = len(seen_recalls) + len(unseen_recalls)
            print(f"[EVAL] Sample-level - Seen Recall@{top_k}: {avg_seen:.2%} ({len(seen_recalls)}/{total_samples} samples), Unseen Recall@{top_k}: {avg_unseen:.2%} ({len(unseen_recalls)}/{total_samples} samples)")
            
            # Term-level seen/unseenåˆ†æ
            term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
            seen_hits, seen_total = 0, 0
            unseen_hits, unseen_total = 0, 0
            
            for gt_term, is_retrieved, sample_info in term_retrieval_pairs:
                if gt_term in seen_terms_set:
                    seen_total += 1
                    if is_retrieved:
                        seen_hits += 1
                else:
                    unseen_total += 1
                    if is_retrieved:
                        unseen_hits += 1
            
            term_seen_recall = seen_hits / seen_total if seen_total > 0 else 0.0
            term_unseen_recall = unseen_hits / unseen_total if unseen_total > 0 else 0.0
            total_terms = seen_total + unseen_total
            unseen_percentage = unseen_total / total_terms * 100 if total_terms > 0 else 0.0
            
            print(f"[EVAL] Term-level - Seen Recall@{top_k}: {term_seen_recall:.2%} ({seen_hits}/{seen_total} terms), Unseen Recall@{top_k}: {term_unseen_recall:.2%} ({unseen_hits}/{unseen_total} terms)")
            print(f"[EVAL] Unseen Term Percentage: {unseen_percentage:.1f}%")
            print()
    else:
        print(f"[WARN] train_terms not provided, skipping seen/unseen analysis")

    model.train()
    return recall_dict


def load_model(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"[INFO] Loading model from {model_path}")
    sys.stdout.flush()
    
    # ç¡®ä¿deviceæ˜¯torch.deviceå¯¹è±¡
    if isinstance(device, str):
        device = torch.device(device)
    
    # åˆå§‹åŒ–ç¼–ç å™¨
    try:
        speech_encoder = SpeechToEmbeddingModelPipeline(
            encoder="sonar_speech_encoder_eng", device=device
        )
    except Exception as e:
        print(f"[ERROR] Failed to initialize speech encoder: {e}")
        sys.stdout.flush()
        print(f"[INFO] Trying alternative initialization...")
        sys.stdout.flush()
        # å°è¯•ä¸ä¼ é€’deviceå‚æ•°
        speech_encoder = SpeechToEmbeddingModelPipeline(
            encoder="sonar_speech_encoder_eng"
        )
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if hasattr(speech_encoder, 'model'):
            speech_encoder.model = speech_encoder.model.to(device)

    try:
        text_encoder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            device=device,
            dtype=torch.float32,
        )
    except Exception as e:
        print(f"[ERROR] Failed to initialize text encoder: {e}")
        sys.stdout.flush()
        print(f"[INFO] Trying alternative initialization...")
        sys.stdout.flush()
        # å°è¯•ä¸ä¼ é€’deviceå‚æ•°
        text_encoder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            dtype=torch.float32,
        )
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if hasattr(text_encoder, 'model'):
            text_encoder.model = text_encoder.model.to(device)

    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œå› ä¸ºç»“æ„éœ€è¦åŒ¹é…ï¼‰
    model = ContrastiveSpeechTextModel(
        speech_encoder, text_encoder, 
        unfreeze_layers=10  # è¿™ä¸ªå‚æ•°ä¸å½±å“æ¨ç†ï¼Œåªå½±å“è®­ç»ƒæ—¶çš„å‚æ•°å†»ç»“
    ).to(device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„å‚æ•°
    state_dict = torch.load(model_path, map_location=device)
    
    # å¤„ç† DataParallel çš„æƒ…å†µ
    if list(state_dict.keys())[0].startswith('module.'):
        # ç§»é™¤ 'module.' å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            new_state_dict[k[7:]] = v  # ç§»é™¤ 'module.' (7ä¸ªå­—ç¬¦)
        state_dict = new_state_dict
    
    model.load_state_dict(state_dict)
    model.eval()
    print(f"[INFO] Model loaded successfully")
    sys.stdout.flush()
    
    # è‡ªåŠ¨å¤šGPUåŒ…è£…
    if torch.cuda.device_count() > 1:
        print(f"[INFO] ğŸš€ Detected {torch.cuda.device_count()} GPUs, wrapping with DataParallel")
        available_gpus = list(range(torch.cuda.device_count()))
        model = torch.nn.DataParallel(model, device_ids=available_gpus)
        print(f"[INFO] âœ… DataParallel enabled on GPUs: {available_gpus}")
        sys.stdout.flush()
    else:
        print(f"[INFO] Single GPU mode: {device}")
        sys.stdout.flush()
    
    return model


def load_acl_terminology(glossary_csv_path):
    """ä»ACLæœ¯è¯­è¯æ±‡è¡¨ä¸­åŠ è½½è‹±æ–‡æœ¯è¯­"""
    print(f"[INFO] Loading ACL terminology from {glossary_csv_path}")
    sys.stdout.flush()
    
    terms = []
    with open(glossary_csv_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        if len(lines) > 1:
            # è·³è¿‡headerè¡Œï¼Œç¬¬ä¸€åˆ—æ˜¯è‹±æ–‡æœ¯è¯­
            for line in lines[1:]:
                parts = line.strip().split(',')
                if len(parts) > 0 and parts[0]:
                    english_term = parts[0].strip().lower()
                    if len(english_term) >= 2:
                        terms.append(english_term)
    
    # å»é‡
    terms = list(set(terms))
    print(f"[INFO] Loaded {len(terms)} unique English terms from ACL glossary")
    sys.stdout.flush()
    return terms


def parse_acl_tagged_text(tagged_text_path):
    """è§£æACLæ ‡æ³¨çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæå–æœ¯è¯­"""
    print(f"[INFO] Parsing ACL tagged text from {tagged_text_path}")
    sys.stdout.flush()
    
    terms = set()
    with open(tagged_text_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            
            # æå–æ–¹æ‹¬å·ä¸­çš„æœ¯è¯­ [term]
            import re
            tagged_terms = re.findall(r'\[([^\]]+)\]', line)
            for term in tagged_terms:
                term = term.strip().lower()
                if len(term) >= 2:
                    terms.add(term)
    
    terms_list = list(terms)
    print(f"[INFO] Extracted {len(terms_list)} unique terms from {tagged_text_path}")
    sys.stdout.flush()
    return terms_list


class ACLDataset(Dataset):
    """ACLæ•°æ®é›†ç±»ï¼Œç”¨äºåŠ è½½ACL segmentedéŸ³é¢‘å’Œå¯¹åº”çš„æœ¯è¯­"""
    
    def __init__(self, acl_root_dir, split="dev", segmentation="gold"):
        """
        Args:
            acl_root_dir: ACLæ•°æ®é›†æ ¹ç›®å½• (å¦‚ data/acl-6060/2/acl_6060)
            split: "dev" æˆ– "eval"
            segmentation: "gold" æˆ– "shas"
        """
        self.acl_root_dir = acl_root_dir
        self.split = split
        self.segmentation = segmentation
        
        # æ„å»ºè·¯å¾„
        self.audio_dir = os.path.join(acl_root_dir, split, "segmented_wavs", segmentation)
        self.text_dir = os.path.join(acl_root_dir, split, "text", "tagged_terminology")
        
        print(f"[INFO] Loading ACL {split} dataset with {segmentation} segmentation")
        print(f"[INFO] Audio dir: {self.audio_dir}")
        print(f"[INFO] Text dir: {self.text_dir}")
        
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
        self.audio_files = []
        if os.path.exists(self.audio_dir):
            for f in os.listdir(self.audio_dir):
                if f.endswith('.wav'):
                    self.audio_files.append(f)
        
        self.audio_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
        
        # åŠ è½½å¯¹åº”çš„æœ¯è¯­æ ‡æ³¨æ–‡ä»¶ (ä½¿ç”¨è‹±æ–‡ç‰ˆæœ¬)
        tagged_file = os.path.join(self.text_dir, f"ACL.6060.{split}.tagged.en-xx.en.txt")
        self.tagged_terms = []
        self.sentence_to_terms = {}  # å¥å­IDåˆ°æœ¯è¯­çš„æ˜ å°„
        
        if os.path.exists(tagged_file):
            with open(tagged_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for i, line in enumerate(lines):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # æå–æ–¹æ‹¬å·ä¸­çš„æœ¯è¯­
                    import re
                    tagged_terms = re.findall(r'\[([^\]]+)\]', line)
                    clean_terms = [term.strip().lower() for term in tagged_terms if len(term.strip()) >= 2]
                    
                    # å¥å­IDç›´æ¥ä»è¡Œå·+1å¼€å§‹ï¼ˆå¯¹åº”sent_1.wav, sent_2.wav, ...ï¼‰
                    sent_id = i + 1
                    if clean_terms:
                        self.sentence_to_terms[sent_id] = clean_terms
        
        print(f"[INFO] Loaded {len(self.sentence_to_terms)} sentences with terms from tagged file")
        
        # è¿‡æ»¤å‡ºæœ‰æœ¯è¯­æ ‡æ³¨çš„éŸ³é¢‘æ–‡ä»¶
        self.valid_samples = []
        for audio_file in self.audio_files:
            # ä»æ–‡ä»¶åæå–å¥å­ID (å¦‚ sent_1.wav -> 1, sent_401.wav -> 401)
            import re
            match = re.search(r'sent_(\d+)\.wav', audio_file)
            if match:
                sent_id = int(match.group(1))
                if sent_id in self.sentence_to_terms:
                    audio_path = os.path.join(self.audio_dir, audio_file)
                    terms = self.sentence_to_terms[sent_id]
                    
                    # éªŒè¯éŸ³é¢‘æ–‡ä»¶
                    is_valid, reason = is_audio_valid(audio_path)
                    if is_valid:
                        self.valid_samples.append({
                            'audio_path': audio_path,
                            'terms': terms,
                            'sent_id': sent_id
                        })
                    else:
                        print(f"[WARN] Invalid audio {audio_path}: {reason}")
                else:
                    # å¯¹äºæ²¡æœ‰æœ¯è¯­æ ‡æ³¨çš„å¥å­ï¼Œæˆ‘ä»¬è·³è¿‡
                    pass
        
        print(f"[INFO] ACL {split} dataset: {len(self.valid_samples)} valid samples with terms")
        print(f"[INFO] Sentence ID range: {min(s['sent_id'] for s in self.valid_samples)} - {max(s['sent_id'] for s in self.valid_samples)}")
        sys.stdout.flush()
    
    def __len__(self):
        return len(self.valid_samples)
    
    def __getitem__(self, idx):
        sample = self.valid_samples[idx]
        return sample['terms'], sample['audio_path'], f"sent_{sample['sent_id']}", True


def main():
    parser = argparse.ArgumentParser(description="Full evaluation with complete glossary or ACL dataset")
    parser.add_argument('--model_path', type=str, required=True,
                       help="Path to trained model (.pt file)")
    
    # === ACL evaluation mode ===
    parser.add_argument('--acl_mode', action='store_true',
                       help="Enable ACL evaluation mode")
    parser.add_argument('--acl_root_dir', type=str, 
                       default="data/acl-6060/2/acl_6060",
                       help="Path to ACL dataset root directory")
    parser.add_argument('--acl_glossary_path', type=str,
                       default="data/acl-6060/2/intermediate_files/terminology_glossary.csv",
                       help="Path to ACL terminology glossary CSV file")
    parser.add_argument('--acl_test_split', type=str, default="eval", choices=["dev", "eval"],
                       help="ACL split to use for testing")
    parser.add_argument('--acl_index_split', type=str, default="dev", choices=["dev", "eval"],
                       help="ACL split to use for building index (terminology source)")
    parser.add_argument('--acl_segmentation', type=str, default="gold", choices=["gold", "shas"],
                       help="ACL segmentation type to use")
    
    # === Original evaluation mode ===
    parser.add_argument('--test_samples_path', type=str, 
                       default="data/xl_term_level_chunks_merged.json",
                       help="Path to test samples (for non-ACL mode)")
    parser.add_argument('--glossary_path', type=str, 
                       default="data/terms/glossary_filtered.json",
                       help="Path to complete glossary file (for non-ACL mode)")
    parser.add_argument('--glossary_emb_path', type=str, default=None,
                       help="Path to pre-built glossary embedding index (.faiss file). If provided, will skip text encoding")
    parser.add_argument('--train_samples_path', type=str,
                       default="data/samples/xl/term_level_chunks_single_0_500000.json",
                       help="Path to training samples for seen/unseen analysis (for non-ACL mode)")
    
    parser.add_argument('--max_eval', type=int, default=1000,
                       help="Maximum number of samples to evaluate")
    parser.add_argument('--batch_size', type=int, default=512,
                       help="Initial batch size for text encoding (will be auto-optimized, max 1024)")
    parser.add_argument('--audio_batch_size', type=int, default=1000,
                       help="Initial batch size for audio encoding (will be auto-optimized, max 128)")

    # === Offline asset building args ===
    parser.add_argument('--build_offline_assets', action='store_true',
                       help='If set, build and save offline glossary assets (embeddings + FAISS index) and exit')
    parser.add_argument('--asset_out_dir', type=str, default='data',
                       help='Output directory for offline assets')
    parser.add_argument('--index_type', type=str, default='ivfpq', choices=['ivfpq', 'flat'],
                       help='Index type to build for offline assets')
    parser.add_argument('--use_ip', action='store_true',
                       help='Use inner-product metric (cosine if vectors are L2-normalized). Default false => L2')
    parser.add_argument('--nlist', type=int, default=4096, help='IVF nlist (coarse clusters)')
    parser.add_argument('--pq_m', type=int, default=64, help='PQ m (number of subvectors)')
    parser.add_argument('--pq_bits', type=int, default=8, help='PQ bits per subvector')
    parser.add_argument('--nprobe', type=int, default=16, help='nprobe for IVF search/add sanity')
    parser.add_argument('--shard_size', type=int, default=0,
                       help='If >0, build multiple sharded indices of at most this many vectors per shard')

    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[INFO] Using device: {device}")
    sys.stdout.flush()

    # === åŠ è½½æ¨¡å‹ ===
    device = torch.device(device)  # è½¬æ¢ä¸ºtorch.deviceå¯¹è±¡
    model = load_model(args.model_path, device)

    # === ACL evaluation mode ===
    if args.acl_mode:
        print("\n" + "="*50)
        print("ACL EVALUATION MODE")
        print("="*50)
        sys.stdout.flush()
        
        # 1. ä»ACL glossaryæˆ–dev setæ„å»ºæœ¯è¯­ç´¢å¼•
        print(f"[INFO] Building index from ACL {args.acl_index_split} split")
        
        # æ–¹æ³•1: ä½¿ç”¨ACLæœ¯è¯­è¯æ±‡è¡¨
        if os.path.exists(args.acl_glossary_path):
            print(f"[INFO] Using ACL terminology glossary: {args.acl_glossary_path}")
            index_terms = load_acl_terminology(args.acl_glossary_path)
        else:
            # æ–¹æ³•2: ä»dev setçš„taggedæ–‡æœ¬ä¸­æå–æœ¯è¯­
            print(f"[INFO] Extracting terms from ACL {args.acl_index_split} tagged text")
            tagged_file = os.path.join(args.acl_root_dir, args.acl_index_split, "text", "tagged_terminology", f"ACL.6060.{args.acl_index_split}.tagged.en-xx.en.txt")
            if os.path.exists(tagged_file):
                index_terms = parse_acl_tagged_text(tagged_file)
            else:
                raise FileNotFoundError(f"ACL tagged text file not found: {tagged_file}")
        
        print(f"[INFO] Building index with {len(index_terms)} ACL terms")
        
        # 2. åŠ è½½ACLæµ‹è¯•æ•°æ®é›†
        test_dataset = ACLDataset(
            acl_root_dir=args.acl_root_dir,
            split=args.acl_test_split,
            segmentation=args.acl_segmentation
        )
        
        # 3. åˆå§‹åŒ–æ£€ç´¢å™¨
        retriever = Retriever(enable_fusion=True, device=device)
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        retriever.model = raw_model
        retriever.index = faiss.IndexFlatL2(512)
        retriever.term_list = [{'term': t} for t in index_terms]
        
        # 4. æ‰§è¡ŒACLè¯„ä¼°
        print(f"\n[INFO] ACL Evaluation Setup:")
        print(f"[INFO] - Index split: {args.acl_index_split} ({len(index_terms)} terms)")
        print(f"[INFO] - Test split: {args.acl_test_split} ({len(test_dataset)} samples)")
        print(f"[INFO] - Segmentation: {args.acl_segmentation}")
        sys.stdout.flush()
        
        recall_results = evaluate_topk_recall(
            model, retriever, test_dataset, device,
            top_ks=(1, 5, 10),
            max_eval=min(args.max_eval, len(test_dataset)),
            train_terms=None,  # ACLæ¨¡å¼ä¸‹ä¸åšseen/unseenåˆ†æ
            show_missed_terms=True,
            glossary_emb_path=None  # ACLæ¨¡å¼ä¸‹ä¸ä½¿ç”¨é¢„æ„å»ºç´¢å¼•
        )
        
        # 5. ä¿å­˜ACLè¯„ä¼°ç»“æœ
        results_path = args.model_path.replace('.pt', f'_acl_{args.acl_test_split}_eval_results.json')
        eval_summary = {
            'model_path': args.model_path,
            'acl_mode': True,
            'acl_root_dir': args.acl_root_dir,
            'acl_glossary_path': args.acl_glossary_path,
            'acl_test_split': args.acl_test_split,
            'acl_index_split': args.acl_index_split,
            'acl_segmentation': args.acl_segmentation,
            'index_terms_count': len(index_terms),
            'test_samples': len(test_dataset),
            'evaluated_samples': min(args.max_eval, len(test_dataset)),
            'results': {}
        }
        
        for top_k in [1, 5, 10]:
            if top_k in recall_results and recall_results[top_k]:
                avg_recall = sum(recall_results[top_k]) / len(recall_results[top_k])
                eval_summary['results'][f'recall@{top_k}'] = float(avg_recall)
        
        with open(results_path, 'w') as f:
            json.dump(eval_summary, f, indent=2)
        
        print(f"\n[INFO] ACL evaluation results saved to {results_path}")
        print(f"[INFO] ACL evaluation completed!")
        sys.stdout.flush()
        return

    # === Offline asset building path ===
    if args.build_offline_assets:
        print("[ASSET] Building offline glossary assets...")
        sys.stdout.flush()

        # 1) Load glossary
        glossary_terms = load_glossary_terms(args.glossary_path)
        print(f"[ASSET] Total glossary terms: {len(glossary_terms)}")
        sys.stdout.flush()

        # 2) Encode texts using the SAME text encoder as training
        raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
        text_emb_t = encode_texts_in_batches(raw_model, glossary_terms, batch_size=args.batch_size, device=device)
        text_emb = text_emb_t.cpu().numpy()

        # 3) L2 normalize if we plan to use IP (cosine via inner product)
        if args.use_ip:
            text_emb = l2_normalize_numpy(text_emb)
            print("[ASSET] L2-normalized embeddings for IP/cosine metric.")
        else:
            print("[ASSET] Using L2 metric; skipping normalization requirement.")
        sys.stdout.flush()

        # 4) Save maps (term2idx, terms.txt)
        save_offline_assets(text_emb, glossary_terms, args.asset_out_dir)

        # 5) Build FAISS index (ivfpq or flat) â€” single or sharded
        index_paths = []
        if args.index_type == 'flat':
            # Flat index (primarily for debugging)
            d = text_emb.shape[1]
            if args.use_ip:
                index = faiss.IndexFlatIP(d)
            else:
                index = faiss.IndexFlatL2(d)
            index.add(text_emb)
            out_path = os.path.join(args.asset_out_dir, 'glossary_emb.flat.faiss')
            faiss.write_index(index, out_path)
            print(f"[ASSET] Wrote flat index -> {out_path} (ntotal={index.ntotal})")
            del index
            index_paths.append(out_path)
        else:
            # ivfpq path (recommended)
            if args.shard_size and args.shard_size > 0:
                index_paths = build_sharded_ivfpq_indices(
                    text_emb, glossary_terms, args.asset_out_dir,
                    shard_size=args.shard_size, use_ip=args.use_ip,
                    nlist=args.nlist, pq_m=args.pq_m, pq_bits=args.pq_bits, train_size=min(args.nlist * 100, text_emb.shape[0]), nprobe=args.nprobe
                )
                print(f"[ASSET] Built {len(index_paths)} sharded indices under {args.asset_out_dir}")
            else:
                index = build_ivfpq_index(
                    text_emb, use_ip=args.use_ip, nlist=args.nlist, pq_m=args.pq_m, pq_bits=args.pq_bits, train_size=min(args.nlist * 100, text_emb.shape[0]), nprobe=args.nprobe
                )
                out_path = os.path.join(args.asset_out_dir, 'glossary_emb.ivfpq.faiss')
                faiss.write_index(index, out_path)
                print(f"[ASSET] Wrote IVF-PQ index -> {out_path} (ntotal={index.ntotal})")
                del index
                index_paths.append(out_path)

        print("[ASSET] Offline asset building finished.")
        sys.stdout.flush()
        return

    # === åŸæœ‰çš„æ­£å¸¸è¯„ä¼°æ¨¡å¼ ===
    # === åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆç‹¬ç«‹æ–‡ä»¶ï¼Œä¸æŒ‰æ¯”ä¾‹åˆ‡åˆ†ï¼‰ ===
    print(f"[INFO] Loading test dataset from {args.test_samples_path}")
    sys.stdout.flush()
    test_dataset = TermLevelDataset(
        None,
        split="test",
        test_path=args.test_samples_path
    )
    print(f"[INFO] Test dataset size: {len(test_dataset)}")
    sys.stdout.flush()

    # === åŠ è½½è®­ç»ƒæ ·æœ¬ç”¨äº seen/unseen ç»Ÿè®¡ï¼ˆä½¿ç”¨å…¨éƒ¨æ ·æœ¬ï¼‰ ===
    print(f"[INFO] Loading training samples for seen/unseen from {args.train_samples_path}")
    sys.stdout.flush()
    train_dataset = TermLevelDataset(
        args.train_samples_path,
        split="train",
        train_ratio=1.0
    )
    train_terms = extract_all_used_terms(train_dataset)
    print(f"[INFO] Training terms collected: {len(train_terms)}")
    sys.stdout.flush()

    # === åŠ è½½å®Œæ•´æœ¯è¯­è¡¨å¹¶åˆå§‹åŒ–æ£€ç´¢å™¨ ===
    if args.glossary_emb_path and os.path.exists(args.glossary_emb_path):
        # ä½¿ç”¨é¢„æ„å»ºç´¢å¼•ï¼Œä¸éœ€è¦åŠ è½½glossary terms
        print(f"[INFO] Using pre-built glossary index: {args.glossary_emb_path}")
        glossary_terms = []  # ç©ºåˆ—è¡¨ï¼Œå› ä¸ºç´¢å¼•ä¸­å·²åŒ…å«æ‰€æœ‰ä¿¡æ¯
    else:
        # éœ€è¦åŠ è½½glossary termsæ¥æ„å»ºç´¢å¼•
        glossary_terms = load_glossary_terms(args.glossary_path)
        print(f"[INFO] Loaded {len(glossary_terms)} terms from glossary")
    
    retriever = Retriever(enable_fusion=True, device=device)
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    retriever.model = raw_model
    retriever.index = faiss.IndexFlatL2(512)
    
    if not args.glossary_emb_path or not os.path.exists(args.glossary_emb_path):
        # åªæœ‰åœ¨æ²¡æœ‰é¢„æ„å»ºç´¢å¼•æ—¶æ‰è®¾ç½®term_list
        retriever.term_list = [{'term': t} for t in glossary_terms]
        print(f"[INFO] Using complete glossary with {len(glossary_terms)} terms")
    else:
        # ä½¿ç”¨é¢„æ„å»ºç´¢å¼•æ—¶ï¼Œterm_listä¼šåœ¨evaluateå‡½æ•°ä¸­ä»ç´¢å¼•è·å–
        print(f"[INFO] Will load term list from pre-built index")
    
    sys.stdout.flush()

    # === ç®€è¦æ•°æ®é›†ç»Ÿè®¡ ===
    print(f"\n[INFO] Dataset statistics:")
    if args.glossary_emb_path and os.path.exists(args.glossary_emb_path):
        # ä½¿ç”¨é¢„æ„å»ºç´¢å¼•æ—¶çš„ç»Ÿè®¡
        print(f"[INFO] - Pre-built index terms: {retriever.index.ntotal}")
    else:
        # ä½¿ç”¨glossaryæ–‡ä»¶æ—¶çš„ç»Ÿè®¡
        print(f"[INFO] - Glossary terms: {len(glossary_terms)}")
    print(f"[INFO] - Training terms (for seen/unseen): {len(train_terms)}")
    sys.stdout.flush()

    # === æ‰§è¡Œå®Œæ•´è¯„ä¼° ===
    print("\n" + "="*50)
    print("FULL EVALUATION WITH COMPLETE GLOSSARY")
    print("="*50)
    sys.stdout.flush()

    recall_results = evaluate_topk_recall(
        model, retriever, test_dataset, device,
        top_ks=(1, 5, 10),
        max_eval=args.max_eval,
        train_terms=train_terms,
        show_missed_terms=True,
        glossary_emb_path=args.glossary_emb_path
    )

    # === ä¿å­˜è¯„ä¼°ç»“æœ ===
    results_path = args.model_path.replace('.pt', '_full_eval_results.json')
    eval_summary = {
        'model_path': args.model_path,
        'glossary_path': args.glossary_path,
        'glossary_emb_path': args.glossary_emb_path,
        'test_samples_path': args.test_samples_path,
        'train_samples_path': args.train_samples_path,
        'total_terms': retriever.index.ntotal if args.glossary_emb_path and os.path.exists(args.glossary_emb_path) else len(glossary_terms),
        'train_terms_count': len(train_terms),
        'test_samples': len(test_dataset),
        'evaluated_samples': min(args.max_eval, len(test_dataset)),
        'results': {}
    }

    for top_k in [1, 5, 10]:
        if top_k in recall_results and recall_results[top_k]:
            avg_recall = sum(recall_results[top_k]) / len(recall_results[top_k])
            eval_summary['results'][f'recall@{top_k}'] = float(avg_recall)

    with open(results_path, 'w') as f:
        json.dump(eval_summary, f, indent=2)

    print(f"\n[INFO] Evaluation results saved to {results_path}")
    print(f"[INFO] Full evaluation completed!")
    sys.stdout.flush()


if __name__ == "__main__":
    main()
