#!/usr/bin/env python3
"""
æµ‹è¯•ä¿®å¤åçš„term-levelè®­ç»ƒæ•°å€¼ç¨³å®šæ€§
"""

import torch
import os
import sys
import json

# æ·»åŠ è·¯å¾„
sys.path.append('/home/jiaxuanluo/InfiniSST/retriever/gigaspeech')

from SONAR_term_level_train import TermLevelDataset, train_step, ContrastiveSpeechTextModel, is_audio_valid
from sonar.inference_pipelines.speech import SpeechToEmbeddingModelPipeline
from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline

def test_numerical_stability():
    print("ğŸ§ª Testing Term-Level Training Numerical Stability")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # 1. æ£€æŸ¥æ•°æ®åŠ è½½
    data_path = "data/samples/xl/term_level_chunks_single_0_500000.json"
    if not os.path.exists(data_path):
        print(f"âŒ Data file not found: {data_path}")
        return False
    
    print(f"âœ… Data file found: {data_path}")
    
    try:
        dataset = TermLevelDataset(data_path, split="train", train_ratio=0.99)
        print(f"âœ… Dataset loaded: {len(dataset)} samples")
    except Exception as e:
        print(f"âŒ Failed to load dataset: {e}")
        return False
    
    # 2. åˆå§‹åŒ–æ¨¡å‹
    try:
        speech_encoder = SpeechToEmbeddingModelPipeline(
            encoder="sonar_speech_encoder_eng", device=device
        )
        text_encoder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            device=device,
            dtype=torch.float32,
        )
        
        model = ContrastiveSpeechTextModel(
            speech_encoder, text_encoder, unfreeze_layers=10
        ).to(device)
        
        print(f"âœ… Model initialized successfully")
    except Exception as e:
        print(f"âŒ Failed to initialize model: {e}")
        return False
    
    # 3. æµ‹è¯•å°batchè®­ç»ƒæ­¥éª¤
    print("\nğŸ”¬ Testing training step with small batch...")
    
    # åˆ›å»ºå°batch (4ä¸ªæ ·æœ¬)
    test_batch = []
    for i in range(min(4, len(dataset))):
        sample = dataset[i]
        if sample is not None:
            test_batch.append(sample)
    
    if len(test_batch) < 2:
        print("âŒ Not enough valid samples for testing")
        return False
    
    print(f"Test batch size: {len(test_batch)}")
    
    # æ‰“å°batchä¿¡æ¯å’ŒéŸ³é¢‘éªŒè¯ç»“æœ
    for i, (gt_terms, audio_path, chunk_text, has_target) in enumerate(test_batch):
        is_valid, reason = is_audio_valid(audio_path)
        print(f"  Sample {i}: GT={gt_terms}, Text='{chunk_text}'")
        print(f"    Audio: {audio_path}")
        print(f"    Valid: {is_valid} ({reason})")
    
    # æµ‹è¯•è®­ç»ƒæ­¥éª¤
    try:
        print("\nâš¡ Running train_step...")
        loss = train_step(model, test_batch, device, temperature=0.1)
        
        print(f"âœ… Train step completed")
        print(f"Loss value: {loss.item():.6f}")
        print(f"Loss requires_grad: {loss.requires_grad}")
        print(f"Loss is NaN: {torch.isnan(loss).any()}")
        print(f"Loss is Inf: {torch.isinf(loss).any()}")
        
        if torch.isnan(loss) or torch.isinf(loss):
            print("âŒ Loss contains NaN or Inf values!")
            return False
        else:
            print("âœ… Loss is numerically stable")
            
    except Exception as e:
        print(f"âŒ Train step failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # 4. æµ‹è¯•æ¢¯åº¦åä¼ 
    try:
        print("\nğŸ”„ Testing gradient computation...")
        model.train()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
        
        loss = train_step(model, test_batch, device, temperature=0.1)
        
        if torch.isnan(loss) or torch.isinf(loss):
            print("âŒ Loss is NaN/Inf, skipping backward")
            return False
        
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        print(f"âœ… Gradient norm: {grad_norm:.6f}")
        
        if torch.isnan(grad_norm) or torch.isinf(grad_norm):
            print("âŒ Gradient norm is NaN/Inf!")
            return False
        
        optimizer.step()
        optimizer.zero_grad()
        
        print("âœ… Gradient computation and optimization step successful")
        
    except Exception as e:
        print(f"âŒ Gradient computation failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\n" + "=" * 60)
    print("ğŸ‰ All numerical stability tests passed!")
    print("âœ… Term-level training should now be stable")
    return True

if __name__ == "__main__":
    success = test_numerical_stability()
    sys.exit(0 if success else 1) 