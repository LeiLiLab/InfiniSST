import torch
from torch.utils.data import DataLoader, Dataset
import numpy as np
import json
from tqdm import tqdm
import argparse, os, sys
import faiss
from new_retrieve import Retriever

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from sonar.inference_pipelines.speech import SpeechToEmbeddingModelPipeline
from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline

# å¯¼å…¥è®­ç»ƒè„šæœ¬ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ç±»
from SONAR_train import ContrastiveSpeechTextModel, InBatchDataset


def load_glossary_terms(glossary_path):
    """åŠ è½½å®Œæ•´çš„æœ¯è¯­è¡¨"""
    print(f"[INFO] Loading glossary from {glossary_path}")
    sys.stdout.flush()
    with open(glossary_path, "r") as f:
        glossary = json.load(f)
    
    # æå–æ‰€æœ‰æœ¯è¯­ï¼Œå¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
    terms = []
    if isinstance(glossary, list):
        for item in glossary:
            if isinstance(item, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•è·å– 'term' æˆ– 'text' å­—æ®µ
                term = item.get('term') or item.get('text') or item.get('word')
                if term:
                    terms.append(term.lower())
            elif isinstance(item, str):
                terms.append(item.lower())
    elif isinstance(glossary, dict):
        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–æ‰€æœ‰å€¼
        for key, value in glossary.items():
            if isinstance(value, str):
                terms.append(value.lower())
            elif isinstance(value, dict) and 'term' in value:
                terms.append(value['term'].lower())
    
    # å»é‡å¹¶è¿‡æ»¤
    terms = list(set(term for term in terms if term and len(term.strip()) >= 2))
    print(f"[INFO] Loaded {len(terms)} unique terms from glossary")
    sys.stdout.flush()
    return terms


def encode_texts_in_batches(model, texts, batch_size=512, device="cuda", auto_batch_size=True, max_chunk_size=1000000):
    """åˆ†æ‰¹ç¼–ç æ–‡æœ¬ï¼Œæ”¯æŒåŠ¨æ€batch_sizeå’Œåˆ†æ®µå¤„ç†"""
    print(f"[INFO] Text encoding setup:")
    print(f"[INFO] - Model type: {type(model)}")
    print(f"[INFO] - Device count: {torch.cuda.device_count()}")
    print(f"[INFO] - Model device: {next(model.parameters()).device if hasattr(model, 'parameters') else 'N/A'}")
    print(f"[INFO] - Initial batch_size: {batch_size}")
    print(f"[INFO] - Total texts: {len(texts)}")
    sys.stdout.flush()
    
    # å¯¹äºå¤§é‡æ–‡æœ¬ï¼Œä½¿ç”¨åˆ†æ®µå¤„ç†
    if len(texts) > max_chunk_size:
        print(f"[INFO] ğŸ“Š Large dataset detected ({len(texts)} texts)")
        print(f"[INFO] ğŸ”„ Using chunked processing with max_chunk_size={max_chunk_size}")
        sys.stdout.flush()
        
        all_results = []
        num_chunks = (len(texts) + max_chunk_size - 1) // max_chunk_size
        
        for chunk_idx in range(num_chunks):
            start_idx = chunk_idx * max_chunk_size
            end_idx = min(start_idx + max_chunk_size, len(texts))
            chunk_texts = texts[start_idx:end_idx]
            
            print(f"[INFO] ğŸ“¦ Processing chunk {chunk_idx + 1}/{num_chunks} ({len(chunk_texts)} texts)")
            sys.stdout.flush()
            
            # é€’å½’è°ƒç”¨å¤„ç†å•ä¸ªchunkï¼ˆä¸ä¼šå†åˆ†æ®µï¼‰
            chunk_result = encode_texts_in_batches(
                model, chunk_texts, batch_size, device, auto_batch_size, max_chunk_size=float('inf')
            )
            all_results.append(chunk_result)
            
            # åŠæ—¶æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            print(f"[INFO] âœ… Chunk {chunk_idx + 1}/{num_chunks} completed, shape: {chunk_result.shape}")
            sys.stdout.flush()
        
        # åˆå¹¶æ‰€æœ‰chunkçš„ç»“æœ
        print(f"[INFO] ğŸ”— Merging {len(all_results)} chunks...")
        sys.stdout.flush()
        final_result = torch.cat(all_results, dim=0)
        
        # æ¸…ç†ä¸­é—´ç»“æœ
        del all_results
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"[INFO] âœ… Chunked processing completed: {final_result.shape}")
        sys.stdout.flush()
        return final_result
    
    # åŠ¨æ€è°ƒæ•´batch_sizeåˆ°æ˜¾å­˜æé™
    if auto_batch_size and torch.cuda.is_available():
        print(f"[INFO] Auto-tuning batch size for optimal GPU memory usage...")
        sys.stdout.flush()
        try_bs = batch_size
        test_texts = texts[:min(try_bs * 4, len(texts))]  # ç”¨å°æ ·æœ¬æµ‹è¯•
        
        while try_bs >= 32:  # æœ€å°batch_size
            try:
                print(f"[DEBUG] Testing batch_size: {try_bs}")
                sys.stdout.flush()
                torch.cuda.empty_cache()  # æ¸…ç†æ˜¾å­˜
                
                with torch.no_grad():
                    test_batch = test_texts[:try_bs] if len(test_texts) >= try_bs else test_texts
                    _ = model.encode_text(test_batch)
                    torch.cuda.empty_cache()
                
                # æˆåŠŸäº†ï¼Œå°è¯•æ›´å¤§çš„batch_sizeï¼ˆä½†ä¸è¶…è¿‡åˆç†ä¸Šé™ï¼‰
                max_reasonable_bs = min(1024, batch_size * 2)  # è®¾ç½®åˆç†ä¸Šé™
                if try_bs < max_reasonable_bs:
                    try_bs = int(try_bs * 1.3)  # æ›´ä¿å®ˆçš„å¢é•¿
                else:
                    break
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) or "out of memory" in str(e):
                    try_bs = max(32, try_bs // 2)
                    print(f"[WARNING] OOM, reducing batch_size to {try_bs}")
                    sys.stdout.flush()
                    torch.cuda.empty_cache()
                else:
                    print(f"[ERROR] Unexpected error during batch size testing: {e}")
                    break
        
                         # å†é€€ä¸€æ­¥ï¼Œç¡®ä¿ç¨³å®šï¼ˆæ›´ä¿å®ˆï¼‰ï¼Œå¹¶è®¾ç½®ç»å¯¹ä¸Šé™
        batch_size = max(32, min(1024, int(try_bs * 0.6)))
        print(f"[INFO] âœ… Optimized batch_size: {batch_size} (capped at 1024)")
        sys.stdout.flush()
    
    # æ‰¹é‡ç¼–ç 
    all_embeddings = []
    total_batches = (len(texts) + batch_size - 1) // batch_size
    print(f"[INFO] Encoding {len(texts)} texts in {total_batches} batches of size {batch_size}")
    sys.stdout.flush()
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        if batch_num % max(1, total_batches // 10) == 0 or batch_num <= 3:
            print(f"[INFO] Processing text batch {batch_num}/{total_batches}")
            sys.stdout.flush()
        
        with torch.no_grad():
            try:
                emb = model.encode_text(batch).cpu()
                all_embeddings.append(emb)
                
                # å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ï¼Œé˜²æ­¢ç´¯ç§¯
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"[ERROR] Failed to encode text batch {batch_num}: {e}")
                sys.stdout.flush()
                # æ¸…ç†æ˜¾å­˜åé‡è¯•
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # å°è¯•æ›´å°çš„batch
                for j in range(0, len(batch), batch_size // 4):
                    mini_batch = batch[j:j + batch_size // 4]
                    try:
                        emb = model.encode_text(mini_batch).cpu()
                        all_embeddings.append(emb)
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except Exception as e2:
                        print(f"[ERROR] Failed mini-batch encoding: {e2}")
                        sys.stdout.flush()
                        continue
    
    if not all_embeddings:
        raise RuntimeError("No texts were successfully encoded")
    
    result = torch.cat(all_embeddings, dim=0)
    print(f"[INFO] âœ… Text encoding completed: {result.shape}")
    sys.stdout.flush()
    return result


def encode_audios_in_batches(model, audio_paths, batch_size=1000, device="cuda", auto_batch_size=True):
    """åˆ†æ‰¹ç¼–ç éŸ³é¢‘ï¼Œæ”¯æŒåŠ¨æ€batch_sizeä¼˜åŒ–"""
    print(f"[INFO] Audio encoding setup:")
    print(f"[INFO] - Model type: {type(model)}")
    print(f"[INFO] - Initial batch_size: {batch_size}")
    sys.stdout.flush()
    
    # åŠ¨æ€è°ƒæ•´audio batch_sizeï¼ˆéŸ³é¢‘ç¼–ç æ›´æ¶ˆè€—æ˜¾å­˜ï¼‰
    if auto_batch_size and torch.cuda.is_available() and len(audio_paths) > batch_size:
        print(f"[INFO] Auto-tuning audio batch size...")
        sys.stdout.flush()
        try_bs = batch_size
        test_paths = audio_paths[:min(try_bs * 2, len(audio_paths))]  # ç”¨å°æ ·æœ¬æµ‹è¯•
        
        while try_bs >= 4:  # éŸ³é¢‘æœ€å°batch_size
            try:
                print(f"[DEBUG] Testing audio batch_size: {try_bs}")
                sys.stdout.flush()
                torch.cuda.empty_cache()
                
                with torch.no_grad():
                    test_batch = test_paths[:try_bs] if len(test_paths) >= try_bs else test_paths
                    _ = model.encode_audio(test_batch)
                    torch.cuda.empty_cache()
                
                # æˆåŠŸäº†ï¼Œå°è¯•ç¨å¤§ä¸€ç‚¹çš„batch_sizeï¼ˆéŸ³é¢‘æ›´ä¿å®ˆï¼‰
                max_reasonable_bs = min(128, batch_size * 2)  # éŸ³é¢‘ä¸Šé™128
                if try_bs < max_reasonable_bs:
                    try_bs = int(try_bs * 1.2)  # æ›´ä¿å®ˆçš„å¢é•¿
                else:
                    break
            except RuntimeError as e:
                if "CUDA out of memory" in str(e) or "out of memory" in str(e):
                    try_bs = max(4, try_bs // 2)
                    print(f"[WARNING] Audio OOM, reducing batch_size to {try_bs}")
                    sys.stdout.flush()
                    torch.cuda.empty_cache()
                else:
                    print(f"[ERROR] Unexpected error during audio batch size testing: {e}")
                    break
        
        batch_size = max(4, min(128, int(try_bs * 0.7)))  # ä¿å®ˆä¸€ç‚¹ï¼Œä¸Šé™128
        print(f"[INFO] âœ… Optimized audio batch_size: {batch_size} (capped at 128)")
        sys.stdout.flush()
    
    # æ‰¹é‡ç¼–ç 
    all_embeddings = []
    total_batches = (len(audio_paths) + batch_size - 1) // batch_size
    print(f"[INFO] Encoding {len(audio_paths)} audio files in {total_batches} batches of size {batch_size}")
    sys.stdout.flush()
    
    for i in range(0, len(audio_paths), batch_size):
        batch_paths = audio_paths[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        if batch_num % max(1, total_batches // 10) == 0 or batch_num <= 3:
            print(f"[INFO] Processing audio batch {batch_num}/{total_batches}")
            sys.stdout.flush()
        
        with torch.no_grad():
            try:
                emb = model.encode_audio(batch_paths).cpu()
                all_embeddings.append(emb)
            except Exception as e:
                print(f"[ERROR] Failed to encode audio batch {batch_num}: {e}")
                sys.stdout.flush()
                print(f"[INFO] Trying single file processing for this batch...")
                sys.stdout.flush()
                # å¦‚æœbatchå¤±è´¥ï¼Œå°è¯•å•ä¸ªå¤„ç†
                for single_path in batch_paths:
                    try:
                        single_emb = model.encode_audio([single_path]).cpu()
                        all_embeddings.append(single_emb)
                    except Exception as e2:
                        print(f"[ERROR] Failed to encode single audio {single_path}: {e2}")
                        sys.stdout.flush()
                        # è·³è¿‡è¿™ä¸ªéŸ³é¢‘æ–‡ä»¶
                        continue
    
    if not all_embeddings:
        raise RuntimeError("No audio files were successfully encoded")
    
    result = torch.cat(all_embeddings, dim=0)
    print(f"[INFO] âœ… Audio encoding completed: {result.shape}")
    sys.stdout.flush()
    return result


def evaluate_topk_recall_full(model, retriever, dataset, device, top_ks=(5, 10, 20), max_eval=1000, field="term", train_terms=None, audio_batch_size=1000):
    """å®Œæ•´è¯„ä¼°å‡½æ•°ï¼Œå¯è¢«è®­ç»ƒå’Œfull evaluateè„šæœ¬å¤ç”¨
    
    Args:
        train_terms: ä»…æ¥è‡ªè®­ç»ƒé›†çš„æœ¯è¯­åˆ—è¡¨ï¼Œç”¨äºåŒºåˆ†seen/unseen terms
    """
    model.eval()
    recall_dict = {k: [] for k in top_ks}

    # === é‡å»ºç´¢å¼• ===
    text_terms = [term['term'] for term in retriever.term_list]
    print(f'[INFO] Building index with {len(text_terms)} terms')
    sys.stdout.flush()
    
    # æ£€æŸ¥æ¨¡å‹å¹¶è¡ŒçŠ¶æ€
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    is_data_parallel = isinstance(model, torch.nn.DataParallel)
    print(f"[INFO] Model parallel status: DataParallel={is_data_parallel}")
    if is_data_parallel:
        print(f"[INFO] DataParallel will use {len(model.device_ids)} GPUs: {model.device_ids}")
    sys.stdout.flush()
    
    # æ–‡æœ¬ç¼–ç ï¼ˆè‡ªåŠ¨ä¼˜åŒ–batch_sizeï¼‰
    print(f"[INFO] ğŸš€ Starting text encoding with GPU optimization...")
    sys.stdout.flush()
    text_emb = encode_texts_in_batches(raw_model, text_terms, batch_size=512, device=device, auto_batch_size=True)
    
    # === æ„å»ºGPUåŠ é€Ÿçš„FAISSç´¢å¼• ===
    print(f"[INFO] ğŸš€ Building GPU-accelerated FAISS index...")
    sys.stdout.flush()
    
    embedding_dim = text_emb.shape[1]
    print(f"[INFO] Converting embeddings to numpy (shape: {text_emb.shape})...")
    sys.stdout.flush()
    
    text_emb_np = text_emb.numpy().astype('float32')
    
    # åŠæ—¶é‡Šæ”¾PyTorch tensor
    del text_emb
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    try:
        # å°è¯•ä½¿ç”¨å¤šGPU FAISSç´¢å¼•
        if torch.cuda.device_count() > 1:
            print(f"[INFO] Using multi-GPU FAISS index ({torch.cuda.device_count()} GPUs)")
            sys.stdout.flush()
            
            # åˆ›å»ºCPUç´¢å¼•ç„¶åè½¬åˆ°æ‰€æœ‰GPU
            cpu_index = faiss.IndexFlatL2(embedding_dim)
            
            # åˆ†æ‰¹æ·»åŠ embeddingsä»¥é¿å…GPUå†…å­˜é—®é¢˜
            add_batch_size = min(500000, len(text_emb_np))  # 50ä¸‡ä¸€æ‰¹
            num_add_batches = (len(text_emb_np) + add_batch_size - 1) // add_batch_size
            
            print(f"[INFO] Adding {len(text_emb_np)} embeddings in {num_add_batches} batches of {add_batch_size}...")
            sys.stdout.flush()
            
            for batch_idx in range(num_add_batches):
                start_idx = batch_idx * add_batch_size
                end_idx = min(start_idx + add_batch_size, len(text_emb_np))
                batch_embeddings = text_emb_np[start_idx:end_idx]
                
                cpu_index.add(batch_embeddings)
                print(f"[INFO] Added batch {batch_idx + 1}/{num_add_batches} to CPU index")
                sys.stdout.flush()
            
            # è½¬æ¢åˆ°GPU
            print(f"[INFO] Converting CPU index to multi-GPU...")
            sys.stdout.flush()
            gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)
            
            retriever.index = gpu_index
            print(f"[INFO] âœ… Multi-GPU FAISS index built successfully!")
            
        else:
            # å•GPU FAISSç´¢å¼•
            print(f"[INFO] Using single-GPU FAISS index")
            sys.stdout.flush()
            
            res = faiss.StandardGpuResources()
            # è®¾ç½®æ›´å¤§çš„ä¸´æ—¶å†…å­˜
            res.setTempMemory(1024 * 1024 * 1024)  # 1GB temp memory
            
            cpu_index = faiss.IndexFlatL2(embedding_dim)
            
            # åˆ†æ‰¹æ·»åŠ åˆ°CPUç´¢å¼•
            add_batch_size = min(1000000, len(text_emb_np))  # 100ä¸‡ä¸€æ‰¹
            num_add_batches = (len(text_emb_np) + add_batch_size - 1) // add_batch_size
            
            print(f"[INFO] Adding {len(text_emb_np)} embeddings in {num_add_batches} batches...")
            sys.stdout.flush()
            
            for batch_idx in range(num_add_batches):
                start_idx = batch_idx * add_batch_size
                end_idx = min(start_idx + add_batch_size, len(text_emb_np))
                batch_embeddings = text_emb_np[start_idx:end_idx]
                
                cpu_index.add(batch_embeddings)
                print(f"[INFO] Added batch {batch_idx + 1}/{num_add_batches} to CPU index")
                sys.stdout.flush()
            
            # è½¬æ¢åˆ°GPU
            print(f"[INFO] Converting CPU index to GPU...")
            sys.stdout.flush()
            gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
            
            retriever.index = gpu_index
            print(f"[INFO] âœ… Single-GPU FAISS index built successfully!")
            
    except Exception as e:
        # å›é€€åˆ°CPUç´¢å¼•
        print(f"[WARNING] GPU FAISS failed ({e}), falling back to CPU index")
        sys.stdout.flush()
        
        cpu_index = faiss.IndexFlatL2(embedding_dim)
        
        # åˆ†æ‰¹æ·»åŠ ä»¥é¿å…å†…å­˜é—®é¢˜
        add_batch_size = min(2000000, len(text_emb_np))  # 200ä¸‡ä¸€æ‰¹
        num_add_batches = (len(text_emb_np) + add_batch_size - 1) // add_batch_size
        
        print(f"[INFO] Adding {len(text_emb_np)} embeddings to CPU index in {num_add_batches} batches...")
        sys.stdout.flush()
        
        for batch_idx in range(num_add_batches):
            start_idx = batch_idx * add_batch_size
            end_idx = min(start_idx + add_batch_size, len(text_emb_np))
            batch_embeddings = text_emb_np[start_idx:end_idx]
            
            cpu_index.add(batch_embeddings)
            print(f"[INFO] Added batch {batch_idx + 1}/{num_add_batches} to CPU index")
            sys.stdout.flush()
        
        retriever.index = cpu_index
        print(f"[INFO] âœ… CPU FAISS index built as fallback")
    
    # æ¸…ç†numpyæ•°ç»„
    del text_emb_np
    sys.stdout.flush()

    print(f"[INFO] Dataset size: {len(dataset)}")
    sys.stdout.flush()
    import random
    random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
    eval_indices = random.sample(range(len(dataset)), min(max_eval, len(dataset)))
    valid_samples = []
    valid_indices = []
    
    for i in eval_indices:
        sample = dataset[i]
        if sample is not None and sample[3] and sample[0]:  # has_target=True and has ground_truth_terms
            valid_samples.append(sample)
            valid_indices.append(i)

    print(f"[INFO] Selected {len(eval_indices)} samples randomly, {len(valid_samples)} are valid for evaluation")
    print(f"[INFO] Filtered out {len(eval_indices) - len(valid_samples)} samples (no ground truth terms or has_target=False)")
    sys.stdout.flush()
    
    # ä½¿ç”¨chunkéŸ³é¢‘è¿›è¡Œç¼–ç ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
    audio_paths = [sample[1] for sample in valid_samples]  # n_chunk_audio paths
    audio_embs = encode_audios_in_batches(raw_model, audio_paths, batch_size=audio_batch_size, device=device, auto_batch_size=True).numpy()

    for j, (i, sample) in enumerate(zip(valid_indices, valid_samples)):
        ground_truth_terms, audio_path, chunk_text, has_target = sample
        audio_emb = audio_embs[j:j+1]  # shape: [1, 512]

        for top_k in top_ks:
            D, I = retriever.index.search(audio_emb, top_k)
            retrieved_terms = [retriever.term_list[idx][field].lower() for idx in I[0]]
            gt_terms = [t.lower() for t in ground_truth_terms]  # ä½¿ç”¨n_chunk_audio_ground_truth_terms

            matched = sum(gt in retrieved_terms for gt in gt_terms)
            recall = matched / len(gt_terms) if gt_terms else 0.0
            recall_dict[top_k].append(recall)

            if j < 3 and top_k == top_ks[0]:  # åªæ‰“å°å‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"[DEBUG] Sample {i}:")
                print(f"[DEBUG] Chunk text: {chunk_text[:100]}...")
                print(f"[DEBUG] GT terms: {gt_terms}")
                print(f"[DEBUG] Retrieved terms: {retrieved_terms}")
                print(f"[DEBUG] Match count: {matched}/{len(gt_terms)}")
                print(f"[DEBUG] Recall: {recall:.2%}")
                sys.stdout.flush()

    # æ‰“å°ç»Ÿè®¡ç»“æœ
    for top_k in top_ks:
        avg_recall = sum(recall_dict[top_k]) / len(recall_dict[top_k]) if recall_dict[top_k] else 0.0
        print(f"[EVAL] Average Recall@{top_k}: {avg_recall:.2%}")
        sys.stdout.flush()

        # === è®¡ç®— seen/unseen recall ===
        if train_terms is not None:
            # åªæœ‰è®­ç»ƒé›†ä¸­çš„æœ¯è¯­æ‰ç®—seen
            seen_terms = set(t.lower() for t in train_terms)
            seen_recalls, unseen_recalls = [], []
            for recall_val, sample in zip(recall_dict[top_k], valid_samples):
                gt_terms = [t.lower() for t in sample[0]]
                if all(gt in seen_terms for gt in gt_terms):
                    seen_recalls.append(recall_val)
                else:
                    unseen_recalls.append(recall_val)

            avg_seen = sum(seen_recalls) / len(seen_recalls) if seen_recalls else 0.0
            avg_unseen = sum(unseen_recalls) / len(unseen_recalls) if unseen_recalls else 0.0
            total_samples = len(seen_recalls) + len(unseen_recalls)
            print(f"[EVAL] Seen Recall@{top_k}: {avg_seen:.2%} ({len(seen_recalls)}/{total_samples} samples), Unseen Recall@{top_k}: {avg_unseen:.2%} ({len(unseen_recalls)}/{total_samples} samples)")
            sys.stdout.flush()
        else:
            print(f"[WARN] train_terms not provided, skipping seen/unseen analysis")
            sys.stdout.flush()

    model.train()
    return recall_dict


def load_model(model_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"[INFO] Loading model from {model_path}")
    sys.stdout.flush()
    
    # ç¡®ä¿deviceæ˜¯torch.deviceå¯¹è±¡
    if isinstance(device, str):
        device = torch.device(device)
    
    # åˆå§‹åŒ–ç¼–ç å™¨
    try:
        speech_encoder = SpeechToEmbeddingModelPipeline(
            encoder="sonar_speech_encoder_eng", device=device
        )
    except Exception as e:
        print(f"[ERROR] Failed to initialize speech encoder: {e}")
        sys.stdout.flush()
        print(f"[INFO] Trying alternative initialization...")
        sys.stdout.flush()
        # å°è¯•ä¸ä¼ é€’deviceå‚æ•°
        speech_encoder = SpeechToEmbeddingModelPipeline(
            encoder="sonar_speech_encoder_eng"
        )
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if hasattr(speech_encoder, 'model'):
            speech_encoder.model = speech_encoder.model.to(device)

    try:
        text_encoder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            device=device,
            dtype=torch.float32,
        )
    except Exception as e:
        print(f"[ERROR] Failed to initialize text encoder: {e}")
        sys.stdout.flush()
        print(f"[INFO] Trying alternative initialization...")
        sys.stdout.flush()
        # å°è¯•ä¸ä¼ é€’deviceå‚æ•°
        text_encoder = TextToEmbeddingModelPipeline(
            encoder="text_sonar_basic_encoder",
            tokenizer="text_sonar_basic_encoder",
            dtype=torch.float32,
        )
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if hasattr(text_encoder, 'model'):
            text_encoder.model = text_encoder.model.to(device)

    # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œå› ä¸ºç»“æ„éœ€è¦åŒ¹é…ï¼‰
    model = ContrastiveSpeechTextModel(
        speech_encoder, text_encoder, 
        unfreeze_layers=10  # è¿™ä¸ªå‚æ•°ä¸å½±å“æ¨ç†ï¼Œåªå½±å“è®­ç»ƒæ—¶çš„å‚æ•°å†»ç»“
    ).to(device)
    
    # åŠ è½½è®­ç»ƒå¥½çš„å‚æ•°
    state_dict = torch.load(model_path, map_location=device)
    
    # å¤„ç† DataParallel çš„æƒ…å†µ
    if list(state_dict.keys())[0].startswith('module.'):
        # ç§»é™¤ 'module.' å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            new_state_dict[k[7:]] = v  # ç§»é™¤ 'module.' (7ä¸ªå­—ç¬¦)
        state_dict = new_state_dict
    
    model.load_state_dict(state_dict)
    model.eval()
    print(f"[INFO] Model loaded successfully")
    sys.stdout.flush()
    
    # è‡ªåŠ¨å¤šGPUåŒ…è£…
    if torch.cuda.device_count() > 1:
        print(f"[INFO] ğŸš€ Detected {torch.cuda.device_count()} GPUs, wrapping with DataParallel")
        available_gpus = list(range(torch.cuda.device_count()))
        model = torch.nn.DataParallel(model, device_ids=available_gpus)
        print(f"[INFO] âœ… DataParallel enabled on GPUs: {available_gpus}")
        sys.stdout.flush()
    else:
        print(f"[INFO] Single GPU mode: {device}")
        sys.stdout.flush()
    
    return model


def main():
    parser = argparse.ArgumentParser(description="Full evaluation with complete glossary")
    parser.add_argument('--model_path', type=str, required=True,
                       help="Path to trained model (.pt file)")
    parser.add_argument('--test_samples_path', type=str, 
                       default="data/samples/xl/test_mfa_3chunks_samples_0_500000.json",
                       help="Path to test samples")
    parser.add_argument('--glossary_path', type=str, 
                       default="data/terms/glossary_filtered.json",
                       help="Path to complete glossary file")
    parser.add_argument('--train_ratio', type=float, default=0.99,
                       help="Train ratio used during training (to get consistent test split)")
    parser.add_argument('--max_eval', type=int, default=1000,
                       help="Maximum number of samples to evaluate")
    parser.add_argument('--batch_size', type=int, default=512,
                       help="Initial batch size for text encoding (will be auto-optimized, max 1024)")
    parser.add_argument('--audio_batch_size', type=int, default=1000,
                       help="Initial batch size for audio encoding (will be auto-optimized, max 128)")

    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[INFO] Using device: {device}")
    sys.stdout.flush()

    # === åŠ è½½æ¨¡å‹ ===
    device = torch.device(device)  # è½¬æ¢ä¸ºtorch.deviceå¯¹è±¡
    model = load_model(args.model_path, device)

    # === åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›† ===
    print(f"[INFO] Loading test dataset from {args.test_samples_path}")
    sys.stdout.flush()
    test_dataset = InBatchDataset(
        args.test_samples_path, 
        split="test", 
        train_ratio=args.train_ratio
    )
    print(f"[INFO] Test dataset size: {len(test_dataset)}")
    sys.stdout.flush()
    
    # åŠ è½½è®­ç»ƒæ•°æ®é›†ä»¥è·å–è®­ç»ƒé›†æœ¯è¯­ï¼ˆç”¨äºseen/unseenåˆ†æï¼‰
    print(f"[INFO] Loading training dataset for seen/unseen analysis...")
    sys.stdout.flush()
    train_dataset = InBatchDataset(
        args.test_samples_path, 
        split="train", 
        train_ratio=args.train_ratio
    )
    
    # æå–è®­ç»ƒé›†ä¸­çš„æœ¯è¯­
    def extract_all_used_terms(dataset):
        """æå–æ•°æ®é›†ä¸­æ‰€æœ‰ä½¿ç”¨çš„æœ¯è¯­"""
        used_terms = set()
        for i in range(len(dataset)):
            sample = dataset[i]
            if sample is None:
                continue
            ground_truth_terms, audio_path, chunk_text, has_target = sample
            if has_target and ground_truth_terms:
                used_terms.update(t.lower() for t in ground_truth_terms if isinstance(t, str))
        return list(used_terms)
    
    train_terms = extract_all_used_terms(train_dataset)
    test_terms = extract_all_used_terms(test_dataset)
    print(f"[INFO] Found {len(train_terms)} unique terms in training set")
    print(f"[INFO] Found {len(test_terms)} unique terms in test set")
    sys.stdout.flush()
    
    # åˆ†ætrain/testæœ¯è¯­é‡å 
    train_set = set(train_terms)
    test_set = set(test_terms)
    overlap = train_set.intersection(test_set)
    unseen_terms = test_set - train_set
    print(f"[INFO] Terms overlap between train/test: {len(overlap)} terms")
    print(f"[INFO] Test terms that are unseen in training: {len(unseen_terms)} terms")
    if len(unseen_terms) > 0:
        print(f"[INFO] Example unseen terms: {list(unseen_terms)[:10]}...")
    sys.stdout.flush()

    # === åŠ è½½å®Œæ•´æœ¯è¯­è¡¨ ===
    glossary_terms = load_glossary_terms(args.glossary_path)

    # === åˆå§‹åŒ– retriever ä½¿ç”¨å®Œæ•´æœ¯è¯­è¡¨ ===
    retriever = Retriever(enable_fusion=True, device=device)
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    retriever.model = raw_model
    retriever.index = faiss.IndexFlatL2(512)  # åˆå§‹åŒ–ç©ºç´¢å¼•
    retriever.term_list = [{'term': t} for t in glossary_terms]

    print(f"[INFO] Using complete glossary with {len(glossary_terms)} terms")
    sys.stdout.flush()

    # === åˆ†ææ•°æ®é›†æœ¯è¯­åˆ†å¸ƒ ===
    print(f"\n[INFO] Dataset statistics:")
    print(f"[INFO] - Training terms: {len(train_terms)}")
    print(f"[INFO] - Glossary terms: {len(glossary_terms)}")
    sys.stdout.flush()
    
    # è®¡ç®—è®­ç»ƒé›†æœ¯è¯­åœ¨å®Œæ•´è¯æ±‡è¡¨ä¸­çš„è¦†ç›–ç‡
    train_terms_set = set(train_terms)
    glossary_terms_set = set(glossary_terms)
    overlap = train_terms_set.intersection(glossary_terms_set)
    coverage = len(overlap) / len(train_terms_set) if train_terms_set else 0
    print(f"[INFO] - Training terms covered in glossary: {len(overlap)}/{len(train_terms)} ({coverage:.1%})")
    sys.stdout.flush()

    # === æ‰§è¡Œå®Œæ•´è¯„ä¼° ===
    print("\n" + "="*50)
    print("FULL EVALUATION WITH COMPLETE GLOSSARY")
    print("="*50)
    sys.stdout.flush()
    
    recall_results = evaluate_topk_recall_full(
        model, retriever, test_dataset, device,
        top_ks=(1, 5, 10),
        max_eval=args.max_eval,
        train_terms=train_terms,  # ä¼ å…¥è®­ç»ƒé›†æœ¯è¯­ç”¨äºseen/unseenåˆ†æ
        audio_batch_size=args.audio_batch_size
    )

    # === ä¿å­˜è¯„ä¼°ç»“æœ ===
    results_path = args.model_path.replace('.pt', '_full_eval_results.json')
    eval_summary = {
        'model_path': args.model_path,
        'glossary_path': args.glossary_path,
        'test_samples_path': args.test_samples_path,
        'total_terms': len(glossary_terms),
        'train_terms': len(train_terms),
        'test_terms': len(test_terms),
        'terms_overlap': len(overlap),
        'unseen_terms': len(unseen_terms),
        'test_samples': len(test_dataset),
        'train_samples': len(train_dataset),
        'evaluated_samples': min(args.max_eval, len(test_dataset)),
        'results': {}
    }
    
    for top_k in [1, 5, 10]:
        if top_k in recall_results and recall_results[top_k]:
            avg_recall = sum(recall_results[top_k]) / len(recall_results[top_k])
            eval_summary['results'][f'recall@{top_k}'] = float(avg_recall)
    
    with open(results_path, 'w') as f:
        json.dump(eval_summary, f, indent=2)
    
    print(f"\n[INFO] Evaluation results saved to {results_path}")
    print(f"[INFO] Full evaluation completed!")
    sys.stdout.flush()


if __name__ == "__main__":
    main()
