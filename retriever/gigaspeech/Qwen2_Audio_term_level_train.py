import torch
from torch.utils.data import DataLoader, Dataset
import numpy as np
import json
from tqdm import tqdm
import argparse
import os
import sys
from torch.optim.lr_scheduler import ReduceLROnPlateau
import faiss
import mmap
from new_retrieve import Retriever
import soundfile as sf

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

# å¯¼å…¥æ–°çš„Qwen2-Audioæ¨¡å‹ç»“æ„
from Qwen2_Audio_train import (
    Qwen2AudioSpeechEncoder, 
    Qwen2AudioTextEncoder, 
    ContrastiveQwen2AudioModel,
    load_glossary_terms, 
    encode_texts_in_batches, 
    encode_audios_in_batches
)


# === Hard Negative Mining Context Helper ===
class HardNegContext:
    """
    Dual-mode hard negative context:
      1) In-memory tensor mode (small bank): provide emb_tensor [N, D] (L2-normalized) and term2idx dict.
      2) FAISS index mode (large bank): provide faiss_index (IVF/HNSW/Flat etc.) and term2idx dict.
    """
    def __init__(self, terms=None, term2idx=None, emb_tensor=None, faiss_index=None, metric='ip'):
        self.terms = terms or []
        self.term2idx = term2idx or {}
        self.emb_tensor = emb_tensor  # torch.FloatTensor [N, D] on device (normalized)
        self.faiss_index = faiss_index  # faiss.Index or None
        # metric: 'ip' (inner product) or 'l2'
        self.metric = metric


# === Utility to load term2idx JSON ===
def load_term2idx_json(path):
    if path is None:
        return {}
    try:
        with open(path, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"[WARN] Failed to load term2idx from {path}: {e}")
        return {}


def is_audio_valid(audio_path, min_duration=0.01, max_duration=30.0):
    """æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
    try:
        if not os.path.exists(audio_path):
            return False, "File does not exist"
        
        data, sr = sf.read(audio_path)
        
        # æ£€æŸ¥åŸºæœ¬å±æ€§
        if len(data) == 0:
            return False, "Empty audio file"
        
        duration = len(data) / sr
        if duration < min_duration:
            return False, f"Too short ({duration:.3f}s < {min_duration}s)"
        
        if duration > max_duration:
            return False, f"Too long ({duration:.3f}s > {max_duration}s)"
        
        # æ£€æŸ¥æ˜¯å¦å…¨é™éŸ³
        if np.allclose(data, 0, atol=1e-6):
            return False, "All silence"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if np.isnan(data).any():
            return False, "Contains NaN values"
        
        if np.isinf(data).any():
            return False, "Contains Inf values"
        
        # æ£€æŸ¥åŠ¨æ€èŒƒå›´
        data_std = np.std(data)
        if data_std < 1e-6:
            return False, f"Very low dynamic range (std={data_std:.2e})"
        
        return True, "Valid"
        
    except Exception as e:
        return False, f"Failed to read: {str(e)}"


def validate_audio_batch(audio_paths, verbose=False):
    """æ‰¹é‡éªŒè¯éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›æœ‰æ•ˆçš„è·¯å¾„åˆ—è¡¨å’Œå¯¹åº”çš„åŸå§‹ç´¢å¼•"""
    valid_paths = []
    valid_indices = []
    invalid_count = 0
    
    for i, path in enumerate(audio_paths):
        is_valid, reason = is_audio_valid(path)
        if is_valid:
            valid_paths.append(path)
            valid_indices.append(i)
        else:
            invalid_count += 1
            if verbose or invalid_count <= 5:  # åªæ‰“å°å‰5ä¸ªæ— æ•ˆæ–‡ä»¶
                print(f"[WARN] Invalid audio {i}: {path} - {reason}")
    
    if invalid_count > 5:
        print(f"[WARN] ... and {invalid_count - 5} more invalid audio files")
    
    return valid_paths, valid_indices


class TermLevelDataset(Dataset):
    def __init__(self, path="data/xl_term_level_chunks_merged.json", split="train", train_ratio=0.99, test_path=None, enable_no_term=False, filter_no_term=True):
        self.enable_no_term = enable_no_term
        self.filter_no_term = filter_no_term
        print(f"[INFO] No-term samples enabled: {enable_no_term}")
        print(f"[INFO] Filter no-term samples: {filter_no_term}")
        
        if split == "test" and test_path is not None:
            # ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®é›†
            print(f"[INFO] Loading test samples from separate file: {test_path}")
            with open(test_path, "r") as f:
                all_samples = json.load(f)
            # å¯¹äºç‹¬ç«‹æµ‹è¯•é›†ï¼Œä¸éœ€è¦train_ratioåˆ†å‰²ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æ ·æœ¬
            use_split_logic = False
        else:
            # ä½¿ç”¨åŸæœ‰çš„åˆ†å‰²é€»è¾‘
            print(f"[INFO] Loading term-level chunk samples from {path}")
            with open(path, "r") as f:
                all_samples = json.load(f)
            use_split_logic = True
        
        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ï¼šåŒ…æ‹¬æœ‰æœ¯è¯­å’Œæ— æœ¯è¯­çš„æ ·æœ¬
        valid_samples = []
        invalid_audio_count = 0
        term_samples_count = 0
        no_term_samples_count = 0
        
        for i, s in enumerate(all_samples):
            terms = s.get('term_chunk_audio_ground_truth_terms')
            if not isinstance(terms, list):
                terms = []
            
            # è¿‡æ»¤æœ¯è¯­ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            filtered_terms = [
                t for t in terms
                if isinstance(t, str)
                and len(t) >= 3
                and sum(c.isdigit() for c in t) <= len(t) // 2
            ]

            # è¿‡æ»¤å‰åç¼€
            black_words = ['yeah','this ']
            black_suffixes = ['years']
            filtered_terms = [
                t for t in filtered_terms 
                if not any(t.lower().startswith(prefix.lower()) for prefix in black_words)
                and not any(t.lower().endswith(suffix.lower()) for suffix in black_suffixes)
            ]
            
            # æ›¿æ¢åŸåˆ—è¡¨ä¸ºè¿‡æ»¤åçš„æœ¯è¯­ï¼ˆå…è®¸ä¸ºç©ºåˆ—è¡¨ï¼‰
            s = dict(s)  # é¿å…ç›´æ¥ä¿®æ”¹åŸå§‹æ•°æ®
            s['term_chunk_audio_ground_truth_terms'] = filtered_terms
            
            # æ£€æŸ¥åŸºæœ¬æ¡ä»¶
            if not (s.get('term_chunk_text', '').strip() and s.get('term_chunk_audio', '')):
                continue
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æœ‰æ•ˆæ€§
            audio_path = s.get("term_chunk_audio", "")
            is_valid, reason = is_audio_valid(audio_path)
            
            if is_valid:
                # æ ¹æ®filter_no_termé…ç½®å†³å®šæ˜¯å¦åŒ…å«æ— æœ¯è¯­æ ·æœ¬
                if filtered_terms:
                    # æœ‰æœ¯è¯­çš„æ ·æœ¬æ€»æ˜¯åŒ…å«
                    valid_samples.append(s)
                    term_samples_count += 1
                elif not self.filter_no_term:
                    # åªæœ‰åœ¨ä¸è¿‡æ»¤no-termæ—¶æ‰åŒ…å«æ— æœ¯è¯­æ ·æœ¬
                    valid_samples.append(s)
                    no_term_samples_count += 1
                # å¦‚æœfilter_no_term=Trueä¸”æ— æœ¯è¯­ï¼Œåˆ™è·³è¿‡è¯¥æ ·æœ¬
            else:
                invalid_audio_count += 1
                # åªæ‰“å°å‰10ä¸ªæ— æ•ˆéŸ³é¢‘çš„è¯¦ç»†ä¿¡æ¯
                if invalid_audio_count <= 10:
                    print(f"[WARN] Skipping sample {i}: {audio_path} - {reason}")
        
        if invalid_audio_count > 10:
            print(f"[WARN] ... and {invalid_audio_count - 10} more samples with invalid audio")
            
        print(f"[INFO] Audio validation: {len(valid_samples)} valid, {invalid_audio_count} invalid")
        print(f"[INFO] Dataset composition: {term_samples_count} term samples + {no_term_samples_count} no-term samples = {len(valid_samples)} total")
        if len(valid_samples) > 0:
            print(f"[INFO] No-term ratio: {no_term_samples_count/len(valid_samples):.1%}")
        
        if self.filter_no_term and no_term_samples_count == 0:
            print(f"[INFO] No-term samples filtered out (filter_no_term=True)")
        
        print(f"[INFO] Filtered {len(valid_samples)} valid term-level samples from {len(all_samples)} total samples")
        
        if use_split_logic:
            # æ•°æ®åˆ†å‰²ï¼š99%è®­ç»ƒï¼Œ1%æµ‹è¯•
            import random
            random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
            random.shuffle(valid_samples)
            
            split_idx = int(len(valid_samples) * train_ratio)
            
            if split == "train":
                self.samples = valid_samples[:split_idx]
                # ç»Ÿè®¡è®­ç»ƒé›†ä¸­çš„no-termæ ·æœ¬
                train_no_term_count = sum(1 for s in self.samples if not s.get('term_chunk_audio_ground_truth_terms'))
                print(f"[INFO] Training split: {len(self.samples)} samples ({train_no_term_count} no-term, {len(self.samples)-train_no_term_count} term)")
            elif split == "test":
                self.samples = valid_samples[split_idx:]
                # ç»Ÿè®¡æµ‹è¯•é›†ä¸­çš„no-termæ ·æœ¬
                test_no_term_count = sum(1 for s in self.samples if not s.get('term_chunk_audio_ground_truth_terms'))
                print(f"[INFO] Test split: {len(self.samples)} samples ({test_no_term_count} no-term, {len(self.samples)-test_no_term_count} term)")
            else:
                raise ValueError(f"Invalid split: {split}. Must be 'train' or 'test'")
        else:
            # ç‹¬ç«‹æµ‹è¯•é›†ï¼Œç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
            self.samples = valid_samples
            test_no_term_count = sum(1 for s in self.samples if not s.get('term_chunk_audio_ground_truth_terms'))
            print(f"[INFO] Using separate test dataset: {len(self.samples)} samples ({test_no_term_count} no-term, {len(self.samples)-test_no_term_count} term)")
        
        print(f"[INFO] Loaded {len(self.samples)} term-level samples for {split} split")

    def __getitem__(self, idx):
        sample = self.samples[idx]
        audio_path = sample["term_chunk_audio"]  # ä½¿ç”¨term chunkéŸ³é¢‘
        chunk_text = sample["term_chunk_text"]   # ä½¿ç”¨term chunkæ–‡æœ¬
        ground_truth_terms = sample.get('term_chunk_audio_ground_truth_terms', [])
        has_target = bool(ground_truth_terms and len(ground_truth_terms) > 0)
        
        return ground_truth_terms, audio_path, chunk_text, has_target

    def __len__(self):
        return len(self.samples)


def train_step(model, batch, device, args, hn_ctx=None, temperature=0.07):
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model

    if len(batch) < 2:
        print("Batch has less than 2 non-None items, skipping...")
        return torch.tensor(0.0, requires_grad=True).to(device)

    # æ‹†åˆ†batchæ•°æ®ï¼šground_truth_terms, audio_path, chunk_text, has_target
    ground_truth_terms_list, audio_paths, chunk_texts, has_targets = zip(*batch)
    
    # å…¨å°å†™å¤„ç†
    ground_truth_terms_list = [[t.lower() for t in terms if isinstance(t, str)] for terms in ground_truth_terms_list]
    chunk_texts = [text.lower() if isinstance(text, str) else "" for text in chunk_texts]

    # === ç¼–ç éŸ³é¢‘å’Œæ–‡æœ¬ ===
    try:
        # å…ˆéªŒè¯éŸ³é¢‘æ–‡ä»¶æ‰¹æ¬¡
        valid_audio_paths, valid_audio_indices = validate_audio_batch(audio_paths, verbose=True)
        
        if len(valid_audio_paths) == 0:
            print(f"[ERROR] No valid audio files in batch, skipping")
            return torch.tensor(0.0, requires_grad=True).to(device)
        
        if len(valid_audio_paths) != len(audio_paths):
            print(f"[WARN] Only {len(valid_audio_paths)}/{len(audio_paths)} audio files are valid")
            # é‡æ–°ç»„ç»‡batchï¼Œåªä¿ç•™æœ‰æ•ˆçš„æ ·æœ¬
            valid_batch_data = []
            for idx in valid_audio_indices:
                valid_batch_data.append((
                    ground_truth_terms_list[idx],
                    audio_paths[idx], 
                    chunk_texts[idx],
                    has_targets[idx]
                ))
            
            # å¦‚æœæœ‰æ•ˆæ ·æœ¬å¤ªå°‘ï¼Œè·³è¿‡è¿™ä¸ªbatch
            if len(valid_batch_data) < 2:
                print(f"[ERROR] Too few valid samples ({len(valid_batch_data)}), skipping batch")
                return torch.tensor(0.0, requires_grad=True).to(device)
            
            # é‡æ–°æå–æ•°æ®
            ground_truth_terms_list, audio_paths, chunk_texts, has_targets = zip(*valid_batch_data)
            ground_truth_terms_list = list(ground_truth_terms_list)
            audio_paths = list(audio_paths)
            chunk_texts = list(chunk_texts)
            has_targets = list(has_targets)
        
        # ç¼–ç éŸ³é¢‘
        audio_emb = raw_model.encode_audio(audio_paths)  # [B, proj_dim]
        
        # æ£€æŸ¥éŸ³é¢‘embedding
        if torch.isnan(audio_emb).any() or torch.isinf(audio_emb).any():
            print(f"[ERROR] NaN/Inf detected in audio embeddings after encoding!")
            return torch.tensor(0.0, requires_grad=True).to(device)
        
        if args.audio_text_loss_ratio > 0:
            text_emb = raw_model.encode_text(chunk_texts)    # [B, proj_dim]
            
            # æ£€æŸ¥æ–‡æœ¬embedding
            if torch.isnan(text_emb).any() or torch.isinf(text_emb).any():
                print(f"[ERROR] NaN/Inf detected in text embeddings!")
                print(f"[DEBUG] Text samples: {chunk_texts[:3]}...")
                return torch.tensor(0.0, requires_grad=True).to(device)
        else:
            text_emb = torch.zeros_like(audio_emb)
        
    except Exception as e:
        print(f"[ERROR] Failed to encode audio/text: {e}")
        import traceback
        traceback.print_exc()
        return torch.tensor(0.0, requires_grad=True).to(device)

    # === è®¡ç®—éŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”æŸå¤± ===
    # å®šä¹‰batch_sizeï¼Œç¡®ä¿åœ¨æ‰€æœ‰ä»£ç è·¯å¾„ä¸­éƒ½å¯ç”¨
    batch_size = len(audio_paths)  # ä½¿ç”¨å®é™…çš„batch sizeï¼ˆå¯èƒ½å·²ç»è¿‡æ»¤ï¼‰
    
    if args.audio_text_loss_ratio > 0:
        sim_matrix = (audio_emb @ text_emb.T) / temperature  # [B, B]
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(sim_matrix).any() or torch.isinf(sim_matrix).any():
            print(f"[ERROR] NaN/Inf in contrastive sim_matrix, skipping batch")
            return torch.tensor(0.0, requires_grad=True).to(device)
        
        # åˆ›å»ºæ­£æ ·æœ¬maskï¼ˆå¯¹è§’çº¿ä¸º1ï¼Œè¡¨ç¤ºéŸ³é¢‘iå’Œæ–‡æœ¬iæ˜¯æ­£æ ·æœ¬å¯¹ï¼‰
        labels = torch.arange(batch_size).to(device)
        
        # è®¡ç®—å¯¹ç§°çš„å¯¹æ¯”æŸå¤±
        try:
            loss_audio_to_text = F.cross_entropy(sim_matrix, labels)
            loss_text_to_audio = F.cross_entropy(sim_matrix.T, labels)
            
            if torch.isnan(loss_audio_to_text) or torch.isnan(loss_text_to_audio):
                print(f"[ERROR] NaN in contrastive cross_entropy, skipping batch")
                return torch.tensor(0.0, requires_grad=True).to(device)
            
            contrastive_loss = (loss_audio_to_text + loss_text_to_audio) / 2
        except Exception as e:
            print(f"[ERROR] Failed to compute contrastive loss: {e}")
            return torch.tensor(0.0, requires_grad=True).to(device)
    else:
        contrastive_loss = torch.tensor(0.0, device=device)

    # === è®¡ç®—éŸ³é¢‘-æœ¯è¯­å¯¹æ¯”æŸå¤± ===
    all_gt_terms = []
    audio_term_pairs = []  # (audio_idx, term_idx) æ­£æ ·æœ¬å¯¹
    
    for i, terms in enumerate(ground_truth_terms_list):
        for term in terms:
            if term and len(term.strip()) > 0:
                term_idx = len(all_gt_terms)
                all_gt_terms.append(term.strip())
                audio_term_pairs.append((i, term_idx))
    
    if len(all_gt_terms) > 0 and len(audio_term_pairs) > 0:
        # ç¼–ç æ‰€æœ‰çš„ground truth terms
        terms_emb = raw_model.encode_text(all_gt_terms)  # [N_terms, proj_dim]
        
        # è®¡ç®—éŸ³é¢‘-æœ¯è¯­ç›¸ä¼¼åº¦çŸ©é˜µ
        audio_term_sim = (audio_emb @ terms_emb.T) / temperature  # [B, N_terms]
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(audio_term_sim).any() or torch.isinf(audio_term_sim).any():
            print(f"[ERROR] NaN/Inf detected in audio_term_sim, skipping batch")
            return torch.tensor(0.0, requires_grad=True).to(device)
        
        # æ„å»ºæ­£æ ·æœ¬æ ‡ç­¾
        audio_term_labels = []
        for i in range(batch_size):
            # æ‰¾åˆ°audio iå¯¹åº”çš„æ‰€æœ‰positive term indices
            positive_terms = [term_idx for audio_idx, term_idx in audio_term_pairs if audio_idx == i]
            if positive_terms:
                # å¦‚æœæœ‰å¤šä¸ªæ­£æ ·æœ¬ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªä½œä¸ºä¸»è¦ç›®æ ‡
                import random
                audio_term_labels.append(random.choice(positive_terms))
            else:
                # å¦‚æœæ²¡æœ‰æ­£æ ·æœ¬ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
                audio_term_labels.append(-1)
        
        # è®¡ç®—æŸå¤±ï¼Œåªå¯¹æœ‰æ­£æ ·æœ¬çš„éŸ³é¢‘æ ·æœ¬è®¡ç®—
        valid_indices = [i for i, label in enumerate(audio_term_labels) if label >= 0]
        
        if len(valid_indices) > 0:
            valid_audio_term_sim = audio_term_sim[valid_indices]  # [valid_B, N_terms]
            valid_labels = torch.tensor([audio_term_labels[i] for i in valid_indices], device=device)
            
            # éŸ³é¢‘åˆ°æœ¯è¯­çš„æŸå¤±
            audio_to_term_loss = F.cross_entropy(valid_audio_term_sim, valid_labels)
            
            # æœ¯è¯­åˆ°éŸ³é¢‘çš„æŸå¤± - ä¸ºäº†å¯¹ç§°æ€§
            term_to_audio_sim = valid_audio_term_sim.T  # [N_terms, valid_B]
            # åˆ›å»ºåå‘æ ‡ç­¾ï¼šå¯¹äºæ¯ä¸ªæœ¯è¯­ï¼Œæ‰¾åˆ°å¯¹åº”çš„éŸ³é¢‘ç´¢å¼•
            term_audio_labels = []
            for term_idx in range(len(all_gt_terms)):
                # æ‰¾åˆ°term_idxå¯¹åº”çš„éŸ³é¢‘åœ¨valid_indicesä¸­çš„ä½ç½®
                corresponding_audios = [j for j, orig_i in enumerate(valid_indices) 
                                      if (orig_i, term_idx) in audio_term_pairs]
                if corresponding_audios:
                    term_audio_labels.append(corresponding_audios[0])  # é€‰æ‹©ç¬¬ä¸€ä¸ª
                else:
                    term_audio_labels.append(-1)
            
            # åªå¯¹æœ‰å¯¹åº”éŸ³é¢‘çš„æœ¯è¯­è®¡ç®—æŸå¤±
            valid_term_indices = [i for i, label in enumerate(term_audio_labels) if label >= 0]
            if len(valid_term_indices) > 0:
                valid_term_audio_sim = term_to_audio_sim[valid_term_indices]
                valid_term_labels = torch.tensor([term_audio_labels[i] for i in valid_term_indices], device=device)
                term_to_audio_loss = F.cross_entropy(valid_term_audio_sim, valid_term_labels)
            else:
                term_to_audio_loss = torch.tensor(0.0, device=device)
            
            # ç»„åˆéŸ³é¢‘-æœ¯è¯­æŸå¤±
            audio_term_loss = (audio_to_term_loss + term_to_audio_loss) / 2
        else:
            audio_term_loss = torch.tensor(0.0, device=device)
    else:
        audio_term_loss = torch.tensor(0.0, device=device)

    # === No-term margin loss (æ‹’ç­”èƒ½åŠ›) ===
    no_term_loss = torch.tensor(0.0, device=device)
    no_term_stats = {
        'no_term_count': 0,
        's_max_values': [],
        'margin_violations': 0,
        'avg_s_max': 0.0
    }

    if getattr(args, "use_no_term_loss", False) and getattr(args, "enable_no_term", True):
        # æ„é€  no-term æ©ç ï¼šhas_targets æ¥è‡ª batch
        has_term_tensor = torch.tensor([bool(x) for x in has_targets], device=device)
        no_term_mask = ~has_term_tensor
        no_term_count = no_term_mask.sum().item()
        no_term_stats['no_term_count'] = no_term_count

        if no_term_mask.any():
            # è·å–æ— æœ¯è¯­æ ·æœ¬çš„éŸ³é¢‘embeddings
            no_term_audio_emb = audio_emb[no_term_mask]  # [B_no_term, D]
            no_term_audio_emb_norm = F.normalize(no_term_audio_emb, p=2, dim=1)
            
            # ä½¿ç”¨FAISSå…¨åº“æ£€ç´¢è®¡ç®—s_maxï¼ˆå¦‚æœæœ‰FAISSç´¢å¼•ï¼‰
            if hn_ctx is not None and getattr(hn_ctx, "faiss_index", None) is not None:
                try:
                    # ä½¿ç”¨FAISSç´¢å¼•è¿›è¡Œå…¨åº“æ£€ç´¢
                    top_m = int(getattr(args, "no_term_top_m", 100))  # æ£€ç´¢top-Må€™é€‰
                    queries = no_term_audio_emb_norm.detach().to("cpu").float().numpy()
                    D, I = hn_ctx.faiss_index.search(queries, top_m)  # D: similarity for IP / distance for L2
                    
                    # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    if hn_ctx.metric == 'l2':
                        # L2è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°ç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
                        sim_scores = -torch.tensor(D, device=device, dtype=torch.float32)
                    else:
                        # IPåˆ†æ•°ç›´æ¥ä½œä¸ºç›¸ä¼¼åº¦
                        sim_scores = torch.tensor(D, device=device, dtype=torch.float32)
                    
                    # å–æ¯ä¸ªno-termæ ·æœ¬çš„æœ€å¤§ç›¸ä¼¼åº¦
                    s_max = sim_scores.max(dim=1).values  # [B_no_term]
                    no_term_stats['s_max_values'] = s_max.detach().cpu().tolist()
                    no_term_stats['avg_s_max'] = s_max.mean().item()
                    
                    # è®¡ç®—margin loss
                    margin = float(getattr(args, "no_term_margin", 0.15))
                    margin_violations = (s_max > margin).sum().item()
                    no_term_stats['margin_violations'] = margin_violations
                    no_term_loss = F.relu(s_max - margin).mean()
                    
                except Exception as e:
                    print(f"[WARN] FAISS no-term loss failed, falling back to batch terms: {e}")
                    # å›é€€åˆ°batchå†…æœ¯è¯­çš„æ–¹å¼
                    if 'terms_emb' in locals() and terms_emb is not None and terms_emb.numel() > 0:
                        t_norm = F.normalize(terms_emb, p=2, dim=1)
                        sim_all = no_term_audio_emb_norm @ t_norm.T  # [B_no_term, N_terms_in_batch]
                        s_max = sim_all.max(dim=1).values
                        no_term_stats['s_max_values'] = s_max.detach().cpu().tolist()
                        no_term_stats['avg_s_max'] = s_max.mean().item()
                        margin = float(getattr(args, "no_term_margin", 0.15))
                        margin_violations = (s_max > margin).sum().item()
                        no_term_stats['margin_violations'] = margin_violations
                        no_term_loss = F.relu(s_max - margin).mean()
            
            # å¦‚æœæ²¡æœ‰FAISSç´¢å¼•ï¼Œä½¿ç”¨batchå†…æœ¯è¯­ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            elif 'terms_emb' in locals() and terms_emb is not None and terms_emb.numel() > 0:
                t_norm = F.normalize(terms_emb, p=2, dim=1)
                sim_all = no_term_audio_emb_norm @ t_norm.T  # [B_no_term, N_terms_in_batch]
                s_max = sim_all.max(dim=1).values
                no_term_stats['s_max_values'] = s_max.detach().cpu().tolist()
                no_term_stats['avg_s_max'] = s_max.mean().item()
                margin = float(getattr(args, "no_term_margin", 0.15))
                margin_violations = (s_max > margin).sum().item()
                no_term_stats['margin_violations'] = margin_violations
                no_term_loss = F.relu(s_max - margin).mean()

    # === Hard Negative Mining Loss ===
    hard_neg_loss = torch.tensor(0.0, device=device)
    if getattr(args, "enable_hard_neg", False) and hn_ctx is not None and len(all_gt_terms) > 0 and len(audio_term_pairs) > 0:
        try:
            # Normalize audio embeddings for cosine/IP stability
            audio_emb_norm = torch.nn.functional.normalize(audio_emb, p=2, dim=1)

            # Build one positive text embedding per sample (detach & normalize)
            sample_pos_emb = [None] * batch_size
            seen_pos = set()
            for (a_i, t_idx) in audio_term_pairs:
                if a_i not in seen_pos:
                    pos_e = terms_emb[t_idx].detach()
                    pos_e = torch.nn.functional.normalize(pos_e, p=2, dim=0)
                    sample_pos_emb[a_i] = pos_e
                    seen_pos.add(a_i)

            k = int(getattr(args, "hard_neg_k", 10))
            cand = int(getattr(args, "hard_neg_candidates", max(50, 5 * k)))
            margin = float(getattr(args, "hard_neg_margin", 0.1))

            losses = []

            # Case A: FAISS index mode (preferred for large glossary)
            if getattr(hn_ctx, "faiss_index", None) is not None:
                # Prepare query matrix on CPU float32 (FAISS expects np.float32)
                queries = audio_emb_norm.detach().to("cpu").float().numpy()
                # Perform ANN search
                D, I = hn_ctx.faiss_index.search(queries, cand)  # D: similarity for IP / distance for L2

                # Per-sample hinge over top-k after filtering GT terms
                for i in range(batch_size):
                    pos_emb_i = sample_pos_emb[i]
                    if pos_emb_i is None:
                        continue

                    # Filter out GT indices (mapped through term2idx)
                    gt_terms_i = set(t for t in ground_truth_terms_list[i] if isinstance(t, str))
                    gt_idx_in_ctx = set(hn_ctx.term2idx[t] for t in gt_terms_i if t in hn_ctx.term2idx)

                    if I.shape[0] == 0:
                        continue
                    cand_idx = I[i].tolist()
                    cand_scores = D[i].tolist()

                    # Keep only non-GT candidates, then take top-k
                    filtered = [(idx, score) for idx, score in zip(cand_idx, cand_scores) if idx not in gt_idx_in_ctx and idx >= 0]
                    if not filtered:
                        continue
                    filtered = filtered[:k] if len(filtered) > k else filtered

                    # Compute sim_pos on torch
                    sim_pos = torch.sum(audio_emb_norm[i] * pos_emb_i)

                    # sim_neg comes directly from FAISS results:
                    if hn_ctx.metric == 'l2':
                        sim_negs_vals = [-float(score) for _, score in filtered]
                    else:
                        sim_negs_vals = [float(score) for _, score in filtered]

                    sim_negs = torch.tensor(sim_negs_vals, device=device, dtype=sim_pos.dtype)
                    loss_i = torch.relu(margin + sim_negs - sim_pos).mean()
                    losses.append(loss_i)

                if len(losses) > 0:
                    hard_neg_loss = torch.stack(losses).mean()

            # Case B: In-memory tensor mode (small bank fallback)
            elif getattr(hn_ctx, "emb_tensor", None) is not None:
                hn_emb_tensor = hn_ctx.emb_tensor.to(device)
                sim_full = audio_emb_norm @ hn_emb_tensor.T
                if k > 0 and sim_full.shape[1] > 0:
                    for i in range(batch_size):
                        pos_emb_i = sample_pos_emb[i]
                        if pos_emb_i is None:
                            continue
                        gt_terms_i = set(t for t in ground_truth_terms_list[i] if isinstance(t, str))
                        gt_idx_in_ctx = [hn_ctx.term2idx[t] for t in gt_terms_i if t in hn_ctx.term2idx]
                        take_n = min(sim_full.shape[1], max(k, cand) + max(0, len(gt_idx_in_ctx)))
                        if take_n == 0:
                            continue
                        top_vals, top_idx = torch.topk(sim_full[i], k=take_n, largest=True)
                        if len(gt_idx_in_ctx) > 0:
                            mask = ~torch.isin(top_idx, torch.tensor(gt_idx_in_ctx, device=top_idx.device))
                            top_idx = top_idx[mask]
                            top_vals = top_vals[mask]
                        if top_idx.numel() == 0:
                            continue
                        if top_idx.numel() > k:
                            top_vals = top_vals[:k]
                        sim_pos = torch.sum(audio_emb_norm[i] * pos_emb_i)
                        loss_i = torch.relu(margin + top_vals - sim_pos).mean()
                        losses.append(loss_i)
                    if len(losses) > 0:
                        hard_neg_loss = torch.stack(losses).mean()
        except Exception as e:
            print(f"[WARN] Hard-negative mining failed: {e}")
            hard_neg_loss = torch.tensor(0.0, device=device)

    # === ç»„åˆæ€»æŸå¤± ===
    total_loss = args.audio_text_loss_ratio * contrastive_loss + args.audio_term_loss_ratio * audio_term_loss
    
    # æ·»åŠ hard negativeæŸå¤±
    if getattr(args, "enable_hard_neg", False):
        hn_weight = float(getattr(args, "hard_neg_weight", 0.2))
        total_loss = total_loss + hn_weight * hard_neg_loss
    
    # æ·»åŠ æ— æœ¯è¯­marginæŸå¤±
    if getattr(args, "use_no_term_loss", False) and getattr(args, "enable_no_term", True):
        lambda_no_term = float(getattr(args, "lambda_no_term", 0.5))
        total_loss = total_loss + lambda_no_term * no_term_loss
    
    # æœ€ç»ˆæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
    if torch.isnan(total_loss) or torch.isinf(total_loss):
        print(f"[ERROR] NaN/Inf total loss detected, skipping batch")
        return torch.tensor(0.0, requires_grad=True).to(device)
    
    # è¿”å›æŸå¤±å’Œç»Ÿè®¡ä¿¡æ¯
    return total_loss, no_term_stats


# === Helper to build hard-neg context ===
def build_hardneg_ctx(raw_model, source_terms, device, batch_size=2048):
    """
    Legacy helper retained for small-bank mode: encodes `source_terms` into an in-memory
    normalized tensor. For large glossary, prefer FAISS index mode (see epoch setup).
    """
    if not source_terms:
        return None

    cleaned = []
    seen = set()
    for t in source_terms:
        if not isinstance(t, str):
            continue
        tl = t.strip().lower()
        if len(tl) < 3:
            continue
        if tl in seen:
            continue
        seen.add(tl)
        cleaned.append(tl)

    if len(cleaned) == 0:
        return None

    text_emb = encode_texts_in_batches(raw_model, cleaned, device=device)
    if text_emb is None or text_emb.numel() == 0:
        return None

    text_emb = text_emb.to(device)
    text_emb = torch.nn.functional.normalize(text_emb, p=2, dim=1)
    term2idx = {t: i for i, t in enumerate(cleaned)}
    return HardNegContext(terms=cleaned, term2idx=term2idx, emb_tensor=text_emb, faiss_index=None, metric='ip')


def extract_all_used_terms(dataset):
    """æå–æ•°æ®é›†ä¸­æ‰€æœ‰ä½¿ç”¨çš„æœ¯è¯­"""
    used_terms = set()
    processed_samples = 0
    valid_samples = 0
    
    for i, sample in enumerate(dataset):
        if sample is None:
            continue
        processed_samples += 1
        ground_truth_terms, audio_path, chunk_text, has_target = sample
        
        if has_target and ground_truth_terms:
            valid_samples += 1
            for t in ground_truth_terms:
                if isinstance(t, str) and len(t.strip()) > 0:
                    used_terms.add(t.lower())
            
            # è°ƒè¯•å‰å‡ ä¸ªæ ·æœ¬
            if i < 5:
                print(f"[DEBUG] extract_all_used_terms - Sample {i}: ground_truth_terms={ground_truth_terms}, chunk_text='{chunk_text}'")
    
    print(f"[DEBUG] extract_all_used_terms - Processed {processed_samples} samples, {valid_samples} valid samples, {len(used_terms)} unique terms")
    return list(used_terms)


def evaluate_topk_recall(model, retriever, dataset, device, top_ks=(5, 10, 20), max_eval=1000, field="term", train_terms=None, show_missed_terms=True, no_term_margin=0.15, enable_no_term=False, filter_no_term=True):
    """è¯„ä¼°top-kå¬å›ç‡ï¼ŒåŒ…æ‹¬no-termæ ·æœ¬çš„æ‹’ç­”èƒ½åŠ›è¯„ä¼°"""
    model.eval()
    print(f"[INFO] Evaluation no-term samples enabled: {enable_no_term}")
    
    # ç”¨äºå­˜å‚¨sample-levelå¬å›ç‡
    recall_dict = {k: [] for k in top_ks}
    
    # ç”¨äºå­˜å‚¨æ‰€æœ‰GTæœ¯è¯­å’Œå¯¹åº”çš„æ£€ç´¢ç»“æœï¼ˆç”¨äºåˆ†ææœªå‘½ä¸­æœ¯è¯­ï¼‰
    all_gt_terms_with_retrieval = {k: [] for k in top_ks}  # æ¯ä¸ªå…ƒç´ æ˜¯ (gt_term, is_retrieved, sample_info)
    sample_info_for_debug = []  # ç”¨äºè°ƒè¯•è¾“å‡º
    
    # ç”¨äºå­˜å‚¨no-termæ ·æœ¬çš„æ‹’ç­”èƒ½åŠ›è¯„ä¼°
    no_term_stats = {k: {'total': 0, 'correct_rejections': 0, 'max_sims': [], 'violations': 0} for k in top_ks}

    # === é‡å»ºç´¢å¼• ===
    text_terms = [term['term'] for term in retriever.term_list]
    print(f'[DEBUG] Building index with {len(text_terms)} terms')
    
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    text_emb = encode_texts_in_batches(raw_model, text_terms, device=device)
    
    retriever.index.reset()
    retriever.index.add(text_emb)
    print(f'[DEBUG] Index built with {retriever.index.ntotal} vectors')

    print(f"[INFO] Dataset size: {len(dataset)}")
    import random
    random.seed(42)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°
    eval_indices = random.sample(range(len(dataset)), min(max_eval, len(dataset)))
    
    # åˆ†ç¦»æœ‰æœ¯è¯­å’Œæ— æœ¯è¯­çš„æ ·æœ¬
    term_samples = []
    term_indices = []
    no_term_samples = []
    no_term_indices = []
    
    for i in eval_indices:
        sample = dataset[i]
        if sample is not None:
            ground_truth_terms, audio_path, chunk_text, has_target = sample
            if has_target and ground_truth_terms:  # æœ‰æœ¯è¯­çš„æ ·æœ¬
                term_samples.append(sample)
                term_indices.append(i)
            elif (not has_target or not ground_truth_terms) and enable_no_term and not filter_no_term:  # æ— æœ¯è¯­çš„æ ·æœ¬ï¼ˆä»…åœ¨å¯ç”¨ä¸”ä¸è¿‡æ»¤æ—¶è¯„ä¼°ï¼‰
                no_term_samples.append(sample)
                no_term_indices.append(i)

    print(f"[INFO] Selected {len(eval_indices)} samples randomly:")
    print(f"[INFO]   - {len(term_samples)} term samples (for recall evaluation)")
    print(f"[INFO]   - {len(no_term_samples)} no-term samples (for rejection evaluation)")
    
    # === å¤„ç†æœ‰æœ¯è¯­çš„æ ·æœ¬ ===
    if len(term_samples) > 0:
        # ä½¿ç”¨term chunkéŸ³é¢‘è¿›è¡Œç¼–ç ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
        term_audio_paths = [sample[1] for sample in term_samples]  # term_chunk_audio paths
        
        # éªŒè¯éŸ³é¢‘æ–‡ä»¶
        print(f"[DEBUG] Validating {len(term_audio_paths)} term audio files for evaluation...")
        valid_term_audio_paths, valid_term_audio_indices = validate_audio_batch(term_audio_paths, verbose=False)
        
        if len(valid_term_audio_paths) != len(term_audio_paths):
            print(f"[WARN] Term evaluation: Only {len(valid_term_audio_paths)}/{len(term_audio_paths)} audio files are valid")
            # è¿‡æ»¤æ‰æ— æ•ˆçš„æ ·æœ¬
            term_samples = [term_samples[i] for i in valid_term_audio_indices]
            term_indices = [term_indices[i] for i in valid_term_audio_indices]
            term_audio_paths = valid_term_audio_paths
        
        if len(term_audio_paths) > 0:
            print(f"[DEBUG] Encoding {len(term_audio_paths)} valid term audio files...")
            term_audio_embs = encode_audios_in_batches(raw_model, term_audio_paths, batch_size=1000, device=device).numpy()
        else:
            term_audio_embs = np.array([])
    else:
        term_audio_embs = np.array([])
    
    # === è¯„ä¼°æœ‰æœ¯è¯­çš„æ ·æœ¬ ===
    if len(term_samples) > 0 and term_audio_embs.size > 0:
        print(f"[INFO] Evaluating {len(term_samples)} term samples for recall...")
        for j, (i, sample) in enumerate(zip(term_indices, term_samples)):
            ground_truth_terms, audio_path, chunk_text, has_target = sample
            audio_emb = term_audio_embs[j:j+1]  # shape: [1, 512]
            gt_terms = [t.lower() for t in ground_truth_terms]  # ä½¿ç”¨term_chunk_audio_ground_truth_terms

            # å¯¹æ¯ä¸ªtop_kè¿›è¡Œæ£€ç´¢
            retrieval_results = {}
            for top_k in top_ks:
                D, I = retriever.index.search(audio_emb, top_k)
                retrieved_terms = [retriever.term_list[idx][field].lower() for idx in I[0]]
                retrieval_results[top_k] = (D[0], I[0], retrieved_terms)
                
                # è®¡ç®—sample-levelå¬å›ç‡
                matched = sum(gt_term in retrieved_terms for gt_term in gt_terms)
                sample_recall = matched / len(gt_terms) if gt_terms else 0.0
                recall_dict[top_k].append(sample_recall)
                
                # åŒæ—¶æ”¶é›†term-levelä¿¡æ¯ç”¨äºåˆ†ææœªå‘½ä¸­æœ¯è¯­
                for gt_term in gt_terms:
                    is_retrieved = gt_term in retrieved_terms
                    sample_info = {
                        'sample_idx': i,
                        'audio_path': audio_path,
                        'chunk_text': chunk_text,
                        'all_gt_terms': gt_terms,
                        'retrieved_terms': retrieved_terms  # æ·»åŠ æ£€ç´¢åˆ°çš„å€™é€‰æœ¯è¯­
                    }
                    all_gt_terms_with_retrieval[top_k].append((gt_term, is_retrieved, sample_info))
    
    # è®¡ç®—sample-levelå’Œterm-levelå¬å›ç‡
    for top_k in top_ks:
        print(f"\n=== Evaluation Results for Top-{top_k} ===")
        
        # === Termæ ·æœ¬å¬å›ç‡è¯„ä¼° ===
        if len(recall_dict[top_k]) > 0:
            # Sample-levelå¹³å‡å¬å›ç‡
            avg_recall = sum(recall_dict[top_k]) / len(recall_dict[top_k])
            print(f"[EVAL] Term Samples - Sample-level Average Recall@{top_k}: {avg_recall:.2%} ({len(recall_dict[top_k])} samples)")
            
            # Term-levelå¾®å¹³å‡å¬å›ç‡
            term_retrieval_pairs = all_gt_terms_with_retrieval[top_k]
            total_terms = len(term_retrieval_pairs)
            hit_terms = sum(1 for _, is_retrieved, _ in term_retrieval_pairs if is_retrieved)
            term_micro_avg_recall = hit_terms / total_terms if total_terms > 0 else 0.0
            print(f"[EVAL] Term Samples - Term-level Micro-Average Recall@{top_k}: {term_micro_avg_recall:.2%} ({hit_terms}/{total_terms} terms)")
        else:
            print(f"[EVAL] Term Samples - No term samples evaluated for Recall@{top_k}")

    model.train()
    return recall_dict


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch_size', type=int, default=256)  # å¯èƒ½éœ€è¦é€‚å½“è°ƒæ•´
    parser.add_argument('--lr', type=float, default=5e-5)  
    parser.add_argument('--patience', type=int, default=2)  
    parser.add_argument('--unfreeze_layers', type=int, default=0, 
                       help="Number of last layers to unfreeze in both encoders (default: 0, all frozen)")
    parser.add_argument('--train_samples_path', type=str, 
                       default="data/xl_term_level_chunks_merged.json",
                       help="Path to term-level chunk samples")
    parser.add_argument('--test_samples_path', type=str, default=None,
                       help="Path to separate test samples. If not provided, will use train_ratio to split training data")
    parser.add_argument('--train_ratio', type=float, default=0.99,
                       help="Ratio of samples to use for training (default: 0.99, only used when test_samples_path is not provided)")
    parser.add_argument('--glossary_path', type=str, default="data/terms/glossary_filtered.json")
    parser.add_argument('--save_path', type=str, default="data/qwen2_audio_term_level.pt")
    parser.add_argument('--best_model_path', type=str, default=None,
                       help="Path to best model checkpoint (.pt file) to continue training from")
    parser.add_argument('--enable_full_eval', action='store_true', 
                       help="Enable full evaluation with complete glossary at the end of training")
    parser.add_argument('--full_eval_every_n_epochs', type=int, default=5,
                       help="Run full evaluation every N epochs (requires --enable_full_eval)")
    parser.add_argument('--audio_text_loss_ratio', type=float, default=0.3,
                       help="Weight for audio-text contrastive loss (default: 0.3)")
    parser.add_argument('--audio_term_loss_ratio', type=float, default=0.7,
                       help="Weight for audio-term contrastive loss (default: 0.7)")

    # æ‹’ç­”ç›¸å…³å‚æ•°
    parser.add_argument('--enable_no_term', action='store_true', default=False,
                        help="Enable no-term samples in dataset and evaluation (default: False)")
    parser.add_argument('--filter_no_term', action='store_true', default=True,
                        help="Filter out no-term samples from dataset (default: True)")
    parser.add_argument('--use_no_term_loss', action='store_true', default=False,
                        help="Enable max-sim margin loss for no-term samples (default: False)")
    parser.add_argument('--no_term_margin', type=float, default=0.15,
                        help="Margin m for max-sim loss: relu(s_max - m)")
    parser.add_argument('--lambda_no_term', type=float, default=0.5,
                        help="Weight for no-term margin loss")
    parser.add_argument('--no_term_top_m', type=int, default=100,
                        help="Top-M candidates to retrieve from FAISS for no-term loss computation")

    # Hard negative miningç›¸å…³å‚æ•°
    parser.add_argument('--enable_hard_neg', action='store_true', default=False,
                        help="Enable hard negative mining against top-k retrieved non-GT terms (default: False)")
    parser.add_argument('--hard_neg_source', type=str, default='used', choices=['used', 'glossary'],
                        help="Source corpus for mining hard negatives: 'used' (train+test used terms) or 'glossary' (default: used)")
    parser.add_argument('--enable_glossary_hard_neg', action='store_true', default=False,
                        help="Enable glossary-based hard negative mining with FAISS index (default: False, use used terms only)")
    parser.add_argument('--hard_neg_k', type=int, default=10,
                        help="Number of hard negatives per sample (top-k)")
    parser.add_argument('--hard_neg_weight', type=float, default=0.2,
                        help="Weight for hard negative hinge loss")
    parser.add_argument('--hard_neg_margin', type=float, default=0.1,
                        help="Margin for hinge loss: max(0, margin + sim_neg - sim_pos)")

    parser.add_argument('--hard_neg_index_path', type=str, default=None,
                        help="Path to FAISS ANN index for the full glossary (IVF/HNSW/Flat). If set, enables large-bank hard negatives.")
    parser.add_argument('--hard_neg_term2idx_path', type=str, default=None,
                        help="Path to JSON mapping term_string -> int_index that matches the FAISS index order.")
    parser.add_argument('--hard_neg_candidates', type=int, default=100,
                        help="Number of ANN candidates to fetch before filtering GT (then take top-k).")
    parser.add_argument('--hard_neg_nprobe', type=int, default=16,
                        help="FAISS nprobe / efSearch parameter for IVF/HNSW-like indices.")
    parser.add_argument('--hard_neg_metric', type=str, default='ip', choices=['ip', 'l2'],
                        help="Similarity metric of the FAISS index: 'ip' for inner product (recommended with normalized vectors) or 'l2'.")

    # GPUè®¾å¤‡é€‰æ‹©
    parser.add_argument('--gpu_ids', type=str, default=None,
                        help="GPU IDs to use (e.g., '0,1' or '2'). If not specified, use all available GPUs.")
    
    # Qwen2-Audioæ¨¡å‹å‚æ•°
    parser.add_argument('--model_name', type=str, default="Qwen/Qwen2-Audio-7B-Instruct",
                        help="Qwen2-Audio model name or path")

    args = parser.parse_args()

    # å¤„ç†no-termé…ç½®é€»è¾‘
    # å¦‚æœenable_no_term=Falseï¼Œåˆ™è‡ªåŠ¨è®¾ç½®filter_no_term=True
    if not args.enable_no_term:
        args.filter_no_term = True
    
    print(f"[DEBUG] audio_text_loss_ratio={args.audio_text_loss_ratio}, audio_term_loss_ratio={args.audio_term_loss_ratio}")
    print(f"[DEBUG] enable_no_term={args.enable_no_term}, filter_no_term={args.filter_no_term}, use_no_term_loss={args.use_no_term_loss}")
    print(f"[DEBUG] enable_hard_neg={args.enable_hard_neg}, enable_glossary_hard_neg={args.enable_glossary_hard_neg}")

    # GPUè®¾å¤‡è®¾ç½®
    if args.gpu_ids is not None:
        # è®¾ç½®CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_ids
        gpu_list = [int(x.strip()) for x in args.gpu_ids.split(',') if x.strip().isdigit()]
        print(f"[INFO] Setting CUDA_VISIBLE_DEVICES={args.gpu_ids}")
        print(f"[INFO] Will use GPUs: {gpu_list}")
        
        # é‡æ–°æ£€æŸ¥CUDAè®¾å¤‡
        if torch.cuda.is_available():
            available_gpus = torch.cuda.device_count()
            print(f"[INFO] Available GPUs after setting CUDA_VISIBLE_DEVICES: {available_gpus}")
            device = torch.device("cuda")
        else:
            print("[WARNING] CUDA not available after setting CUDA_VISIBLE_DEVICES, falling back to CPU")
            device = torch.device("cpu")
    else:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if device == "cuda":
            available_gpus = torch.cuda.device_count()
            print(f"[INFO] Using all available GPUs: {available_gpus}")
        
    print(f"[INFO] Using device: {device}")

    # === æ¨¡å‹åˆå§‹åŒ– ===
    print(f"[INFO] Initializing Qwen2-Audio model: {args.model_name}")
    
    speech_encoder = Qwen2AudioSpeechEncoder(
        model_name=args.model_name, device=device
    )

    text_encoder = Qwen2AudioTextEncoder(
        model_name=args.model_name, device=device
    )

    model = ContrastiveQwen2AudioModel(
        speech_encoder, text_encoder, 
        hidden_dim=4096,  # Qwen2-Audio typical hidden size
        proj_dim=512,
        unfreeze_layers=args.unfreeze_layers
    ).to(device)
    
    # å¦‚æœæä¾›äº†best modelè·¯å¾„ï¼ŒåŠ è½½é¢„è®­ç»ƒæƒé‡
    if args.best_model_path and os.path.exists(args.best_model_path):
        print(f"[INFO] Loading pre-trained weights from {args.best_model_path}")
        try:
            state_dict = torch.load(args.best_model_path, map_location=device)
            
            # å¤„ç† DataParallel çš„æƒ…å†µ
            if list(state_dict.keys())[0].startswith('module.'):
                # ç§»é™¤ 'module.' å‰ç¼€
                new_state_dict = {}
                for k, v in state_dict.items():
                    new_state_dict[k[7:]] = v  # ç§»é™¤ 'module.' (7ä¸ªå­—ç¬¦)
                state_dict = new_state_dict
            
            model.load_state_dict(state_dict, strict=False)  # ä½¿ç”¨strict=Falseä»¥é˜²æ¨¡å‹ç»“æ„ç¨æœ‰ä¸åŒ
            print(f"[INFO] Successfully loaded pre-trained weights")
        except Exception as e:
            print(f"[WARNING] Failed to load pre-trained weights: {e}")
            print(f"[INFO] Continuing with random initialization")
    else:
        print(f"[INFO] No pre-trained weights provided, using random initialization")
    
    # ä¸ºä¸åŒçš„å‚æ•°ç»„è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    encoder_params = []
    projection_params = []
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if 'proj_' in name:
                projection_params.append(param)
            else:
                encoder_params.append(param)
    
    # æŠ•å½±å±‚ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡ï¼Œç¼–ç å™¨ä½¿ç”¨è¾ƒä½çš„å­¦ä¹ ç‡
    optimizer = torch.optim.AdamW([
        {'params': encoder_params, 'lr': args.lr},
        {'params': projection_params, 'lr': args.lr * 10}  # æŠ•å½±å±‚å­¦ä¹ ç‡æ›´é«˜
    ])
    
    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2
    )
    
    # è‡ªåŠ¨å¤šGPUåŒ…è£…
    if torch.cuda.device_count() > 1:
        print(f"[INFO] ğŸš€ Detected {torch.cuda.device_count()} GPUs, wrapping with DataParallel")
        if args.gpu_ids is not None:
            # ä½¿ç”¨æŒ‡å®šçš„GPUï¼ˆå·²é€šè¿‡CUDA_VISIBLE_DEVICESè®¾ç½®ï¼Œæ‰€ä»¥è¿™é‡Œä½¿ç”¨è¿ç»­ç¼–å·ï¼‰
            available_gpus = list(range(torch.cuda.device_count()))
            print(f"[INFO] Using specified GPUs (remapped): {available_gpus} (original: {args.gpu_ids})")
        else:
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
            available_gpus = list(range(torch.cuda.device_count()))
            print(f"[INFO] Using all available GPUs: {available_gpus}")
        
        model = torch.nn.DataParallel(model, device_ids=available_gpus)
        print(f"[INFO] âœ… DataParallel enabled on GPUs: {available_gpus}")
    else:
        if args.gpu_ids is not None:
            print(f"[INFO] Single GPU mode (using specified GPU: {args.gpu_ids})")
        else:
            print(f"[INFO] Single GPU mode")

    # === åŠ è½½æ•°æ®é›† ===
    print(f"[INFO] Loading term-level dataset from {args.train_samples_path}")
    if args.test_samples_path:
        print(f"[INFO] Using separate test dataset: {args.test_samples_path}")
        train_dataset = TermLevelDataset(args.train_samples_path, split="train", train_ratio=1.0, enable_no_term=args.enable_no_term, filter_no_term=args.filter_no_term)  # ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®
        test_dataset = TermLevelDataset(None, split="test", train_ratio=args.train_ratio, test_path=args.test_samples_path, enable_no_term=args.enable_no_term, filter_no_term=args.filter_no_term)
    else:
        print(f"[INFO] Using train ratio: {args.train_ratio:.1%} train, {1-args.train_ratio:.1%} test")
        train_dataset = TermLevelDataset(args.train_samples_path, split="train", train_ratio=args.train_ratio, enable_no_term=args.enable_no_term, filter_no_term=args.filter_no_term)
        test_dataset = TermLevelDataset(args.train_samples_path, split="test", train_ratio=args.train_ratio, enable_no_term=args.enable_no_term, filter_no_term=args.filter_no_term)
    
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, collate_fn=lambda x: x)
    
    print(f"[INFO] Training samples: {len(train_dataset)}")
    print(f"[INFO] Test samples: {len(test_dataset)}")
    
    # === æ„å»ºæœ¯è¯­è¯è¡¨ç”¨äºè¯„ä¼° ===
    print(f"[INFO] Building term vocabulary from training + test data...")
    used_terms_train = extract_all_used_terms(train_dataset)
    used_terms_test = extract_all_used_terms(test_dataset)

    # åˆå¹¶ã€å»é‡å¹¶å°å†™
    used_terms = list(set(t.lower() for t in (used_terms_train + used_terms_test)))
    print(f"[INFO] Found {len(used_terms)} unique terms")
    
    # === åˆå§‹åŒ– retriever ç”¨äºè¯„ä¼° ===
    retriever = Retriever(enable_fusion=True, device=device)
    raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    retriever.model = raw_model
    retriever.index = faiss.IndexFlatL2(512)  # åˆå§‹åŒ–ç©ºç´¢å¼•
    retriever.term_list = [{'term': t} for t in used_terms]

    # æ‰“å°æ¨¡å‹å‚æ•°ä¿¡æ¯
    print("[DEBUG] Trainable parameters:")
    trainable_params = 0
    encoder_params_count = 0
    projection_params_count = 0
    
    for name, param in model.named_parameters():
        if param.requires_grad:
            if 'proj_' in name:
                projection_params_count += param.numel()
                print(f" - [PROJ] {name}: {param.shape}")
            else:
                encoder_params_count += param.numel()
                print(f" - [ENC] {name}: {param.shape}")
            trainable_params += param.numel()

    frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    print(f"[INFO] Frozen parameters: {frozen:,} / {total:,} ({frozen / total:.2%})")
    print(f"[INFO] Trainable parameters: {trainable_params:,}")
    print(f"[INFO]   - Encoder parameters: {encoder_params_count:,} ({encoder_params_count/trainable_params:.1%})")
    print(f"[INFO]   - Projection parameters: {projection_params_count:,} ({projection_params_count/trainable_params:.1%})")
    print(f"[INFO] Training with {len(train_dataset)} term-level chunk samples")
    print(f"[INFO] Unfrozen layers: {args.unfreeze_layers}")

    # === å‡†å¤‡hard negative miningä¸Šä¸‹æ–‡ ===
    hardneg_source_terms = None
    faiss_index = None
    term2idx_map = {}
    
    if args.enable_hard_neg:
        if args.enable_glossary_hard_neg and args.hard_neg_source == 'glossary':
            try:
                hardneg_source_terms = load_glossary_terms(args.glossary_path)
                print(f"[INFO] Hard-neg source: glossary with {len(hardneg_source_terms)} terms")
            except Exception as e:
                print(f"[WARN] Failed to load glossary for hard negs: {e}. Falling back to used terms.")
                hardneg_source_terms = used_terms
        else:
            hardneg_source_terms = used_terms
            print(f"[INFO] Hard-neg source: used terms ({len(hardneg_source_terms)} terms)")
        
        # åŠ è½½FAISSç´¢å¼•ï¼ˆä»…å½“å¯ç”¨glossary hard negæ—¶ï¼‰
        if args.enable_glossary_hard_neg and args.hard_neg_index_path:
            try:
                print(f"[INFO] Loading FAISS index from: {args.hard_neg_index_path}")
                faiss_index = faiss.read_index(args.hard_neg_index_path)
                # Try to set nprobe/efSearch if available
                try:
                    if hasattr(faiss_index, 'nprobe'):
                        faiss_index.nprobe = int(args.hard_neg_nprobe)
                        print(f"[INFO] Set index.nprobe = {faiss_index.nprobe}")
                except Exception as e:
                    print(f"[WARN] Could not set nprobe: {e}")
                term2idx_map = load_term2idx_json(args.hard_neg_term2idx_path)
                print(f"[INFO] term2idx loaded: {len(term2idx_map)} entries")
            except Exception as e:
                print(f"[WARN] Failed to load FAISS index ({args.hard_neg_index_path}): {e}")
                faiss_index = None
    else:
        print(f"[INFO] Hard negative mining disabled")

    best_recall = 0.0
    no_improve_epochs = 0

    for epoch in range(args.epochs):
        # æ„å»ºhard negativeä¸Šä¸‹æ–‡ï¼ˆæ¯ä¸ªepochåˆ·æ–°ï¼‰
        hn_ctx = None
        if args.enable_hard_neg:
            # ä¼˜å…ˆä½¿ç”¨FAISSç´¢å¼•æ¨¡å¼ï¼ˆä»…å½“å¯ç”¨glossary hard negæ—¶ï¼‰
            if args.enable_glossary_hard_neg and faiss_index is not None and term2idx_map:
                hn_ctx = HardNegContext(terms=None, term2idx=term2idx_map, emb_tensor=None,
                                        faiss_index=faiss_index, metric=getattr(args, "hard_neg_metric", "ip"))
                print(f"[INFO] Hard-neg (FAISS) ready: {len(term2idx_map)} term ids, metric={hn_ctx.metric}, nprobe={getattr(faiss_index, 'nprobe', 'N/A')}")
            elif hardneg_source_terms:
                raw_model = model.module if isinstance(model, torch.nn.DataParallel) else model
                hn_ctx = build_hardneg_ctx(raw_model, hardneg_source_terms, device=device)
                if hn_ctx is not None and hn_ctx.emb_tensor is not None:
                    print(f"[INFO] Hard-neg (in-memory) built: {len(hn_ctx.terms)} terms, emb_tensor: {tuple(hn_ctx.emb_tensor.shape)}")
                else:
                    print(f"[WARN] Hard-neg context not available this epoch")
        
        model.train()
        total_loss = 0.0

        # è®­ç»ƒå¾ªç¯
        epoch_no_term_stats = {
            'total_no_term_samples': 0,
            'total_violations': 0,
            'avg_s_max_sum': 0.0,
            'batch_count': 0
        }
        
        for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f"[Epoch {epoch+1}/{args.epochs}]")):
            result = train_step(model, batch, device, args, hn_ctx=hn_ctx)
            
            # å¤„ç†è¿”å›ç»“æœï¼ˆå¯èƒ½æ˜¯å•ä¸ªlossæˆ–(loss, stats)å…ƒç»„ï¼‰
            if isinstance(result, tuple):
                loss, no_term_batch_stats = result
                # ç´¯ç§¯no-termç»Ÿè®¡ä¿¡æ¯
                if no_term_batch_stats['no_term_count'] > 0:
                    epoch_no_term_stats['total_no_term_samples'] += no_term_batch_stats['no_term_count']
                    epoch_no_term_stats['total_violations'] += no_term_batch_stats['margin_violations']
                    epoch_no_term_stats['avg_s_max_sum'] += no_term_batch_stats['avg_s_max'] * no_term_batch_stats['no_term_count']
                    epoch_no_term_stats['batch_count'] += 1
            else:
                loss = result
            
            if loss.requires_grad and not torch.isnan(loss) and not torch.isinf(loss):
                loss.backward()

                # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()
                optimizer.zero_grad()
                total_loss += loss.item()
            elif torch.isnan(loss) or torch.isinf(loss):
                print(f"[WARNING] Skipping batch due to NaN/Inf loss: {loss.item()}")
                optimizer.zero_grad()  # æ¸…ç†æ¢¯åº¦

        avg_loss = total_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0.0
        print(f"[INFO] Epoch {epoch+1} avg loss: {avg_loss:.4f}")
        
        # æ‰“å°no-term lossç»Ÿè®¡ä¿¡æ¯
        if args.use_no_term_loss and args.enable_no_term:
            print(f"[INFO] No-term loss settings: enabled=True, margin={args.no_term_margin:.3f}, weight={args.lambda_no_term:.3f}, top_m={args.no_term_top_m}")
            
            if epoch_no_term_stats['total_no_term_samples'] > 0:
                epoch_avg_s_max = epoch_no_term_stats['avg_s_max_sum'] / epoch_no_term_stats['total_no_term_samples']
                violation_rate = epoch_no_term_stats['total_violations'] / epoch_no_term_stats['total_no_term_samples']
                print(f"[INFO] No-term epoch stats: {epoch_no_term_stats['total_no_term_samples']} samples, "
                      f"avg_s_max={epoch_avg_s_max:.4f}, violation_rate={violation_rate:.2%} "
                      f"({epoch_no_term_stats['total_violations']}/{epoch_no_term_stats['total_no_term_samples']})")
            else:
                print(f"[WARN] No-term: 0 samples processed in this epoch")
        elif not args.enable_no_term:
            print(f"[INFO] No-term processing disabled")
        
        if args.enable_hard_neg:
            mode = "FAISS" if (args.enable_glossary_hard_neg and faiss_index is not None and term2idx_map) else ("in-memory" if hardneg_source_terms else "disabled")
            print(f"[INFO] Hard-neg settings: mode={mode}, k={args.hard_neg_k}, candidates={args.hard_neg_candidates}, weight={args.hard_neg_weight:.3f}, margin={args.hard_neg_margin:.3f}, source={args.hard_neg_source}, glossary_enabled={args.enable_glossary_hard_neg}, metric={args.hard_neg_metric}, nprobe={args.hard_neg_nprobe}")
        else:
            print(f"[INFO] Hard negative mining disabled")

        # === ä¿å­˜æ£€æŸ¥ç‚¹ ===
        ckpt_path = f"data/qwen2_audio_term_level_epoch{epoch+1}.pt"
        torch.save(model.state_dict(), ckpt_path)
        print(f"[INFO] Model saved to {ckpt_path}")

        # === è¯„ä¼° ===
        print(f"\n[INFO] Epoch {epoch+1} - Evaluation with training-seen terms:")
        recall_results = evaluate_topk_recall(
            model, retriever, test_dataset, device, 
            top_ks=(5, 10), max_eval=min(1000, len(test_dataset)),  # æœ€å¤šè¯„ä¼°1000ä¸ªæ ·æœ¬
            train_terms=used_terms_train,  # ä¼ å…¥ä»…æ¥è‡ªè®­ç»ƒé›†çš„æœ¯è¯­
            show_missed_terms=(epoch + 1) % 2 == 0 or epoch == args.epochs - 1,  # æ¯2ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            no_term_margin=args.no_term_margin,  # ä¼ å…¥no-termé˜ˆå€¼
            enable_no_term=args.enable_no_term,  # ä¼ å…¥no-termå¯ç”¨çŠ¶æ€
            filter_no_term=args.filter_no_term  # ä¼ å…¥no-termè¿‡æ»¤çŠ¶æ€
        )
        
        # ä½¿ç”¨ Recall@10 ä½œä¸ºæ—©åœæŒ‡æ ‡
        current_recall = sum(recall_results[10]) / len(recall_results[10]) if recall_results[10] else 0.0
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler.step(current_recall)
        
        # æ‰“å°å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        current_proj_lr = optimizer.param_groups[1]['lr']
        print(f"[INFO] Current LR - Encoder: {current_lr:.2e}, Projection: {current_proj_lr:.2e}")
        
        if current_recall > best_recall:
            best_recall = current_recall
            no_improve_epochs = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            best_model_path = args.save_path.replace('.pt', '_best.pt')
            torch.save(model.state_dict(), best_model_path)
            print(f"[INFO] New best model saved to {best_model_path} (Recall@10: {best_recall:.2%})")
        else:
            no_improve_epochs += 1
            print(f"[INFO] No improvement for {no_improve_epochs} epochs (best: {best_recall:.2%})")
            
            if no_improve_epochs >= args.patience:
                print(f"[EARLY STOPPING] No improvement in {args.patience} epochs. Best Recall@10: {best_recall:.2%}")
                break

    # === æœ€ç»ˆä¿å­˜ ===
    torch.save(model.state_dict(), args.save_path)
    print(f"[INFO] Final model saved to {args.save_path}")
    print(f"[INFO] Training completed. Best Recall@10: {best_recall:.2%}")


if __name__ == "__main__":
    main()
