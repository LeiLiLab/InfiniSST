import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import json
import librosa
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
from peft import LoraConfig, get_peft_model, TaskType
from typing import List, Union
import warnings
warnings.filterwarnings("ignore")
from contextlib import nullcontext

AUDIO_PROMPT = "<|audio_bos|><|AUDIO|><|audio_eos|>"

def downsample_mask_to(hidden_len: int, feature_mask: torch.Tensor) -> torch.Tensor:
    """Downsample a feature-level mask [B, L_feat] to [B, L_hidden] using max-pooling.
    Returns a boolean mask of shape [B, L_hidden]."""
    if not isinstance(feature_mask, torch.Tensor):
        raise ValueError("feature_mask must be a torch.Tensor")
    if feature_mask.dtype != torch.float32:
        feature_mask = feature_mask.float()
    # [B, L] -> [B, 1, L]
    m = feature_mask.unsqueeze(1)
    # Adaptive max pool to target length
    m_ds = F.adaptive_max_pool1d(m, output_size=hidden_len)  # [B, 1, L_hidden]
    return (m_ds.squeeze(1) > 0.5)

def build_qwen2_audio_inputs(processor, audio_np, device, max_mel_len=3000):
    """
    Build inputs for Qwen2-Audio strictly via its processor to keep
    the alignment between audio features and audio tokens.
    Do NOT manually resize/interpolate features; let the processor
    decide correct shapes and masks.
    
    Args:
        processor: Qwen2Audioçš„processor
        audio_np: numpyéŸ³é¢‘æ•°æ®æˆ–åˆ—è¡¨
        device: ç›®æ ‡è®¾å¤‡
        max_mel_len: ä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼Œä½†å›ºå®šä¸º3000
    
    Returns:
        å¤„ç†å¥½çš„inputså­—å…¸
    """
    # ç¡®ä¿audio_npæ˜¯åˆ—è¡¨æ ¼å¼ï¼ˆå³ä½¿åªæœ‰ä¸€ä¸ªéŸ³é¢‘ï¼‰
    if isinstance(audio_np, np.ndarray):
        audio_list = [audio_np]
    else:
        audio_list = audio_np
    
    # ä¸ºæ¯ä¸ªéŸ³é¢‘åˆ›å»ºå¯¹åº”çš„æ–‡æœ¬ï¼ˆéƒ½æ˜¯AUDIO_PROMPTï¼‰
    text_list = [AUDIO_PROMPT] * len(audio_list)
    
    # ä½¿ç”¨å•æ¬¡processorè”åˆå¤„ç†text+audioï¼Œé¿å…åˆ†ç¦»è°ƒç”¨
    try:
        proc = processor(
            text=text_list,
            audio=audio_list,
            sampling_rate=16000,
            return_tensors="pt",
            padding=True,
            return_attention_mask=True
        )
    except Exception as e:
        # å¦‚æœè”åˆå¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°åˆ†ç¦»å¤„ç†ï¼ˆå…¼å®¹æ€§ï¼‰
        print(f"[WARN] Joint processing failed ({e}), falling back to separate processing")
        
        # åˆ†åˆ«å¤„ç†ï¼ˆåŸæ–¹æ³•ï¼‰
        text_inputs = processor.tokenizer(
            text_list[0] if len(text_list) == 1 else text_list,
            return_tensors="pt",
            padding=True,
            truncation=True,
            return_attention_mask=True
        )
        
        audio_inputs = processor.feature_extractor(
            audio_list[0] if len(audio_list) == 1 else audio_list,
            sampling_rate=16000,
            return_tensors="pt",
            padding=True,
            return_attention_mask=True
        )
        
        proc = {
            "input_ids": text_inputs["input_ids"],
            "attention_mask": text_inputs["attention_mask"],
            "input_features": audio_inputs["input_features"],
            "feature_attention_mask": audio_inputs.get("attention_mask", None)
        }

    # å°† processor äº§ç‰©ç§»åˆ°ç›®æ ‡è®¾å¤‡ï¼Œä¸åšä»»ä½•å½¢çŠ¶ç¯¡æ”¹
    inputs = {}
    for k, v in proc.items():
        if isinstance(v, torch.Tensor):
            inputs[k] = v.to(device)
        else:
            inputs[k] = v

    # å…œåº•ï¼šè‹¥processoræœªæä¾› feature_attention_maskï¼Œåˆ™åŸºäº input_features ç”Ÿæˆå…¨1æ©ç 
    if "feature_attention_mask" not in inputs:
        feats = inputs.get("input_features", None)
        if isinstance(feats, torch.Tensor):
            # é€šå¸¸æ—¶é—´ç»´æ˜¯æœ€åä¸€ç»´ï¼Œç”Ÿæˆ [B, T]
            time_len = feats.shape[-1]
            inputs["feature_attention_mask"] = torch.ones(
                (feats.shape[0], time_len), dtype=torch.bool, device=device
            )
    else:
        fam = inputs.get("feature_attention_mask")
        if isinstance(fam, torch.Tensor):
            # squeeze [B,1,T] -> [B,T] if needed
            if fam.dim() == 3 and fam.shape[1] == 1:
                fam = fam.squeeze(1)
            # è½¬ä¸ºboolä»¥å…¼å®¹SDPAè¦æ±‚
            if fam.dtype != torch.bool:
                fam = fam != 0
            inputs["feature_attention_mask"] = fam
    return inputs


def calculate_feature_length(audio_duration_seconds, sampling_rate=16000, 
                           hop_length=160, frame_length=400):
    """
    è®¡ç®—éŸ³é¢‘å¯¹åº”çš„ç‰¹å¾é•¿åº¦
    
    Args:
        audio_duration_seconds: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        sampling_rate: é‡‡æ ·ç‡ï¼ŒQwen2-Audio ä½¿ç”¨ 16kHz
        hop_length: å¸§ç§»ï¼Œé€šå¸¸æ˜¯ 160 samples (10ms at 16kHz)
        frame_length: å¸§é•¿ï¼Œé€šå¸¸æ˜¯ 400 samples (25ms at 16kHz)
    
    Returns:
        ç‰¹å¾åºåˆ—é•¿åº¦
    """
    audio_samples = int(audio_duration_seconds * sampling_rate)
    # è®¡ç®—ç‰¹å¾å¸§æ•°ï¼š(samples - frame_length) // hop_length + 1
    feature_frames = (audio_samples - frame_length) // hop_length + 1
    return max(1, feature_frames)  # è‡³å°‘1å¸§


def get_dynamic_audio_chunk_length(audio_batch, default_max_seconds=5.0):
    """
    åŸºäºæ‰¹æ¬¡å†…éŸ³é¢‘é•¿åº¦åŠ¨æ€è®¡ç®—éŸ³é¢‘æ³¢å½¢çš„å¤„ç†é•¿åº¦
    æ³¨æ„ï¼šè¿™åªå½±å“æ³¢å½¢å±‚é¢çš„è£å‰ªï¼Œmelç‰¹å¾é•¿åº¦å›ºå®šä¸º3000
    
    Args:
        audio_batch: éŸ³é¢‘æ•°æ®åˆ—è¡¨
        default_max_seconds: é»˜è®¤æœ€å¤§ç§’æ•°
    
    Returns:
        åŠ¨æ€è®¡ç®—çš„éŸ³é¢‘æ³¢å½¢é•¿åº¦ï¼ˆæ ·æœ¬æ•°ï¼‰
    """
    if not audio_batch:
        return int(default_max_seconds * 16000)  # é»˜è®¤é•¿åº¦
    
    # è®¡ç®—æ‰¹æ¬¡å†…æœ€é•¿éŸ³é¢‘çš„é•¿åº¦
    max_samples = 0
    for audio in audio_batch:
        if isinstance(audio, (np.ndarray, torch.Tensor)):
            if isinstance(audio, torch.Tensor):
                samples = audio.numel()
            else:
                samples = len(audio.flatten())
            max_samples = max(max_samples, samples)
    
    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    max_samples = min(max_samples, int(default_max_seconds * 16000))
    
    return max_samples


class Qwen2AudioSpeechEncoder:
    """
    Qwen2-Audio Speech Encoder wrapper for speech-to-embedding
    """
    def __init__(self, model_name="Qwen/Qwen2-Audio-7B-Instruct", device="cuda"):
        self.device = device
        print(f"[INFO] Loading Qwen2-Audio model: {model_name}")
        
        # Load processor and model
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.model = Qwen2AudioForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map=None  # ä¸ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…ï¼Œæ‰‹åŠ¨æ§åˆ¶
        ).to(device)
        try:
            if hasattr(self.model, "config"):
                setattr(self.model.config, "use_cache", False)
            self.model.gradient_checkpointing_enable()
            print("[INFO] Enabled gradient checkpointing and disabled use_cache")
        except Exception as e:
            print(f"[WARN] Failed to enable gradient checkpointing: {e}")
        # ä¸è¦å¼ºåˆ¶è®¾ç½®ä¸º eval æ¨¡å¼ï¼Œè®©ä¸Šå±‚æ¨¡å—æ§åˆ¶ train/eval çŠ¶æ€
        # self.model.eval()  # REMOVED: è¿™ä¼šé˜»æ­¢æ¢¯åº¦ä¼ æ’­
        
        print(f"[INFO] Qwen2-Audio model loaded successfully on {device}")
    
        # åˆ†æå¹¶ç¡®å®šæ¨¡å‹ç»“æ„
        self._analyze_model_structure()
    
    def _print_module_tree(self, module, prefix="", max_depth=3, current_depth=0):
        """é€’å½’æ‰“å°æ¨¡å—ç»“æ„æ ‘"""
        if current_depth >= max_depth:
            return
        
        # æ‰“å°å­æ¨¡å—
        children = list(module.named_children())
        for i, (name, child) in enumerate(children):
            is_last = (i == len(children) - 1)
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            # è·å–æ¨¡å—ä¿¡æ¯
            module_type = type(child).__name__
            
            # ç»Ÿè®¡å‚æ•°
            num_params = sum(p.numel() for p in child.parameters())
            trainable_params = sum(p.numel() for p in child.parameters() if p.requires_grad)
            
            param_info = f"params={num_params:,}"
            if trainable_params > 0:
                param_info += f" (trainable={trainable_params:,})"
            
            print(f"  {prefix}{connector}{name}: {module_type} [{param_info}]")
            
            # é€’å½’æ‰“å°å­æ¨¡å—
            extension = "    " if is_last else "â”‚   "
            self._print_module_tree(child, prefix + extension, max_depth, current_depth + 1)
    
    def _analyze_model_structure(self):
        """åˆ†æå¹¶ç¼“å­˜æ¨¡å‹ç»“æ„ä¿¡æ¯ï¼Œé¿å…è¿è¡Œæ—¶å¤šåˆ†æ”¯åˆ¤æ–­"""
        print("\n" + "="*80)
        print("ğŸ” QWEN2-AUDIO MODEL STRUCTURE ANALYSIS")
        print("="*80)
        
        # ==================== ç¬¬1æ­¥ï¼šæ‰“å°æ¨¡å‹åŸºæœ¬ä¿¡æ¯ ====================
        print(f"\n[STEP 1] Model Basic Information:")
        print(f"  Model class: {type(self.model).__name__}")
        print(f"  Model module: {type(self.model).__module__}")
        
        # ==================== ç¬¬2æ­¥ï¼šåˆ—å‡ºæ‰€æœ‰é¡¶å±‚å±æ€§ ====================
        print(f"\n[STEP 2] All Top-Level Attributes:")
        all_attrs = [attr for attr in dir(self.model) if not attr.startswith('_')]
        print(f"  Total attributes: {len(all_attrs)}")
        
        # æŒ‰ç±»å‹åˆ†ç±»
        module_attrs = []
        config_attrs = []
        other_attrs = []
        
        for attr in all_attrs:
            try:
                obj = getattr(self.model, attr)
                if isinstance(obj, nn.Module):
                    module_attrs.append((attr, type(obj).__name__))
                elif 'config' in attr.lower() or attr == 'config':
                    config_attrs.append((attr, type(obj).__name__))
                elif not callable(obj):  # æ’é™¤æ–¹æ³•
                    other_attrs.append((attr, type(obj).__name__))
            except:
                pass
        
        print(f"\n  ğŸ“¦ Module attributes ({len(module_attrs)}):")
        for name, type_name in module_attrs[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
            print(f"    - {name}: {type_name}")
        if len(module_attrs) > 20:
            print(f"    ... and {len(module_attrs) - 20} more")
        
        print(f"\n  âš™ï¸  Config attributes ({len(config_attrs)}):")
        for name, type_name in config_attrs:
            print(f"    - {name}: {type_name}")
        
        # ==================== ç¬¬3æ­¥ï¼šæ£€æŸ¥audio_tower ====================
        print(f"\n[STEP 3] Inspecting audio_tower:")
        if hasattr(self.model, 'audio_tower'):
            audio_tower = self.model.audio_tower
            print(f"  âœ… Found 'audio_tower': {type(audio_tower).__name__}")
            print(f"     Sub-modules:")
            for sub_name, sub_module in audio_tower.named_children():
                print(f"       - {sub_name}: {type(sub_module).__name__}")
            
            # æ£€æŸ¥layers
            if hasattr(audio_tower, 'layers'):
                num_layers = len(audio_tower.layers)
                print(f"     âœ… Has {num_layers} transformer layers")
                if num_layers > 0:
                    print(f"        First layer sub-modules:")
                    for name, mod in audio_tower.layers[0].named_children():
                        print(f"          - {name}: {type(mod).__name__}")
        else:
            print(f"  âŒ No 'audio_tower' - will use full_forward strategy")
        
        # ==================== ç¬¬4æ­¥ï¼šæ£€æŸ¥ language_model ====================
        print(f"\n[STEP 4] Inspecting language_model:")
        if hasattr(self.model, 'language_model'):
            lm = self.model.language_model
            print(f"  âœ… Found 'language_model': {type(lm).__name__}")
            if hasattr(lm, 'config'):
                hidden_size = lm.config.hidden_size
                num_layers = getattr(lm.config, 'num_hidden_layers', 'unknown')
                print(f"     Config: hidden_size={hidden_size}, num_layers={num_layers}")
        else:
            print(f"  âŒ No 'language_model'")
        
        # ==================== ç¬¬5æ­¥ï¼šæ£€æŸ¥ config äº†è§£æ¨¡å‹æ¶æ„ ====================
        print(f"\n[STEP 5] Model Config Analysis:")
        if hasattr(self.model, 'config'):
            config = self.model.config
            print(f"  Config type: {type(config).__name__}")
            
            # æ‰“å°å…³é”®é…ç½®
            important_config_keys = ['hidden_size', 'num_hidden_layers', 'num_attention_heads',
                                    'audio_config', 'text_config', 'encoder_hidden_size']
            
            for key in important_config_keys:
                if hasattr(config, key):
                    value = getattr(config, key)
                    print(f"  - {key}: {value}")
            
            # æ‰“å°æ‰€æœ‰é…ç½®é”®
            all_config_keys = [k for k in dir(config) if not k.startswith('_')]
            print(f"\n  All config keys ({len(all_config_keys)}):")
            for key in all_config_keys[:30]:  # åªæ˜¾ç¤ºå‰30ä¸ª
                try:
                    value = getattr(config, key)
                    if not callable(value):
                        print(f"    - {key}: {type(value).__name__}")
                except:
                    pass
        
        # ==================== ç¬¬6æ­¥ï¼šé€’å½’æ‰“å°æ¨¡å‹ç»“æ„æ ‘ ====================
        print(f"\n[STEP 6] Model Structure Tree (first 3 levels):")
        self._print_module_tree(self.model, max_depth=3)
        
        # ==================== ç¬¬7æ­¥ï¼šæŸ¥æ‰¾å¯ä»¥åº”ç”¨ LoRA çš„æ¨¡å— ====================
        print(f"\n[STEP 7] Finding LoRA Target Modules:")
        
        # å¸¸è§çš„ LoRA target module åç§°
        common_targets = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 
                         'gate_proj', 'up_proj', 'down_proj',
                         'query', 'key', 'value', 'dense']
        
        found_targets = {}
        for name, module in self.model.named_modules():
            # è·å–æ¨¡å—åçš„æœ€åä¸€éƒ¨åˆ†
            module_name = name.split('.')[-1]
            if module_name in common_targets:
                if module_name not in found_targets:
                    found_targets[module_name] = []
                found_targets[module_name].append(name)
        
        if found_targets:
            print(f"  Found potential LoRA target modules:")
            for target_name, locations in found_targets.items():
                print(f"    '{target_name}' appears {len(locations)} times:")
                # æ˜¾ç¤ºå‰3ä¸ªä½ç½®
                for loc in locations[:3]:
                    print(f"      - {loc}")
                if len(locations) > 3:
                    print(f"      ... and {len(locations) - 3} more locations")
        else:
            print(f"  âŒ No common LoRA target modules found!")
            print(f"  Available module names (first 50):")
            all_module_names = set()
            for name, _ in self.model.named_modules():
                module_name = name.split('.')[-1]
                if module_name and not module_name.startswith('_'):
                    all_module_names.add(module_name)
            for i, name in enumerate(sorted(all_module_names)[:50]):
                print(f"    - {name}")
        
        print("\n" + "="*80)
        
        # ç°åœ¨åŸºäºå®é™…æ£€æŸ¥ç»“æœè¿›è¡Œé…ç½®
        print(f"\n[STEP 8] Determining Encoding Strategy Based on Analysis:")
        
        # å…ˆæ‰“å°æ¨¡å‹çš„é¡¶å±‚å±æ€§ï¼Œäº†è§£å®é™…ç»“æ„
        important_attrs = ['audio_tower', 'audio_encoder', 'language_model', 'lm_head', 'config']
        for attr in important_attrs:
            has_it = hasattr(self.model, attr)
            obj = getattr(self.model, attr, None) if has_it else None
            print(f"  - {attr}: {has_it} {'(type: ' + type(obj).__name__ + ')' if obj is not None else ''}")
        
        # 1. æ£€æŸ¥audio tower
        # æ ¹æ®å®é™…æ¨¡å‹ç»“æ„ï¼ŒQwen2-Audio ä½¿ç”¨ 'audio_tower'
        self.has_audio_tower = hasattr(self.model, 'audio_tower') and self.model.audio_tower is not None
        self.audio_tower_name = 'audio_tower' if self.has_audio_tower else None
        
        print(f"[STRUCT] Has audio_tower: {self.has_audio_tower}")
        if self.has_audio_tower:
            print(f"[STRUCT] Audio module name: '{self.audio_tower_name}'")
        
        if self.has_audio_tower:
            audio_tower = self.model.audio_tower
            print(f"[STRUCT] Audio tower type: {type(audio_tower).__name__}")
            
            # æ ¹æ®æ¨¡å‹configç›´æ¥è·å–hidden dimensionï¼Œä¸éœ€è¦æµ‹è¯•
            if hasattr(self.model.config, 'audio_config'):
                self.audio_hidden_dim = self.model.config.audio_config.d_model
                print(f"[STRUCT] Audio hidden dimension (from config): {self.audio_hidden_dim}")
            else:
                # Fallback: æµ‹è¯•è·å–
                print(f"[STRUCT] Testing audio tower output format...")
                dummy_input = torch.randn(1, 128, 80, device=self.device, dtype=torch.float16)
                with torch.no_grad():
                    try:
                        test_output = audio_tower(dummy_input)
                        if hasattr(test_output, 'last_hidden_state'):
                            self.audio_hidden_dim = test_output.last_hidden_state.shape[-1]
                        elif isinstance(test_output, tuple):
                            self.audio_hidden_dim = test_output[0].shape[-1]
                        elif isinstance(test_output, torch.Tensor):
                            self.audio_hidden_dim = test_output.shape[-1]
                        print(f"[STRUCT] Audio hidden dimension (from test): {self.audio_hidden_dim}")
                    except Exception as e:
                        print(f"[ERROR] Failed to determine audio hidden dim: {e}")
                        self.has_audio_tower = False
            
            # Qwen2-Audio çš„ audio_tower è¾“å‡º BaseModelOutput
            self.audio_tower_output_type = 'BaseModelOutput'
            print(f"[STRUCT] Audio tower output type: {self.audio_tower_output_type}")
        
        # 2. æ£€æŸ¥language model
        self.has_language_model = hasattr(self.model, 'language_model') and self.model.language_model is not None
        print(f"[STRUCT] Has language_model: {self.has_language_model}")
        
        if self.has_language_model:
            self.language_model_hidden_dim = self.model.language_model.config.hidden_size
            print(f"[STRUCT] Language model hidden dimension: {self.language_model_hidden_dim}")
        else:
            # ä»é¡¶å±‚configè·å–
            self.language_model_hidden_dim = self.model.config.hidden_size
            print(f"[STRUCT] Hidden dimension (from top config): {self.language_model_hidden_dim}")
        
        # 3. ç¡®å®šä½¿ç”¨çš„ç¼–ç è·¯å¾„
        if self.has_audio_tower:
            self.encoding_strategy = 'audio_tower'
            self.hidden_size = self.audio_hidden_dim
            print(f"[STRUCT] âœ… Will use AUDIO_TOWER for encoding (recommended)")
        else:
            self.encoding_strategy = 'full_forward'
            self.hidden_size = self.language_model_hidden_dim
            print(f"[STRUCT] âš ï¸  Will use FULL_FORWARD for encoding (fallback)")
        
        print(f"[STRUCT] Final hidden size for projection: {self.hidden_size}")
        print("="*80 + "\n")
    
    def get_shared_model(self):
        """Return the model and processor for sharing with text encoder"""
        return {
            'model': self.model,
            'processor': self.processor
        }
    
    def get_hidden_size(self):
        """è·å–éŸ³é¢‘ç¼–ç å™¨çš„hidden size - ä½¿ç”¨ç¼“å­˜çš„å€¼"""
        return self.hidden_size
    
    def predict(self, audio_inputs: List[Union[str, np.ndarray, torch.Tensor]], max_length: int = None, dynamic_padding: bool = True) -> torch.Tensor:
        """
        Extract audio embeddings from audio files or tensors
        
        Args:
            audio_inputs: List of audio file paths, numpy arrays, or torch tensors
            max_length: Maximum audio length in samples (optional)
            dynamic_padding: æ˜¯å¦ä½¿ç”¨åŠ¨æ€paddingï¼ˆæ¨èï¼‰
            
        Returns:
            torch tensor of shape [batch_size, embedding_dim]
        """
        # é¢„å¤„ç†æ‰€æœ‰éŸ³é¢‘æ•°æ®
        processed_audios = []
        
        for audio_input in audio_inputs:
            try:
                # Load and preprocess audio (support file path or in-memory waveform)
                if isinstance(audio_input, (np.ndarray, torch.Tensor)):
                    if isinstance(audio_input, torch.Tensor):
                        # ç¡®ä¿tensoråœ¨CPUä¸Šè¿›è¡Œnumpyè½¬æ¢ï¼Œä½†ä¿æŒåŸå§‹ç²¾åº¦
                        audio_np = audio_input.detach().cpu().float().numpy()
                    else:
                        audio_np = audio_input
                    
                    # å¤„ç†å¤šå£°é“éŸ³é¢‘ï¼šè½¬ä¸ºå•å£°é“
                    if audio_np.ndim > 1:
                        audio_np = np.mean(audio_np, axis=0)
                    
                    audio = np.array(audio_np, dtype=np.float32)
                    sr = 16000  # mmapéŸ³é¢‘æ•°æ®é»˜è®¤å·²ç»æ˜¯16kHz
                else:
                    # æ–‡ä»¶è·¯å¾„è¾“å…¥ï¼ˆå‘åå…¼å®¹ï¼‰
                    audio, sr = librosa.load(audio_input, sr=16000)  # Qwen2-Audio expects 16kHz
                
                # Ensure audio is not empty
                if len(audio) == 0:
                    print(f"[WARN] Empty audio input: {type(audio_input)}")
                    # åˆ›å»º1ç§’çš„é™éŸ³ä½œä¸ºfallback
                    audio = np.zeros(16000, dtype=np.float32)
                
                # Limit audio length if specified (default to 3 seconds max)
                max_samples = max_length if max_length else int(16000 * 3.0)
                if len(audio) > max_samples:
                    audio = audio[:max_samples]
                
                # Ensure minimum length (pad if too short)
                min_samples = 1600  # 0.1 seconds
                if len(audio) < min_samples:
                    audio = np.pad(audio, (0, min_samples - len(audio)), mode='constant')
                
                # Convert to float32 and ensure it's a numpy array
                audio = np.array(audio, dtype=np.float32)
                
                # Validate audio data
                if np.isnan(audio).any() or np.isinf(audio).any():
                    raise ValueError(f"Audio input contains NaN or Inf values")
                
                # Normalize audio to prevent extreme values
                if np.max(np.abs(audio)) > 0:
                    audio = audio / np.max(np.abs(audio)) * 0.95  # Prevent clipping
                
                processed_audios.append(audio)
                
            except Exception as e:
                print(f"[ERROR] Failed to preprocess audio input {type(audio_input)}: {e}")
                # ä½¿ç”¨1ç§’é™éŸ³ä½œä¸ºfallback
                processed_audios.append(np.zeros(16000, dtype=np.float32))
        
        # æ‰¹é‡å¤„ç†æ‰€æœ‰éŸ³é¢‘ï¼ˆäº¤ç”±processorå¤„ç†padding/truncationï¼‰
        embeddings = self._batch_extract_embeddings(processed_audios)
        
        # ç›´æ¥è¿”å›embeddingsï¼Œå› ä¸º_batch_extract_embeddingsç°åœ¨è¿”å›æ­£ç¡®æ ¼å¼çš„å¼ é‡
        return embeddings
    
    def _batch_extract_embeddings(self, audio_batch):
        """æ‰¹é‡æå–éŸ³é¢‘embeddings - ä½¿ç”¨åˆå§‹åŒ–æ—¶ç¡®å®šçš„ç­–ç•¥"""
        # ä¸ºæ¯ä¸ªéŸ³é¢‘åˆ›å»ºå¯¹åº”çš„æ–‡æœ¬ï¼ˆéƒ½æ˜¯AUDIO_PROMPTï¼‰
        text_list = [AUDIO_PROMPT] * len(audio_batch)
        
        # ä½¿ç”¨processorå¤„ç†text+audio
        inputs = self.processor(
            text=text_list,
            audio=audio_batch,
            sampling_rate=16000,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        # Move inputs to device
        for key in inputs:
            if isinstance(inputs[key], torch.Tensor):
                inputs[key] = inputs[key].to(self.model.device)
        
        # æ ¹æ®åˆå§‹åŒ–æ—¶ç¡®å®šçš„ç­–ç•¥è¿›è¡Œç¼–ç 
        # CRITICAL: ä½¿ç”¨ torch.is_grad_enabled() è€Œä¸æ˜¯ self.model.training
        # å› ä¸ºæˆ‘ä»¬éœ€è¦æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡ï¼ˆè€Œä¸æ˜¯æ¨¡å‹çŠ¶æ€ï¼‰æ¥å†³å®šæ˜¯å¦è®¡ç®—æ¢¯åº¦
        with torch.set_grad_enabled(torch.is_grad_enabled()):
            if self.encoding_strategy == 'audio_tower':
                embeddings = self._extract_from_audio_tower(inputs)
            else:  # 'full_forward'
                embeddings = self._extract_from_full_forward(inputs)
        
        # CRITICAL: åªåœ¨éœ€è¦æ—¶è½¬æ¢æ•°æ®ç±»å‹ï¼Œå¹¶ä¿æŒæ¢¯åº¦
        # .float() ä¸ä¼šæ–­å¼€æ¢¯åº¦å›¾ï¼Œä½†ä¸ºäº†ç¡®ä¿ï¼Œæˆ‘ä»¬æ˜ç¡®æ£€æŸ¥
        if embeddings.dtype != torch.float32:
            embeddings = embeddings.float()  # è¿™ä¸ªæ“ä½œä¿æŒæ¢¯åº¦
        
        return embeddings
    
    def _extract_from_audio_tower(self, inputs):
        """ä»audio toweræå–ç‰¹å¾ - ç¡®å®šæ€§è·¯å¾„ï¼ŒåŸºäºQwen2-Audioç»“æ„"""
        audio_features = inputs['input_features']
        feature_attention_mask = inputs.get('feature_attention_mask')
        
        # DEBUG: æ£€æŸ¥è¾“å…¥çš„æ¢¯åº¦çŠ¶æ€
        if not hasattr(self, '_logged_input_grad'):
            print(f"[DEBUG INPUT] audio_features.requires_grad: {audio_features.requires_grad}")
            print(f"[DEBUG INPUT] audio_features.dtype: {audio_features.dtype}")
            print(f"[DEBUG INPUT] audio_features contains NaN: {torch.isnan(audio_features).any()}")
            print(f"[DEBUG INPUT] audio_features contains Inf: {torch.isinf(audio_features).any()}")
            print(f"[DEBUG INPUT] audio_features min/max: {audio_features.min():.4f}/{audio_features.max():.4f}")
            self._logged_input_grad = True
        
        # CRITICAL: ç¡®ä¿ audio_features æœ‰æ¢¯åº¦ï¼
        # Processor ç”Ÿæˆçš„ input_features é»˜è®¤ä¸éœ€è¦æ¢¯åº¦
        if not audio_features.requires_grad and torch.is_grad_enabled():
            audio_features = audio_features.requires_grad_(True)
            print(f"[DEBUG INPUT] Force enabled requires_grad for audio_features")
        
        # DEBUG: æ£€æŸ¥ audio_tower çš„ç¬¬ä¸€å±‚å‚æ•°çŠ¶æ€
        if hasattr(self.model, 'base_model') and not hasattr(self, '_logged_lora_layer_status'):
            audio_tower_test = self.model.base_model.model.audio_tower
            first_q_proj = audio_tower_test.layers[0].self_attn.q_proj
            print(f"[DEBUG LORA] First q_proj type: {type(first_q_proj).__name__}")
            if hasattr(first_q_proj, 'lora_A'):
                lora_A = first_q_proj.lora_A['default']
                lora_B = first_q_proj.lora_B['default']
                print(f"[DEBUG LORA] lora_A weight dtype: {lora_A.weight.dtype}, requires_grad: {lora_A.weight.requires_grad}")
                print(f"[DEBUG LORA] lora_B weight dtype: {lora_B.weight.dtype}, requires_grad: {lora_B.weight.requires_grad}")
                print(f"[DEBUG LORA] lora_A training mode: {lora_A.training}")
                print(f"[DEBUG LORA] lora_dropout: {first_q_proj.lora_dropout}")
            self._logged_lora_layer_status = True
        
        # CRITICAL: PEFT åŒ…è£…åï¼Œéœ€è¦é€šè¿‡ base_model æˆ– model è®¿é—®åŸå§‹æ¨¡å—
        # ç›´æ¥è®¿é—® self.model.audio_tower ä¼šç»•è¿‡ PEFT åŒ…è£…ï¼
        if hasattr(self.model, 'base_model'):
            # PEFT åŒ…è£…åçš„æ¨¡å‹
            audio_tower = self.model.base_model.model.audio_tower
            if not hasattr(self, '_logged_peft_access'):
                print(f"[DEBUG] Using PEFT-wrapped audio_tower")
                print(f"[DEBUG] First layer q_proj type: {type(audio_tower.layers[0].self_attn.q_proj)}")
                print(f"[DEBUG] Has lora_A: {hasattr(audio_tower.layers[0].self_attn.q_proj, 'lora_A')}")
                self._logged_peft_access = True
        else:
            # æœªåŒ…è£…çš„åŸå§‹æ¨¡å‹
            audio_tower = self.model.audio_tower
            if not hasattr(self, '_logged_direct_access'):
                print(f"[DEBUG] Using direct audio_tower (no PEFT wrapping detected)")
                self._logged_direct_access = True
        
        # Qwen2-Audio: audio_tower è¿”å› BaseModelOutput
        audio_tower_output = audio_tower(audio_features)
        audio_hidden_states = audio_tower_output.last_hidden_state  # [B, T, 1280]
        
        # DEBUG: æ£€æŸ¥ hidden states çš„æ¢¯åº¦çŠ¶æ€
        if not hasattr(self, '_logged_gradient_debug'):
            print(f"[DEBUG] audio_hidden_states.requires_grad: {audio_hidden_states.requires_grad}")
            print(f"[DEBUG] audio_hidden_states.dtype: {audio_hidden_states.dtype}")
            print(f"[DEBUG] audio_hidden_states.shape: {audio_hidden_states.shape}")
            self._logged_gradient_debug = True
        
        # ä½¿ç”¨attention maskè¿›è¡Œmasked pooling
        if feature_attention_mask is not None:
            # å¦‚æœé•¿åº¦ä¸åŒ¹é…ï¼Œä¸‹é‡‡æ ·attention mask
            if feature_attention_mask.shape[-1] != audio_hidden_states.shape[1]:
                target_len = audio_hidden_states.shape[1]
                feature_attention_mask = downsample_mask_to(target_len, feature_attention_mask)
            
            # Masked mean pooling
            mask_expanded = feature_attention_mask.unsqueeze(-1).float()  # [B, T, 1]
            masked_hidden = audio_hidden_states * mask_expanded  # [B, T, H]
            pooled_features = masked_hidden.sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-8)  # [B, H]
        else:
            # ç®€å•å¹³å‡æ± åŒ–
            pooled_features = audio_hidden_states.mean(dim=1)  # [B, H]
        
        # DEBUG: æ£€æŸ¥æ± åŒ–åçš„æ¢¯åº¦çŠ¶æ€
        if not hasattr(self, '_logged_pooled_debug'):
            print(f"[DEBUG] pooled_features.requires_grad: {pooled_features.requires_grad}")
            print(f"[DEBUG] pooled_features.dtype: {pooled_features.dtype}")
            self._logged_pooled_debug = True
        
        return pooled_features
    
    def _extract_from_full_forward(self, inputs):
        """ä»å®Œæ•´forward passæå–ç‰¹å¾ - ç¡®å®šæ€§è·¯å¾„ï¼Œæ— å…œåº•é€»è¾‘"""
        outputs = self.model(
            **inputs,
            output_hidden_states=True,
            return_dict=True
        )
        # ä¼˜å…ˆä½¿ç”¨hidden_states
        if hasattr(outputs, "hidden_states") and outputs.hidden_states is not None:
            last_hidden_state = outputs.hidden_states[-1]
        else:
            last_hidden_state = outputs.last_hidden_state
        
        # å¯¹åºåˆ—ç»´åº¦è¿›è¡Œå¹³å‡æ± åŒ–
        pooled_features = last_hidden_state.mean(dim=1)  # [batch_size, hidden_dim]
        
        return pooled_features


class Qwen2AudioTextEncoder:
    """
    Qwen2-Audio Text Encoder wrapper for text-to-embedding
    """
    def __init__(self, model_name="Qwen/Qwen2-Audio-7B-Instruct", device="cuda", shared_model=None):
        self.device = device
        
        if shared_model is not None:
            # Reuse the shared model from speech encoder
            print(f"[INFO] Reusing shared Qwen2-Audio model for text encoder")
            self.processor = shared_model['processor']
            self.model = shared_model['model']
        else:
            # Load new model (fallback for backward compatibility)
            print(f"[INFO] Loading Qwen2-Audio text encoder: {model_name}")
            self.processor = AutoProcessor.from_pretrained(model_name)
            self.model = Qwen2AudioForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map=None  # ä¸ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…ï¼Œæ‰‹åŠ¨æ§åˆ¶
            ).to(device)

            try:
                if hasattr(self.model, "config"):
                    setattr(self.model.config, "use_cache", False)
                self.model.gradient_checkpointing_enable()
                print("[INFO] Enabled gradient checkpointing and disabled use_cache")
            except Exception as e:
                print(f"[WARN] Failed to enable gradient checkpointing: {e}")
            
            # ä¸è¦å¼ºåˆ¶è®¾ç½®ä¸º eval æ¨¡å¼ï¼Œè®©ä¸Šå±‚æ¨¡å—æ§åˆ¶ train/eval çŠ¶æ€
            # self.model.eval()  # REMOVED: è¿™ä¼šé˜»æ­¢æ¢¯åº¦ä¼ æ’­
            print(f"[INFO] Qwen2-Audio text encoder loaded successfully on {device}")
        
        # åˆ†ææ¨¡å‹ç»“æ„
        self._analyze_model_structure()
    
    def _analyze_model_structure(self):
        """åˆ†ææ–‡æœ¬ç¼–ç å™¨ç»“æ„"""
        print("\n" + "="*80)
        print("ğŸ” TEXT ENCODER STRUCTURE ANALYSIS")
        print("="*80)
        
        # æ£€æŸ¥language model
        if hasattr(self.model, "language_model") and self.model.language_model is not None:
            self.hidden_size = self.model.language_model.config.hidden_size
            print(f"[STRUCT] Using language_model.hidden_size: {self.hidden_size}")
        else:
            self.hidden_size = self.model.config.hidden_size
            print(f"[STRUCT] Using top-level config.hidden_size: {self.hidden_size}")
        
        print("="*80 + "\n")
    
    def get_hidden_size(self):
        """è·å–æ–‡æœ¬ç¼–ç å™¨çš„hidden size - ä½¿ç”¨ç¼“å­˜çš„å€¼"""
        return self.hidden_size
    
    def predict(self, texts: List[str], source_lang: str = "eng_Latn") -> torch.Tensor:
        """
        Extract text embeddings from text strings using text encoder hidden layers
        
        Args:
            texts: List of text strings
            source_lang: Source language (kept for compatibility, not used in Qwen2-Audio)
            
        Returns:
            torch tensor of shape [batch_size, embedding_dim]
        """
        # Tokenize all texts at once
        inputs = self.processor.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # Move inputs to device
        for key in inputs:
            inputs[key] = inputs[key].to(self.model.device)
        
        # ä½¿ç”¨æ¢¯åº¦ä¸Šä¸‹æ–‡ï¼ˆè®­ç»ƒæ—¶ä¿æŒæ¢¯åº¦ï¼Œè¯„ä¼°æ—¶æ–­é“¾ï¼‰
        with torch.set_grad_enabled(self.model.training):
            # Get text embeddings from the language model
            outputs = self.model.language_model(**inputs, output_hidden_states=True)
            
            # Use the last hidden state and pool it
            last_hidden_state = outputs.hidden_states[-1]  # [batch, seq_len, hidden_dim]
            
            # Masked mean pooling over sequence dimension, excluding padding tokens
            attention_mask = inputs["attention_mask"]
            mask_expanded = attention_mask.unsqueeze(-1).float()  # [batch, seq_len, 1]
            masked_embeddings = last_hidden_state * mask_expanded  # [batch, seq_len, hidden_dim]
            
            # Sum over sequence dimension and divide by actual length (excluding padding)
            pooled_embeddings = masked_embeddings.sum(dim=1) / (mask_expanded.sum(dim=1) + 1e-8)  # [batch, hidden_dim]
        
        # ç¡®ä¿è¾“å‡ºä¸ºfloat32
        return pooled_embeddings.float()


class ContrastiveQwen2AudioModel(nn.Module):
    """
    Contrastive Speech-Text Model using Qwen2-Audio encoders with LoRA fine-tuning
    """
    def __init__(self, speech_encoder, text_encoder, proj_dim=512, 
                 lora_r=16, lora_alpha=32, lora_dropout=0.1,
                 speech_hidden_dim=None, text_hidden_dim=None):
        super().__init__()
        self.speech_encoder = speech_encoder
        self.text_encoder = text_encoder
        
        # è‡ªåŠ¨æ¨æ–­å„è‡ªçš„hidden size
        if speech_hidden_dim is None:
            speech_hidden_dim = self.speech_encoder.get_hidden_size()
        if text_hidden_dim is None:
            text_hidden_dim = self.text_encoder.get_hidden_size()
        
        print(f"[INFO] Speech encoder hidden size: {speech_hidden_dim}")
        print(f"[INFO] Text encoder hidden size: {text_hidden_dim}")
        
        # Projection layers (always trainable) - ä½¿ç”¨å„è‡ªçš„è¾“å…¥ç»´åº¦
        self.proj_speech = nn.Linear(speech_hidden_dim, proj_dim)
        self.proj_text = nn.Linear(text_hidden_dim, proj_dim)
        
        # æ ¹æ®ç¼–ç ç­–ç•¥å†³å®š LoRA åº”ç”¨ä½ç½®
        # åŸºäºå®é™…æ¨¡å‹ç»“æ„ï¼š
        # - audio_tower æœ‰: q_proj, k_proj, v_proj (æ²¡æœ‰ o_proj!)
        # - language_model æœ‰: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
        
        if self.speech_encoder.encoding_strategy == 'audio_tower':
            # åªåœ¨éŸ³é¢‘ç¼–ç å™¨çš„attentionå±‚åŠ LoRA
            print(f"[INFO] LoRA strategy: Applying to AUDIO_TOWER only (audio_tower encoding)")
            target_modules = ["q_proj", "k_proj", "v_proj"]  # audio_tower æ²¡æœ‰ o_proj
            print(f"[INFO] Note: audio_tower attention uses q/k/v_proj only (no o_proj)")
        else:
            # åœ¨language modelåŠ LoRAï¼ˆfallbackç­–ç•¥ï¼Œä¸€èˆ¬ä¸ä¼šç”¨åˆ°ï¼‰
            print(f"[INFO] LoRA strategy: Applying to LANGUAGE_MODEL (full_forward encoding)")
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        
        # CRITICAL: ç¦ç”¨ lora_dropout ä»¥é¿å… FP16 ä¸‹çš„æ¢¯åº¦æ¶ˆå¤±
        # lora_A çš„è¾“å‡ºåœ¨ FP16 ä¸‹å¯èƒ½å¾ˆå°ï¼Œç»è¿‡ dropout åæ¢¯åº¦ä¼šä¸‹æº¢
        effective_lora_dropout = 0.0 if lora_dropout > 0 else 0.0
        if lora_dropout > 0:
            print(f"[WARN] LoRA dropout disabled (was {lora_dropout}) to prevent gradient underflow in FP16")
        
        self.lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=lora_r,  # rank
            lora_alpha=lora_alpha,  # scaling parameter
            lora_dropout=effective_lora_dropout,  # ç¦ç”¨ dropout
            target_modules=target_modules,
            bias="none",
        )
        
        print(f"[INFO] LoRA target modules: {target_modules}")

        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        print(f"[DEBUG] Model structure analysis:")
        if hasattr(self.speech_encoder.model, 'audio_tower'):
            print(f"[DEBUG] - Has audio_tower: Yes")
        if hasattr(self.speech_encoder.model, 'language_model'):
            print(f"[DEBUG] - Has language_model: Yes")

        # ç”¨äºæ§åˆ¶åªæ‰“å°ä¸€æ¬¡çš„è°ƒè¯•ä¿¡æ¯
        self._logged_speech_shape = False
        
        # å†»ç»“æ‰€æœ‰åŸå§‹å‚æ•°
        for param in self.speech_encoder.model.parameters():
            param.requires_grad = False
        for param in self.text_encoder.model.parameters():
            param.requires_grad = False
        
        # åº”ç”¨LoRAï¼ˆåªåº”ç”¨ä¸€æ¬¡ï¼Œå› ä¸ºæ¨¡å‹æ˜¯å…±äº«çš„ï¼‰
        if self.speech_encoder.model is self.text_encoder.model:
            print(f"[INFO] Applying LoRA to shared Qwen2-Audio model")
            
            # æ£€æŸ¥æ¨¡å‹ç»“æ„å¹¶æ‰“å°LoRAå°†åº”ç”¨åˆ°å“ªäº›æ¨¡å—
            print(f"[DEBUG] Checking modules before LoRA application:")
            module_count = 0
            target_modules_found = []
            for name, module in self.speech_encoder.model.named_modules():
                if any(target in name for target in self.lora_config.target_modules):
                    module_count += 1
                    target_modules_found.append(name)
                    if module_count <= 10:  # æ‰“å°å‰10ä¸ª
                        print(f"[DEBUG] - Target module: {name} ({type(module).__name__})")
            print(f"[DEBUG] Total target modules found: {module_count}")
            
            # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼ä»¥å¯ç”¨LoRA
            self.speech_encoder.model.train()
            
            # åº”ç”¨LoRA
            try:
                self.speech_encoder.model = get_peft_model(self.speech_encoder.model, self.lora_config)
                self.text_encoder.model = self.speech_encoder.model  # ä¿æŒå…±äº«
                print(f"[INFO] LoRA applied successfully")
            except Exception as e:
                print(f"[ERROR] Failed to apply LoRA: {e}")
                import traceback
                traceback.print_exc()
                raise
            
            # éªŒè¯LoRAå‚æ•°çš„åˆ›å»ºå’ŒçŠ¶æ€
            lora_params_found = 0
            lora_params_trainable = 0
            all_lora_params = []
            lora_params_converted_to_fp32 = 0
            
            for name, param in self.speech_encoder.model.named_parameters():
                if 'lora' in name.lower():
                    lora_params_found += 1
                    all_lora_params.append((name, param))
                    if param.requires_grad:
                        lora_params_trainable += 1
                    else:
                        # å¼ºåˆ¶å¯ç”¨æ¢¯åº¦
                        param.requires_grad = True
                        lora_params_trainable += 1
                        print(f"[DEBUG] Force-enabled gradient for: {name}")
                    
                    # CRITICAL: å°† LoRA å‚æ•°è½¬æ¢ä¸º FP32 ä»¥é¿å…æ¢¯åº¦ä¸‹æº¢
                    if param.dtype != torch.float32:
                        param.data = param.data.float()
                        lora_params_converted_to_fp32 += 1
            
            print(f"[DEBUG] LoRA parameters found: {lora_params_found}")
            print(f"[DEBUG] LoRA parameters trainable: {lora_params_trainable}")
            if lora_params_converted_to_fp32 > 0:
                print(f"[INFO] âœ… Converted {lora_params_converted_to_fp32} LoRA parameters to FP32 for stable gradients")
            
            # è¯¦ç»†æ£€æŸ¥å‰å‡ ä¸ªLoRAå‚æ•°
            print(f"[DEBUG] First few LoRA parameters:")
            for i, (name, param) in enumerate(all_lora_params[:5]):
                print(f"[DEBUG]   {name}: shape={param.shape}, dtype={param.dtype}, requires_grad={param.requires_grad}, device={param.device}")
            
            # éªŒè¯LoRAé€‚é…å™¨æ˜¯å¦æ­£ç¡®æ·»åŠ 
            if hasattr(self.speech_encoder.model, 'peft_config'):
                print(f"[DEBUG] PEFT config found: {self.speech_encoder.model.peft_config}")
            else:
                print(f"[WARN] No PEFT config found - LoRA may not be applied correctly")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„é€‚é…å™¨
            if hasattr(self.speech_encoder.model, 'active_adapters'):
                print(f"[DEBUG] Active adapters: {self.speech_encoder.model.active_adapters}")
            
        else:
            print(f"[INFO] Applying LoRA to separate speech and text models")
            self.speech_encoder.model.train()
            self.text_encoder.model.train()
            self.speech_encoder.model = get_peft_model(self.speech_encoder.model, self.lora_config)
            self.text_encoder.model = get_peft_model(self.text_encoder.model, self.lora_config)

        # Register the Qwen2 backbone(s) as nn.Module submodules so that
        # downstream optimizers (and DDP) can discover the LoRA parameters.
        # Without an explicit registration, model.parameters() would only expose
        # the projection heads defined on this wrapper module.
        self.add_module("speech_qwen2_model", self.speech_encoder.model)
        if self.speech_encoder.model is not self.text_encoder.model:
            self.add_module("text_qwen2_model", self.text_encoder.model)

        # è®¡ç®—LoRAå‚æ•°æ•°é‡ï¼ˆéœ€è¦åœ¨åº”ç”¨LoRAä¹‹åï¼‰
        self.actual_lora_params = sum(p.numel() for p in self.speech_encoder.model.parameters() if p.requires_grad)
        if self.speech_encoder.model is not self.text_encoder.model:
            self.actual_lora_params += sum(p.numel() for p in self.text_encoder.model.parameters() if p.requires_grad)
        
        print(f"[INFO] ContrastiveQwen2AudioModel initialized with LoRA (r={lora_r}, alpha={lora_alpha}, dropout={lora_dropout})")
        print(f"[INFO] LoRA trainable parameters: {self.actual_lora_params:,}")
        
        # è¯¦ç»†æ‰“å°LoRAå‚æ•°ä¿¡æ¯
        self._print_detailed_lora_info()
    
    def _print_detailed_lora_info(self):
        """æ‰“å°è¯¦ç»†çš„LoRAå‚æ•°ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ” DETAILED LORA PARAMETER ANALYSIS")
        print("="*60)
        
        # åˆ†æspeech encoderçš„LoRAå‚æ•°
        print("\nğŸ“¢ Speech Encoder LoRA Parameters:")
        speech_lora_params = 0
        speech_total_params = 0
        lora_modules_found = []
        
        for name, param in self.speech_encoder.model.named_parameters():
            speech_total_params += param.numel()
            if param.requires_grad:
                speech_lora_params += param.numel()
                # æ£€æŸ¥æ˜¯å¦æ˜¯LoRAå‚æ•°
                if 'lora' in name.lower():
                    lora_modules_found.append(name)
                    print(f"  âœ… {name}: {param.numel():,} params, shape={param.shape}, requires_grad={param.requires_grad}")
                else:
                    print(f"  âš ï¸  Non-LoRA trainable param: {name}: {param.numel():,} params")
        
        print(f"\nğŸ“Š Speech Encoder Summary:")
        print(f"  - Total parameters: {speech_total_params:,}")
        print(f"  - Trainable (LoRA) parameters: {speech_lora_params:,}")
        print(f"  - LoRA modules found: {len(lora_modules_found)}")
        print(f"  - LoRA ratio: {speech_lora_params/speech_total_params*100:.4f}%")
        
        # å¦‚æœæ˜¯åˆ†ç¦»æ¨¡å‹ï¼Œä¹Ÿåˆ†ætext encoder
        if self.speech_encoder.model is not self.text_encoder.model:
            print("\nğŸ“ Text Encoder LoRA Parameters:")
            text_lora_params = 0
            text_total_params = 0
            
            for name, param in self.text_encoder.model.named_parameters():
                text_total_params += param.numel()
                if param.requires_grad:
                    text_lora_params += param.numel()
                    if 'lora' in name.lower():
                        print(f"  âœ… {name}: {param.numel():,} params, shape={param.shape}")
                    else:
                        print(f"  âš ï¸  Non-LoRA trainable param: {name}: {param.numel():,} params")
            
            print(f"\nğŸ“Š Text Encoder Summary:")
            print(f"  - Total parameters: {text_total_params:,}")
            print(f"  - Trainable (LoRA) parameters: {text_lora_params:,}")
            print(f"  - LoRA ratio: {text_lora_params/text_total_params*100:.4f}%")
        else:
            print("\nğŸ“ Text Encoder: Sharing model with Speech Encoder")
        
        # åˆ†ææŠ•å½±å±‚å‚æ•°
        print("\nğŸ¯ Projection Layers:")
        proj_speech_params = sum(p.numel() for p in self.proj_speech.parameters())
        proj_text_params = sum(p.numel() for p in self.proj_text.parameters())
        total_proj_params = proj_speech_params + proj_text_params
        
        print(f"  - Speech projection: {proj_speech_params:,} params")
        print(f"  - Text projection: {proj_text_params:,} params")
        print(f"  - Total projection: {total_proj_params:,} params")
        
        # æ€»ä½“ç»Ÿè®¡
        total_trainable = self.actual_lora_params + total_proj_params
        print(f"\nğŸ¯ OVERALL TRAINING SUMMARY:")
        print(f"  - LoRA parameters: {self.actual_lora_params:,}")
        print(f"  - Projection parameters: {total_proj_params:,}")
        print(f"  - Total trainable: {total_trainable:,}")
        
        # éªŒè¯LoRAæ˜¯å¦æ­£ç¡®åº”ç”¨
        if len(lora_modules_found) == 0:
            print("\nâŒ WARNING: No LoRA modules found! LoRA may not be applied correctly!")
        else:
            print(f"\nâœ… SUCCESS: Found {len(lora_modules_found)} LoRA modules")
        
        print("="*60 + "\n")
    
    def train(self, mode: bool = True):
        """ç¡®ä¿è®­ç»ƒ/è¯„ä¼°æ¨¡å¼æ­£ç¡®ä¼ æ’­åˆ°åº•å±‚æ¨¡å‹"""
        super().train(mode)
        # ç¡®ä¿å…±äº«çš„ Qwen2 æ¨¡å‹éµå¾ªæ­¤æ¨¡å¼
        self.speech_encoder.model.train(mode)
        self.text_encoder.model.train(mode)
        if mode:
            print(f"[INFO] Set ContrastiveQwen2AudioModel to TRAINING mode")
        else:
            print(f"[INFO] Set ContrastiveQwen2AudioModel to EVAL mode")
        return self
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        return self.train(False)
    
    def diagnose_lora_step_by_step(self):
        """
        é€æ­¥è¯Šæ–­LoRAä¸ºä»€ä¹ˆæ²¡æœ‰ç”Ÿæ•ˆ
        è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯Šæ–­æµç¨‹ï¼Œä¼šæ‰“å°æ¯ä¸€æ­¥çš„ç»“æœ
        """
        print("\n" + "="*80)
        print("ğŸ”¬ LORA TROUBLESHOOTING - STEP BY STEP DIAGNOSIS")
        print("="*80)
        
        # ============ æ­¥éª¤ 1: æ£€æŸ¥ LoRA æ˜¯å¦è¢«æ­£ç¡®åº”ç”¨ ============
        print("\nã€æ­¥éª¤ 1/7ã€‘æ£€æŸ¥ LoRA é€‚é…å™¨æ˜¯å¦æ­£ç¡®åº”ç”¨")
        print("-" * 60)
        
        has_peft = hasattr(self.speech_encoder.model, 'peft_config')
        print(f"Has peft_config: {has_peft}")
        
        if has_peft:
            print(f"âœ… PEFT config keys: {list(self.speech_encoder.model.peft_config.keys())}")
            for key, config in self.speech_encoder.model.peft_config.items():
                print(f"   - Adapter '{key}': {config}")
        else:
            print(f"âŒ No peft_config found - LoRA was NOT applied!")
            print(f"   â†’ åŸå› : get_peft_model() è°ƒç”¨å¤±è´¥æˆ–æœªæ‰§è¡Œ")
            return
        
        has_active_adapters = hasattr(self.speech_encoder.model, 'active_adapters')
        if has_active_adapters:
            print(f"Active adapters: {self.speech_encoder.model.active_adapters}")
        
        # ============ æ­¥éª¤ 2: æ£€æŸ¥ LoRA å‚æ•°æ˜¯å¦å­˜åœ¨ ============
        print("\nã€æ­¥éª¤ 2/7ã€‘æ£€æŸ¥ LoRA å‚æ•°æ˜¯å¦è¢«åˆ›å»º")
        print("-" * 60)
        
        lora_params = []
        for name, param in self.speech_encoder.model.named_parameters():
            if 'lora' in name.lower():
                lora_params.append((name, param))
        
        print(f"Found {len(lora_params)} LoRA parameters")
        if len(lora_params) == 0:
            print(f"âŒ No LoRA parameters found!")
            print(f"   â†’ åŸå› : LoRA é€‚é…å™¨æœªæ­£ç¡®æ·»åŠ å‚æ•°")
            return
        else:
            print(f"âœ… LoRA parameters exist")
            print(f"   First 3 LoRA params:")
            for name, param in lora_params[:3]:
                print(f"   - {name}: shape={param.shape}, dtype={param.dtype}")
        
        # ============ æ­¥éª¤ 3: æ£€æŸ¥ LoRA å‚æ•°çš„ requires_grad ============
        print("\nã€æ­¥éª¤ 3/7ã€‘æ£€æŸ¥ LoRA å‚æ•°çš„ requires_grad æ ‡å¿—")
        print("-" * 60)
        
        lora_trainable = sum(1 for name, param in lora_params if param.requires_grad)
        lora_frozen = sum(1 for name, param in lora_params if not param.requires_grad)
        
        print(f"Trainable LoRA params: {lora_trainable}/{len(lora_params)}")
        print(f"Frozen LoRA params: {lora_frozen}/{len(lora_params)}")
        
        if lora_trainable == 0:
            print(f"âŒ All LoRA parameters are frozen (requires_grad=False)!")
            print(f"   â†’ åŸå› : å‚æ•°è¢«æ„å¤–å†»ç»“")
            print(f"   â†’ è§£å†³: è°ƒç”¨ force_enable_lora_gradients()")
            # è‡ªåŠ¨ä¿®å¤
            self.force_enable_lora_gradients()
        else:
            print(f"âœ… LoRA parameters have requires_grad=True")
        
        # ============ æ­¥éª¤ 4: æ£€æŸ¥æ¨¡å‹è®­ç»ƒæ¨¡å¼ ============
        print("\nã€æ­¥éª¤ 4/7ã€‘æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼")
        print("-" * 60)
        
        print(f"ContrastiveQwen2AudioModel.training: {self.training}")
        print(f"speech_encoder.model.training: {self.speech_encoder.model.training}")
        print(f"text_encoder.model.training: {self.text_encoder.model.training}")
        
        if not self.training:
            print(f"âŒ Model is in EVAL mode!")
            print(f"   â†’ åŸå› : æ¨¡å‹è¢«è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼")
            print(f"   â†’ è§£å†³: è°ƒç”¨ model.train()")
            return
        else:
            print(f"âœ… Model is in TRAINING mode")
        
        # ============ æ­¥éª¤ 5: æµ‹è¯•å‰å‘ä¼ æ’­æ˜¯å¦è§¦åŠ LoRA å±‚ ============
        print("\nã€æ­¥éª¤ 5/7ã€‘æµ‹è¯•å‰å‘ä¼ æ’­æ˜¯å¦ç»è¿‡ LoRA å±‚")
        print("-" * 60)
        
        # ä¿å­˜åˆå§‹å‚æ•°å€¼
        initial_values = {}
        for name, param in lora_params[:5]:  # åªæ£€æŸ¥å‰5ä¸ª
            initial_values[name] = param.data.clone()
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•è¾“å…¥
        try:
            print("Creating test audio input...")
            test_audio = [np.random.randn(16000).astype(np.float32)]  # 1ç§’éŸ³é¢‘
            
            print("Running forward pass...")
            with torch.set_grad_enabled(True):
                audio_emb = self.encode_audio(test_audio)
            
            print(f"âœ… Forward pass successful")
            print(f"   Output shape: {audio_emb.shape}")
            print(f"   Output requires_grad: {audio_emb.requires_grad}")
            
            if not audio_emb.requires_grad:
                print(f"âŒ Output does not require gradients!")
                print(f"   â†’ åŸå› : å‰å‘ä¼ æ’­ä¸­æ¢¯åº¦è¢«æ–­å¼€")
                return
            
        except Exception as e:
            print(f"âŒ Forward pass failed: {e}")
            return
        
        # ============ æ­¥éª¤ 6: æµ‹è¯•åå‘ä¼ æ’­ ============
        print("\nã€æ­¥éª¤ 6/7ã€‘æµ‹è¯•åå‘ä¼ æ’­æ˜¯å¦æ›´æ–° LoRA å‚æ•°")
        print("-" * 60)
        
        try:
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æŸå¤±
            loss = audio_emb.sum()
            print(f"Created dummy loss: {loss.item()}")
            
            # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
            self.zero_grad()
            
            # åå‘ä¼ æ’­
            print("Running backward pass...")
            loss.backward()
            
            print(f"âœ… Backward pass successful")
            
            # æ£€æŸ¥ LoRA å‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦
            lora_with_grad = 0
            lora_without_grad = 0
            
            for name, param in lora_params:
                if param.grad is not None and param.grad.abs().sum() > 0:
                    lora_with_grad += 1
                else:
                    lora_without_grad += 1
            
            print(f"LoRA params with gradients: {lora_with_grad}/{len(lora_params)}")
            print(f"LoRA params without gradients: {lora_without_grad}/{len(lora_params)}")
            
            if lora_with_grad == 0:
                print(f"âŒ No LoRA parameters received gradients!")
                print(f"   â†’ åŸå› åˆ†æ:")
                print(f"      1. å‰å‘ä¼ æ’­æœªç»è¿‡ LoRA å±‚")
                print(f"      2. æ¢¯åº¦åœ¨æŸå¤„è¢«é˜»æ–­ï¼ˆdetach/no_gradï¼‰")
                print(f"      3. ä½¿ç”¨äº†é”™è¯¯çš„ç¼–ç è·¯å¾„ï¼ˆæœªä½¿ç”¨ audio_towerï¼‰")
                
                # è¯¦ç»†æ£€æŸ¥å“ªäº›å±‚æœ‰æ¢¯åº¦
                print(f"\n   æ£€æŸ¥æŠ•å½±å±‚çš„æ¢¯åº¦:")
                for name, param in self.named_parameters():
                    if 'proj' in name and param.requires_grad:
                        has_grad = param.grad is not None and param.grad.abs().sum() > 0
                        status = "âœ…" if has_grad else "âŒ"
                        print(f"      {status} {name}: {'HAS GRAD' if has_grad else 'NO GRAD'}")
                
                return
            else:
                print(f"âœ… LoRA parameters received gradients!")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªæœ‰æ¢¯åº¦çš„å‚æ•°
                print(f"\n   Sample LoRA gradients:")
                shown = 0
                for name, param in lora_params:
                    if param.grad is not None and param.grad.abs().sum() > 0:
                        grad_norm = param.grad.norm().item()
                        print(f"      âœ… {name}: grad_norm={grad_norm:.6f}")
                        shown += 1
                        if shown >= 5:
                            break
            
        except Exception as e:
            print(f"âŒ Backward pass failed: {e}")
            import traceback
            traceback.print_exc()
            return
        
        # ============ æ­¥éª¤ 7: æ£€æŸ¥ç¼–ç ç­–ç•¥ ============
        print("\nã€æ­¥éª¤ 7/7ã€‘æ£€æŸ¥ä½¿ç”¨çš„ç¼–ç ç­–ç•¥")
        print("-" * 60)
        
        print(f"Encoding strategy: {self.speech_encoder.encoding_strategy}")
        print(f"Has audio_tower: {self.speech_encoder.has_audio_tower}")
        
        if self.speech_encoder.has_audio_tower:
            print(f"   Audio tower name: {self.speech_encoder.audio_tower_name}")
        
        if self.speech_encoder.encoding_strategy == 'audio_tower':
            print(f"âœ… Using audio_tower (recommended)")
            print(f"   Audio tower output type: {self.speech_encoder.audio_tower_output_type}")
            print(f"   Audio hidden dim: {self.speech_encoder.audio_hidden_dim}")
            
            # æ£€æŸ¥ audio_tower ä¸­çš„ LoRA (ä½¿ç”¨åŠ¨æ€åç§°)
            audio_module_name = self.speech_encoder.audio_tower_name
            audio_tower_lora = sum(1 for name, _ in lora_params if audio_module_name in name)
            print(f"   LoRA params in {audio_module_name}: {audio_tower_lora}")
            
            if audio_tower_lora == 0:
                print(f"   âš ï¸  Warning: No LoRA in {audio_module_name}!")
                print(f"      LoRA å¯èƒ½åªåº”ç”¨åœ¨ language_model ä¸Š")
                print(f"      ä½†ä½¿ç”¨ {audio_module_name} ç¼–ç æ—¶ä¸ä¼šç»è¿‡ language_model")
                print(f"   âŒ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ LoRA æ²¡æœ‰æ¢¯åº¦çš„åŸå› ï¼")
        else:
            print(f"âš ï¸  Using full_forward (fallback)")
            if not self.speech_encoder.has_audio_tower:
                print(f"   âŒ åŸå› : æ¨¡å‹æ²¡æœ‰æ‰¾åˆ° audio æ¨¡å—")
                print(f"   â†’ æ£€æŸ¥çš„æ¨¡å—å: audio_tower, audio_encoder, audio_model, encoder")
                print(f"   â†’ å®é™…æ¨¡å‹ç±»å‹: {type(self.speech_encoder.model).__name__}")
                print(f"   â†’ å¯èƒ½çš„é—®é¢˜: æ¨¡å‹ç‰ˆæœ¬ä¸å¯¹æˆ–åŠ è½½æ–¹å¼æœ‰è¯¯")
        
        # ============ æ€»ç»“ ============
        print("\n" + "="*80)
        print("ğŸ“Š DIAGNOSIS SUMMARY")
        print("="*80)
        print(f"âœ… LoRA is properly configured and receiving gradients!")
        print(f"âœ… Total LoRA params: {len(lora_params)}")
        print(f"âœ… LoRA params with gradients: {lora_with_grad}")
        print(f"âœ… Model is ready for training")
        print("="*80 + "\n")
    
    def force_enable_lora_gradients(self):
        """å¼ºåˆ¶å¯ç”¨LoRAå‚æ•°çš„æ¢¯åº¦"""
        print("\nğŸ”§ FORCING LORA GRADIENTS ENABLED")
        print("-" * 40)
        
        enabled_count = 0
        for name, param in self.speech_encoder.model.named_parameters():
            if 'lora' in name.lower() and not param.requires_grad:
                param.requires_grad = True
                enabled_count += 1
                print(f"  âœ… Enabled gradient for: {name}")
        
        if self.speech_encoder.model is not self.text_encoder.model:
            for name, param in self.text_encoder.model.named_parameters():
                if 'lora' in name.lower() and not param.requires_grad:
                    param.requires_grad = True
                    enabled_count += 1
                    print(f"  âœ… Enabled gradient for: {name}")
        
        print(f"\nğŸ“Š Force-enabled gradients for {enabled_count} LoRA parameters")
        print("-" * 40)
    
    def check_lora_gradients(self, step=None):
        """æ£€æŸ¥LoRAå‚æ•°çš„æ¢¯åº¦æ›´æ–°æƒ…å†µ"""
        step_info = f" (Step {step})" if step is not None else ""
        print(f"\nğŸ” LoRA Gradient Check{step_info}")
        print("-" * 60)
        
        lora_with_grad = 0
        lora_without_grad = 0
        non_lora_with_grad = 0
        lora_params_details = []
        
        # é¦–å…ˆæ£€æŸ¥æ¨¡å‹æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
        print(f"[DEBUG] Model training mode: {self.speech_encoder.model.training}")
        
        # æ£€æŸ¥PEFTçŠ¶æ€
        if hasattr(self.speech_encoder.model, 'peft_config'):
            print(f"[DEBUG] PEFT config exists: {list(self.speech_encoder.model.peft_config.keys())}")
        else:
            print(f"[DEBUG] âŒ No PEFT config found!")
        
        if hasattr(self.speech_encoder.model, 'active_adapters'):
            print(f"[DEBUG] Active adapters: {self.speech_encoder.model.active_adapters}")
        
        # ç»Ÿè®¡æ‰€æœ‰å‚æ•°
        total_params = 0
        trainable_params = 0
        
        for name, param in self.speech_encoder.model.named_parameters():
            total_params += 1
            if param.requires_grad:
                trainable_params += 1
                has_grad = param.grad is not None and param.grad.abs().sum() > 0
                
                if 'lora' in name.lower():
                    lora_params_details.append({
                        'name': name,
                        'shape': param.shape,
                        'has_grad': has_grad,
                        'grad_norm': param.grad.norm().item() if param.grad is not None else 0.0,
                        'param_norm': param.data.norm().item(),
                        'dtype': param.dtype,
                        'device': param.device
                    })
                    
                    if has_grad:
                        lora_with_grad += 1
                        grad_norm = param.grad.norm().item()
                        print(f"  âœ… LoRA {name}: grad_norm={grad_norm:.6f}")
                    else:
                        lora_without_grad += 1
                        grad_status = "None" if param.grad is None else "Zero"
                        print(f"  âŒ LoRA {name}: NO GRADIENT ({grad_status})")
                else:
                    if has_grad:
                        non_lora_with_grad += 1
                        grad_norm = param.grad.norm().item()
                        print(f"  âš ï¸  Non-LoRA {name}: grad_norm={grad_norm:.6f}")
        
        print(f"\nğŸ“Š Parameter Summary{step_info}:")
        print(f"  - Total parameters: {total_params}")
        print(f"  - Trainable parameters: {trainable_params}")
        print(f"  - LoRA params with gradients: {lora_with_grad}")
        print(f"  - LoRA params without gradients: {lora_without_grad}")
        print(f"  - Non-LoRA params with gradients: {non_lora_with_grad}")
        
        # è¯¦ç»†åˆ†æLoRAå‚æ•°
        if lora_params_details:
            print(f"\nğŸ” Detailed LoRA Analysis:")
            for detail in lora_params_details[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  {detail['name']}: shape={detail['shape']}, "
                      f"grad_norm={detail['grad_norm']:.6f}, "
                      f"param_norm={detail['param_norm']:.6f}, "
                      f"dtype={detail['dtype']}")
        
        # è¯Šæ–­å»ºè®®
        if lora_with_grad == 0:
            print(f"\nâŒ CRITICAL: NO LoRA parameters have gradients!")
            print(f"   Possible issues:")
            print(f"   1. LoRA not applied correctly")
            print(f"   2. Model not in training mode")
            print(f"   3. Forward pass not reaching LoRA layers")
            print(f"   4. Loss computation issues")
        else:
            print(f"\nâœ… SUCCESS: {lora_with_grad} LoRA parameters are being updated")
        
        print("-" * 60)
    
    def print_parameter_stats_before_after(self, before_state=None):
        """æ¯”è¾ƒè®­ç»ƒå‰åçš„å‚æ•°å˜åŒ–"""
        if before_state is None:
            # ä¿å­˜å½“å‰çŠ¶æ€
            state = {}
            for name, param in self.speech_encoder.model.named_parameters():
                if param.requires_grad and 'lora' in name.lower():
                    state[name] = param.data.clone().detach()
            return state
        else:
            # æ¯”è¾ƒå˜åŒ–
            print("\nğŸ”„ LoRA Parameter Changes:")
            print("-" * 40)
            
            changes_found = 0
            for name, param in self.speech_encoder.model.named_parameters():
                if param.requires_grad and 'lora' in name.lower() and name in before_state:
                    old_param = before_state[name]
                    diff = (param.data - old_param).abs().sum().item()
                    if diff > 1e-8:
                        changes_found += 1
                        print(f"  âœ… {name}: changed by {diff:.8f}")
                    else:
                        print(f"  âŒ {name}: NO CHANGE")
            
            print(f"\nğŸ“Š Parameter Change Summary:")
            print(f"  - LoRA parameters changed: {changes_found}")
            
            if changes_found == 0:
                print("  âŒ WARNING: NO LoRA parameters changed during training!")
            else:
                print(f"  âœ… SUCCESS: {changes_found} LoRA parameters were updated")
            
            print("-" * 40)
    
    def get_trainable_parameters(self):
        """è·å–å¯è®­ç»ƒå‚æ•°æ•°é‡å’Œè¯¦æƒ…"""
        lora_params = 0
        proj_params = 0
        
        # è®¡ç®—LoRAå‚æ•°
        for name, param in self.speech_encoder.model.named_parameters():
            if param.requires_grad:
                lora_params += param.numel()
        
        # å¦‚æœä¸æ˜¯å…±äº«æ¨¡å‹ï¼Œè¿˜è¦è®¡ç®—text encoderçš„LoRAå‚æ•°
        if self.speech_encoder.model is not self.text_encoder.model:
            for name, param in self.text_encoder.model.named_parameters():
                if param.requires_grad:
                    lora_params += param.numel()
        
        # è®¡ç®—æŠ•å½±å±‚å‚æ•°
        proj_params = sum(p.numel() for p in self.proj_speech.parameters()) + \
                     sum(p.numel() for p in self.proj_text.parameters())
        
        return {
            'lora_params': lora_params,
            'proj_params': proj_params,
            'total_trainable': lora_params + proj_params
        }
    
    def get_optimizer_parameters(self):
        """è·å–ä¼˜åŒ–å™¨éœ€è¦çš„å‚æ•°åˆ—è¡¨ï¼ˆåŒ…æ‹¬LoRAå’ŒæŠ•å½±å±‚å‚æ•°ï¼‰"""
        # æ”¶é›†LoRAå‚æ•°
        lora_params = [p for n, p in self.speech_encoder.model.named_parameters() if p.requires_grad]
        
        # å¦‚æœä¸æ˜¯å…±äº«æ¨¡å‹ï¼Œæ·»åŠ text encoderçš„LoRAå‚æ•°
        if self.speech_encoder.model is not self.text_encoder.model:
            lora_params.extend([p for n, p in self.text_encoder.model.named_parameters() if p.requires_grad])
        
        # æ·»åŠ æŠ•å½±å±‚å‚æ•°
        head_params = list(self.proj_speech.parameters()) + list(self.proj_text.parameters())
        
        return lora_params + head_params
    
    def encode_audio(self, audio_inputs: List[Union[str, torch.Tensor]], dynamic_padding: bool = True) -> torch.Tensor:
        """Encode audio files or tensors to embeddings"""
        # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä¿æŒæ¢¯åº¦ï¼Œè¯„ä¼°æ¨¡å¼ä¸‹æ–­é“¾
        if self.training:
            speech_embeddings = self.speech_encoder.predict(audio_inputs, dynamic_padding=dynamic_padding)  # [B, hidden_dim]
        else:
            with torch.no_grad():
                speech_embeddings = self.speech_encoder.predict(audio_inputs, dynamic_padding=dynamic_padding)  # [B, hidden_dim]
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32å¹¶ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        if not isinstance(speech_embeddings, torch.Tensor):
            speech_embeddings = torch.from_numpy(speech_embeddings)
        speech_embeddings = speech_embeddings.float().to(self.proj_speech.weight.device)
        
        # ç¡®ä¿å¼ é‡æ˜¯2Dçš„ [batch_size, hidden_dim]
        if speech_embeddings.dim() == 3:
            # å¦‚æœæ˜¯3Då¼ é‡ï¼Œéœ€è¦è¿›è¡Œæ± åŒ–æˆ–è€…å–å¹³å‡
            if speech_embeddings.shape[1] == speech_embeddings.shape[0]:
                # å¦‚æœç¬¬äºŒä¸ªç»´åº¦ç­‰äºbatch_sizeï¼Œå¯èƒ½æ˜¯é”™è¯¯çš„å †å ï¼Œå–å¯¹è§’çº¿
                print(f"[WARN] Detected 3D tensor with shape {speech_embeddings.shape}, extracting diagonal")
                speech_embeddings = torch.diagonal(speech_embeddings, dim1=0, dim2=1).T  # [batch_size, hidden_dim]
            else:
                # å¦åˆ™å¯¹ç¬¬äºŒä¸ªç»´åº¦è¿›è¡Œå¹³å‡æ± åŒ–
                print(f"[WARN] Detected 3D tensor with shape {speech_embeddings.shape}, applying mean pooling on dim=1")
                speech_embeddings = speech_embeddings.mean(dim=1)  # [batch_size, hidden_dim]
        elif speech_embeddings.dim() == 1:
            # å¦‚æœæ˜¯1Då¼ é‡ï¼Œæ·»åŠ batchç»´åº¦
            speech_embeddings = speech_embeddings.unsqueeze(0)
        
        # # ç¡®ä¿æœ€ç»ˆå½¢çŠ¶æ­£ç¡®
        # if self.training and not getattr(self, "_logged_speech_shape", False):
        #     print(f"[DEBUG] Final speech_embeddings shape: {speech_embeddings.shape}")
        #     self._logged_speech_shape = True
        
        return F.normalize(self.proj_speech(speech_embeddings), dim=-1)
    
    def encode_text(self, texts: List[str], source_lang: str = "eng_Latn") -> torch.Tensor:
        """Encode text strings to embeddings"""
        # åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä¿æŒæ¢¯åº¦ï¼Œè¯„ä¼°æ¨¡å¼ä¸‹æ–­é“¾
        if self.training:
            text_embeddings = self.text_encoder.predict(texts, source_lang=source_lang)
        else:
            with torch.no_grad():
                text_embeddings = self.text_encoder.predict(texts, source_lang=source_lang)
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32å¹¶ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
        if not isinstance(text_embeddings, torch.Tensor):
            text_embeddings = torch.from_numpy(text_embeddings)
        text_embeddings = text_embeddings.float().to(self.proj_text.weight.device)
        
        return F.normalize(self.proj_text(text_embeddings), dim=-1)


def load_glossary_terms(path):
    """Load glossary terms from JSON file"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            return [item["term"] if isinstance(item, dict) else str(item) for item in data]
        elif isinstance(data, dict):
            return list(data.keys())
        else:
            return []
    except Exception as e:
        print(f"[ERROR] Failed to load glossary from {path}: {e}")
        return []


def encode_texts_in_batches(model, texts, batch_size=1024, device="cuda"):
    """Encode texts in batches using the model's text encoder"""
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©º
    if not texts or len(texts) == 0:
        print("[WARN] Empty text list provided to encode_texts_in_batches, returning empty tensor")
        return torch.empty(0, 512, dtype=torch.float32, device=device)
    
    all_embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        try:
            embeddings = model.encode_text(batch_texts)
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼Œä½†ä¿æŒæ¢¯åº¦ï¼ˆè®­ç»ƒæ—¶ï¼‰æˆ–æ–­é“¾ï¼ˆè¯„ä¼°æ—¶ï¼‰
            embeddings = embeddings.float()
            if not model.training:
                embeddings = embeddings.detach()
            all_embeddings.append(embeddings)
        except Exception as e:
            print(f"[ERROR] Failed to encode text batch {i//batch_size}: {e}")
            # Create dummy embeddings
            dummy_emb = torch.zeros(len(batch_texts), 512, dtype=torch.float32, device=device)
            all_embeddings.append(dummy_emb)
    
    # ç¡®ä¿all_embeddingsä¸ä¸ºç©º
    if not all_embeddings:
        print("[WARN] No embeddings generated, returning empty tensor")
        return torch.empty(0, 512, dtype=torch.float32, device=device)
    
    return torch.cat(all_embeddings, dim=0)


def encode_audios_in_batches(model, audio_inputs, batch_size=64, device="cuda"):
    """
    Encode audios in batches using the model's audio encoder
    Optimized for both file paths and tensor inputs (mmap data)
    """
    all_embeddings = []
    
    for i in range(0, len(audio_inputs), batch_size):
        batch_inputs = audio_inputs[i:i + batch_size]
        try:
            embeddings = model.encode_audio(batch_inputs)
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼Œä½†ä¿æŒæ¢¯åº¦ï¼ˆè®­ç»ƒæ—¶ï¼‰æˆ–æ–­é“¾ï¼ˆè¯„ä¼°æ—¶ï¼‰
            embeddings = embeddings.float()
            if not model.training:
                embeddings = embeddings.detach()
            all_embeddings.append(embeddings)
        except Exception as e:
            print(f"[ERROR] Failed to encode audio batch {i//batch_size}: {e}")
            print(f"[DEBUG] Batch input types: {[type(inp) for inp in batch_inputs[:3]]}")  # åªæ‰“å°å‰3ä¸ªçš„ç±»å‹
            # Create dummy embeddings
            dummy_emb = torch.zeros(len(batch_inputs), 512, dtype=torch.float32, device=device)
            all_embeddings.append(dummy_emb)
    
    return torch.cat(all_embeddings, dim=0)


def encode_audio_tensors_in_batches_optimized(model, audio_tensors, batch_size=32, device="cuda"):
    """
    ä¸“é—¨ä¸ºmmap tensoræ•°æ®ä¼˜åŒ–çš„æ‰¹é‡éŸ³é¢‘ç¼–ç å‡½æ•°
    ç›¸æ¯”åŸç‰ˆæœ¬ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
    """
    if not audio_tensors:
        return torch.empty(0, 512, dtype=torch.float32, device=device)
    
    all_embeddings = []
    
    for i in range(0, len(audio_tensors), batch_size):
        batch_tensors = audio_tensors[i:i + batch_size]
        try:
            # ç›´æ¥ä½¿ç”¨tensorè¾“å…¥ï¼Œé¿å…é‡å¤çš„ç±»å‹æ£€æŸ¥
            if model.training:
                embeddings = model.encode_audio(batch_tensors)
            else:
                with torch.no_grad():
                    embeddings = model.encode_audio(batch_tensors)
            
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32
            embeddings = embeddings.float()
            if not model.training:
                embeddings = embeddings.detach()
            all_embeddings.append(embeddings)
            
        except Exception as e:
            print(f"[ERROR] Failed to encode audio tensor batch {i//batch_size}: {e}")
            print(f"[DEBUG] Batch tensor shapes: {[t.shape if isinstance(t, torch.Tensor) else 'Not tensor' for t in batch_tensors[:3]]}")
            # Create dummy embeddings with correct shape
            dummy_emb = torch.zeros(len(batch_tensors), 512, dtype=torch.float32, device=device)
            all_embeddings.append(dummy_emb)
    
    if not all_embeddings:
        return torch.empty(0, 512, dtype=torch.float32, device=device)
    
    return torch.cat(all_embeddings, dim=0)

class SimpleRetriever:
    """
    ç®€å•çš„æ£€ç´¢å™¨ç±»ï¼Œç”¨äºè®­ç»ƒè¯„ä¼°
    æ›¿ä»£å¤æ‚çš„new_retrieve.Retrieverç±»
    """
    def __init__(self, enable_fusion=True, device="cuda"):
        self.device = device
        self.enable_fusion = enable_fusion
        self.model = None  # å°†ç”±è®­ç»ƒè„šæœ¬è®¾ç½®
        self.index = None  # å°†ç”±è®­ç»ƒè„šæœ¬è®¾ç½®ä¸ºFAISSç´¢å¼•
        self.term_list = []  # å°†ç”±è®­ç»ƒè„šæœ¬è®¾ç½®ä¸ºæœ¯è¯­åˆ—è¡¨
