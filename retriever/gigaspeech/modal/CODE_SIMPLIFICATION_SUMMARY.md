# ä»£ç ç²¾ç®€æ€»ç»“

## åŸºäºçœŸå®æ¨¡å‹ç»“æ„çš„ä¼˜åŒ–

é€šè¿‡è¿è¡Œæ—¶æ—¥å¿—ï¼Œæˆ‘ä»¬è·å¾—äº†Qwen2-Audioçš„çœŸå®ç»“æ„ä¿¡æ¯ï¼Œæ®æ­¤ç²¾ç®€äº†ä»£ç ã€‚

## ğŸ” å…³é”®å‘ç°

### 1. æ¨¡å‹ç»“æ„ç¡®è®¤

```
ğŸ“¦ Module attributes:
  - audio_tower: Qwen2AudioEncoder      â† ç¡®è®¤å­˜åœ¨ä¸”åå­—å°±æ˜¯è¿™ä¸ª
  - language_model: Qwen2ForCausalLM
  - multi_modal_projector: Qwen2AudioMultiModalProjector
```

### 2. Audio Tower å±‚ç»“æ„

```
audio_tower (32å±‚):
  â”œâ”€â”€ layers[0-31]: Qwen2AudioEncoderLayer
  â”‚   â”œâ”€â”€ self_attn: Qwen2AudioAttention
  â”‚   â”‚   â”œâ”€â”€ q_proj âœ…
  â”‚   â”‚   â”œâ”€â”€ k_proj âœ…
  â”‚   â”‚   â”œâ”€â”€ v_proj âœ…
  â”‚   â”‚   â””â”€â”€ (æ—  o_proj) âŒ
  â”‚   â”œâ”€â”€ fc1: Linear (MLP)
  â”‚   â”œâ”€â”€ fc2: Linear (MLP)
```

### 3. Language Model å±‚ç»“æ„

```
language_model (32å±‚):
  â”œâ”€â”€ layers[0-31]: Qwen2DecoderLayer
  â”‚   â”œâ”€â”€ self_attn: Qwen2Attention
  â”‚   â”‚   â”œâ”€â”€ q_proj âœ…
  â”‚   â”‚   â”œâ”€â”€ k_proj âœ…
  â”‚   â”‚   â”œâ”€â”€ v_proj âœ…
  â”‚   â”‚   â””â”€â”€ o_proj âœ…  â† è¿™é‡Œæ‰æœ‰
  â”‚   â”œâ”€â”€ mlp: Qwen2MLP
  â”‚   â”‚   â”œâ”€â”€ gate_proj âœ…
  â”‚   â”‚   â”œâ”€â”€ up_proj âœ…
  â”‚   â”‚   â””â”€â”€ down_proj âœ…
```

### 4. Config ä¿¡æ¯

```python
audio_config.d_model = 1280  # audio hidden size
text_config.hidden_size = 4096  # language model hidden size
```

## âœ‚ï¸ ç²¾ç®€çš„å†…å®¹

### 1. ç§»é™¤å¤šä½™çš„æ¨¡å—åç§°æ£€æµ‹

**ä¹‹å‰ï¼š**
```python
audio_module_names = ['audio_tower', 'audio_encoder', 'audio_model', 'encoder']
for name in audio_module_names:
    if hasattr(self.model, name):
        self.audio_tower_name = name
        break
```

**ç°åœ¨ï¼š**
```python
# Qwen2-Audio ä½¿ç”¨ 'audio_tower'
self.has_audio_tower = hasattr(self.model, 'audio_tower')
self.audio_tower_name = 'audio_tower'
```

### 2. ç®€åŒ–è¾“å‡ºç±»å‹æ£€æµ‹

**ä¹‹å‰ï¼š**
```python
# è¿è¡Œæµ‹è¯•è¾“å…¥æ¥ç¡®å®šè¾“å‡ºç±»å‹
dummy_input = torch.randn(...)
test_output = audio_tower(dummy_input)

if hasattr(test_output, 'last_hidden_state'):
    self.audio_tower_output_type = 'BaseModelOutput'
elif isinstance(test_output, tuple):
    self.audio_tower_output_type = 'tuple'
elif isinstance(test_output, torch.Tensor):
    self.audio_tower_output_type = 'tensor'
```

**ç°åœ¨ï¼š**
```python
# ä»configç›´æ¥è·å–
self.audio_hidden_dim = self.model.config.audio_config.d_model  # 1280
self.audio_tower_output_type = 'BaseModelOutput'  # Qwen2-Audioå›ºå®šè¿”å›è¿™ä¸ª
```

### 3. ç²¾ç®€ LoRA target modules

**ä¹‹å‰ï¼š**
```python
if self.speech_encoder.encoding_strategy == 'audio_tower':
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]  # o_projä¸å­˜åœ¨ï¼
```

**ç°åœ¨ï¼š**
```python
if self.speech_encoder.encoding_strategy == 'audio_tower':
    # audio_tower åªæœ‰ q/k/v_projï¼Œæ²¡æœ‰ o_proj
    target_modules = ["q_proj", "k_proj", "v_proj"]
```

### 4. ç®€åŒ–æå–é€»è¾‘

**ä¹‹å‰ï¼š**
```python
def _extract_from_audio_tower(self, inputs):
    audio_tower = getattr(self.model, self.audio_tower_name)  # åŠ¨æ€è·å–
    audio_tower_output = audio_tower(audio_features)
    
    # å¤šåˆ†æ”¯åˆ¤æ–­è¾“å‡ºç±»å‹
    if self.audio_tower_output_type == 'BaseModelOutput':
        audio_hidden_states = audio_tower_output.last_hidden_state
    elif self.audio_tower_output_type == 'tuple':
        audio_hidden_states = audio_tower_output[0]
    elif self.audio_tower_output_type == 'tensor':
        audio_hidden_states = audio_tower_output
```

**ç°åœ¨ï¼š**
```python
def _extract_from_audio_tower(self, inputs):
    # Qwen2-Audio: ç›´æ¥ä½¿ç”¨ï¼Œå›ºå®šè¿”å›ç±»å‹
    audio_tower_output = self.model.audio_tower(audio_features)
    audio_hidden_states = audio_tower_output.last_hidden_state  # ç¡®å®šæ€§è·¯å¾„
```

### 5. ç²¾ç®€æ—¥å¿—è¾“å‡º

**STEP 3 ä¹‹å‰ï¼š** æ£€æŸ¥8ä¸ªå¯èƒ½çš„éŸ³é¢‘æ¨¡å—åç§°
**STEP 3 ç°åœ¨ï¼š** åªæ£€æŸ¥ `audio_tower`

**STEP 4 ä¹‹å‰ï¼š** æ£€æŸ¥5ä¸ªå¯èƒ½çš„language modelåç§°  
**STEP 4 ç°åœ¨ï¼š** åªæ£€æŸ¥ `language_model`

## ğŸ“Š ç²¾ç®€æ•ˆæœ

### ä»£ç è¡Œæ•°
- åˆ é™¤äº†çº¦ 50+ è¡Œçš„å¤šä½™æ£€æµ‹é€»è¾‘
- åˆ é™¤äº†çº¦ 30+ è¡Œçš„fallbackåˆ†æ”¯

### æ‰§è¡Œæ•ˆç‡
- ä¸éœ€è¦è¿è¡Œæµ‹è¯•è¾“å…¥ â†’ èŠ‚çœåˆå§‹åŒ–æ—¶é—´
- ä¸éœ€è¦åŠ¨æ€è·å–æ¨¡å— â†’ å‡å°‘è¿è¡Œæ—¶å¼€é”€
- ç¡®å®šæ€§çš„ä»£ç è·¯å¾„ â†’ æ›´å¿«çš„æ‰§è¡Œ

### å¯ç»´æŠ¤æ€§
- âœ… ä»£ç æ›´æ¸…æ™°
- âœ… æ›´å°‘çš„åˆ†æ”¯ â†’ æ›´å°‘çš„bug
- âœ… åŸºäºå®é™…ç»“æ„ â†’ æ›´å¯é 

## ğŸ¯ LoRA é…ç½®ä¼˜åŒ–

### æ­£ç¡®çš„é…ç½®

```python
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "k_proj", "v_proj"],  # åªè¿™3ä¸ªï¼
    bias="none",
)
```

### ä¸ºä»€ä¹ˆåªéœ€è¦ 3 ä¸ªæ¨¡å—ï¼Ÿ

1. **audio_tower çš„ attention å±‚åªæœ‰ q/k/v_proj**
   - 32 å±‚ Ã— 3 ä¸ªæ¨¡å— = 96 ä¸ª LoRA çŸ©é˜µ

2. **ä¸éœ€è¦ o_proj**
   - audio_tower çš„ attention æ²¡æœ‰è¿™ä¸ªæ¨¡å—
   - åªæœ‰ language_model æ‰æœ‰

3. **ä¸éœ€è¦ MLP å±‚ (gate/up/down_proj)**
   - è¿™äº›åªåœ¨ language_model ä¸­
   - æˆ‘ä»¬ä¸ä½¿ç”¨ language_model åšç¼–ç 

### é¢„æœŸçš„ LoRA å‚æ•°é‡

```
æ¯å±‚: (d_model Ã— r + r Ã— d_model) Ã— 3
    = (1280 Ã— 16 + 16 Ã— 1280) Ã— 3
    = 40,960 Ã— 3
    = 122,880 å‚æ•°/å±‚

æ€»å…±: 122,880 Ã— 32 å±‚ = 3,932,160 å‚æ•° (~4M)
```

## ğŸ“ ä¿ç•™çš„æ¢ç´¢æ€§æ—¥å¿—

è™½ç„¶ç²¾ç®€äº†ä»£ç ï¼Œä½†ä¿ç•™äº†è¯¦ç»†çš„ 8 æ­¥åˆ†ææ—¥å¿—ï¼š
- STEP 1-7: ç”¨äºè°ƒè¯•å’Œç†è§£æ¨¡å‹
- STEP 8: æ˜¾ç¤ºæœ€ç»ˆå†³ç­–

è¿™äº›æ—¥å¿—åœ¨åˆå§‹åŒ–æ—¶è¿è¡Œä¸€æ¬¡ï¼Œå¸®åŠ©ç¡®è®¤æ¨¡å‹ç»“æ„ã€‚

## âœ… éªŒè¯ç»“æœ

1. **è¯­æ³•æ£€æŸ¥:** âœ… é€šè¿‡
2. **Linteræ£€æŸ¥:** âœ… æ— é”™è¯¯
3. **ç¼–è¯‘æ£€æŸ¥:** âœ… æˆåŠŸ
4. **é€»è¾‘éªŒè¯:** âœ… åŸºäºçœŸå®æ¨¡å‹ç»“æ„

## ğŸš€ ä¸‹æ¬¡è¿è¡Œé¢„æœŸ

```
[INFO] LoRA strategy: Applying to AUDIO_TOWER only
[INFO] Note: audio_tower attention uses q/k/v_proj only (no o_proj)
[INFO] LoRA target modules: ['q_proj', 'k_proj', 'v_proj']

[DEBUG] Total target modules found: 96
[DEBUG] LoRA parameters found: 192  (A + BçŸ©é˜µ)
[DEBUG] LoRA parameters trainable: 192

âœ… All LoRA parameters should have gradients now!
```

## æ€»ç»“

é€šè¿‡å®é™…è¿è¡Œè·å–æ¨¡å‹ç»“æ„ä¿¡æ¯ï¼Œæˆ‘ä»¬ï¼š
1. âœ… ç§»é™¤äº†æ‰€æœ‰ä¸å¿…è¦çš„å‡è®¾å’ŒçŒœæµ‹
2. âœ… ç®€åŒ–äº†å¤šåˆ†æ”¯é€»è¾‘
3. âœ… ä½¿ç”¨äº†ç¡®å®šæ€§çš„ä»£ç è·¯å¾„
4. âœ… æ­£ç¡®é…ç½®äº† LoRA target modules
5. âœ… æå‡äº†ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§

**æ ¸å¿ƒåŸåˆ™ï¼šå…ˆè§‚å¯ŸçœŸå®ç»“æ„ï¼Œå†ç¼–å†™ç¡®å®šæ€§ä»£ç ï¼**
