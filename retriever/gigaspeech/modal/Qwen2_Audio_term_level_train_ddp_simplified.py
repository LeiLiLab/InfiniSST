#!/usr/bin/env python3
"""
ç®€åŒ–çš„Qwen2-Audio Term-Level DDPè®­ç»ƒè„šæœ¬
ä¸“ä¸ºModalç¯å¢ƒä¼˜åŒ–ï¼Œç§»é™¤äº†å¤æ‚çš„hard negative miningç­‰åŠŸèƒ½
"""

import os
import sys

# ç¦ç”¨ tokenizers çš„å¹¶è¡Œè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import time
import argparse
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
import numpy as np
from tqdm import tqdm
import soundfile as sf
import faiss

# å¯¼å…¥æ¨¡å‹ç›¸å…³
from Qwen2_Audio_train import (
    Qwen2AudioSpeechEncoder, 
    Qwen2AudioTextEncoder, 
    ContrastiveQwen2AudioModel,
    encode_texts_in_batches, 
    SimpleRetriever
)
from mmap_audio_reader import MMapAudioCollection, extract_audio_key_from_path

# å¯ç”¨TF32ä»¥æé«˜æ€§èƒ½
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
# å¯ç”¨cudnn benchmarkä»¥ä¼˜åŒ–å›ºå®šè¾“å…¥å¤§å°çš„æ€§èƒ½
torch.backends.cudnn.benchmark = True


def collate_keep(batch):
    """ä¿ç•™æ ·æœ¬åŸæ ·ï¼ˆè·³è¿‡Noneï¼‰ï¼Œè¿”å›list[tuple]"""
    batch = [b for b in batch if b is not None]
    return batch


def worker_init_fn(worker_id):
    """Workerè¿›ç¨‹åˆå§‹åŒ–å‡½æ•°ï¼Œè®¾ç½®éšæœºç§å­"""
    import random
    import numpy as np
    random.seed(42 + worker_id)
    np.random.seed(42 + worker_id)


def is_audio_valid(audio_path, min_duration=0.01, max_duration=30.0):
    """æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
    try:
        if not os.path.exists(audio_path):
            return False, "File does not exist"
        
        data, sr = sf.read(audio_path)
        
        if len(data) == 0:
            return False, "Empty audio file"
        
        duration = len(data) / sr
        if duration < min_duration or duration > max_duration:
            return False, f"Duration {duration:.3f}s out of range"
        
        if np.allclose(data, 0, atol=1e-6):
            return False, "All silence"
        
        if np.isnan(data).any() or np.isinf(data).any():
            return False, "Contains NaN/Inf values"
        
        return True, "Valid"
        
    except Exception as e:
        return False, f"Failed to read: {str(e)}"


def validate_audio_batch(audio_paths, verbose=False):
    """æ‰¹é‡éªŒè¯éŸ³é¢‘æ–‡ä»¶"""
    valid_paths = []
    valid_indices = []
    invalid_count = 0
    
    for i, path in enumerate(audio_paths):
        is_valid, reason = is_audio_valid(path)
        if is_valid:
            valid_paths.append(path)
            valid_indices.append(i)
        else:
            invalid_count += 1
            if verbose and invalid_count <= 5:
                print(f"[WARN] Invalid audio {i}: {path} - {reason}")
    
    return valid_paths, valid_indices


class TermLevelDatasetMMap(Dataset):
    """åŸºäº mmap çš„ Term-Level æ•°æ®é›†"""
    
    def __init__(self, path, mmap_shard_dir, split="train", train_ratio=0.99, test_path=None):
        # åŠ è½½ JSON å…ƒæ•°æ®
        if split == "test" and test_path is not None:
            if dist.get_rank() == 0:
                print(f"[INFO] Loading test samples from: {test_path}")
            with open(test_path, "r") as f:
                all_samples = json.load(f)
            use_split_logic = False
        else:
            if dist.get_rank() == 0:
                print(f"[INFO] Loading samples from: {path}")
            with open(path, "r") as f:
                all_samples = json.load(f)
            use_split_logic = True
        
        # åˆå§‹åŒ– mmap éŸ³é¢‘æ•°æ®åº“
        if dist.get_rank() == 0:
            print(f"[INFO] Initializing mmap audio database from: {mmap_shard_dir}")
        self.audio_db = MMapAudioCollection(mmap_shard_dir)
        
        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
        valid_samples = []
        invalid_count = 0
        
        for sample in all_samples:
            # æ£€æŸ¥åŸºæœ¬å­—æ®µ
            if not (sample.get('term_chunk_text', '').strip() and sample.get('term_chunk_audio', '')):
                continue
            
            # æ£€æŸ¥æœ¯è¯­
            terms = sample.get('term_chunk_audio_ground_truth_terms', [])
            if not isinstance(terms, list):
                terms = []
            
            # è¿‡æ»¤æœ¯è¯­
            filtered_terms = [
                t for t in terms
                if isinstance(t, str) and len(t.strip()) >= 3
            ]
            
            # åªä¿ç•™æœ‰æœ¯è¯­çš„æ ·æœ¬
            if not filtered_terms:
                continue
            
            # æ£€æŸ¥éŸ³é¢‘æ˜¯å¦åœ¨ mmap æ•°æ®åº“ä¸­
            audio_path = sample.get("term_chunk_audio", "")
            audio_key = extract_audio_key_from_path(audio_path)
            
            if audio_key in self.audio_db.k2loc:
                sample = dict(sample)
                sample['term_chunk_audio_ground_truth_terms'] = filtered_terms
                sample['audio_key'] = audio_key  # æ·»åŠ  mmap key
                valid_samples.append(sample)
            else:
                invalid_count += 1
        
        if dist.get_rank() == 0:
            print(f"[INFO] Filtered {len(valid_samples)} valid samples from {len(all_samples)} total")
            print(f"[INFO] Audio files not in mmap: {invalid_count}")
        
        # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
        if use_split_logic:
            if split == "train":
                split_idx = int(len(valid_samples) * train_ratio)
                self.samples = valid_samples[:split_idx]
            else:  # test
                split_idx = int(len(valid_samples) * train_ratio)
                self.samples = valid_samples[split_idx:]
        else:
            self.samples = valid_samples
        
        if dist.get_rank() == 0:
            print(f"[INFO] {split} dataset: {len(self.samples)} samples")

    def __getitem__(self, index):
        sample = self.samples[index]
        audio_key = sample["audio_key"]
        chunk_text = sample["term_chunk_text"]
        ground_truth_terms = sample.get('term_chunk_audio_ground_truth_terms', [])
        
        # ä» mmap æ•°æ®åº“è¯»å–éŸ³é¢‘
        try:
            wav, sr, _, _ = self.audio_db.get_by_key(audio_key)
            # è½¬æ¢ä¸º PyTorch tensor
            audio_tensor = torch.from_numpy(wav.copy()).float()
            
            # è°ƒè¯•ï¼šæ£€æŸ¥éŸ³é¢‘å†…å®¹ä¸æ–‡æœ¬çš„åŒ¹é…æ€§
            if dist.get_rank() == 0 and index < 5:  # åªæ‰“å°å‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"\n[DEBUG] Sample {index} MMAP content check:")
                print(f"[DEBUG] Audio key: {audio_key}")
                print(f"[DEBUG] Audio shape: {audio_tensor.shape}, duration: {len(audio_tensor)/16000:.2f}s")
                print(f"[DEBUG] Audio stats: min={audio_tensor.min():.4f}, max={audio_tensor.max():.4f}, mean={audio_tensor.mean():.4f}")
                print(f"[DEBUG] Text chunk: '{chunk_text[:100]}...'")
                print(f"[DEBUG] Ground truth terms: {ground_truth_terms}")
                # æ£€æŸ¥éŸ³é¢‘æ˜¯å¦ä¸ºå…¨é›¶ï¼ˆé™éŸ³ï¼‰
                if torch.allclose(audio_tensor, torch.zeros_like(audio_tensor), atol=1e-6):
                    print(f"[DEBUG] âš ï¸  WARNING: Audio appears to be silence!")
                else:
                    print(f"[DEBUG] âœ… Audio contains non-zero values")
                    
        except Exception as e:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œè¿”å›é›¶éŸ³é¢‘
            if dist.get_rank() == 0:
                print(f"[WARN] Failed to load audio for key {audio_key}: {e}")
                if index < 5:  # è¯¦ç»†æŠ¥å‘Šå‰5ä¸ªå¤±è´¥çš„æ ·æœ¬
                    print(f"[DEBUG] Sample {index} FAILED to load from MMAP:")
                    print(f"[DEBUG] Audio key: {audio_key}")
                    print(f"[DEBUG] Text chunk: '{chunk_text[:100]}...'")
                    print(f"[DEBUG] Ground truth terms: {ground_truth_terms}")
            audio_tensor = torch.zeros(16000, dtype=torch.float32)  # 1ç§’çš„é™éŸ³
        
        return ground_truth_terms, audio_tensor, chunk_text

    def __len__(self):
        return len(self.samples)
    
    def close(self):
        """å…³é—­ mmap æ•°æ®åº“"""
        if hasattr(self, 'audio_db'):
            self.audio_db.close()


class TermLevelDataset(Dataset):
    """ç®€åŒ–çš„Term-Levelæ•°æ®é›†ï¼ˆåŸç‰ˆæœ¬ï¼Œç”¨äºå‘åå…¼å®¹ï¼‰"""
    
    def __init__(self, path, split="train", train_ratio=0.99, test_path=None):
        if split == "test" and test_path is not None:
            # ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®é›†
            if dist.get_rank() == 0:
                print(f"[INFO] Loading test samples from: {test_path}")
            with open(test_path, "r") as f:
                all_samples = json.load(f)
            use_split_logic = False
        else:
            if dist.get_rank() == 0:
                print(f"[INFO] Loading samples from: {path}")
            with open(path, "r") as f:
                all_samples = json.load(f)
            use_split_logic = True
        
        # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
        valid_samples = []
        invalid_count = 0
        
        for sample in all_samples:
            # æ£€æŸ¥åŸºæœ¬å­—æ®µ
            if not (sample.get('term_chunk_text', '').strip() and sample.get('term_chunk_audio', '')):
                continue
            
            # æ£€æŸ¥æœ¯è¯­
            terms = sample.get('term_chunk_audio_ground_truth_terms', [])
            if not isinstance(terms, list):
                terms = []
            
            # è¿‡æ»¤æœ¯è¯­
            filtered_terms = [
                t for t in terms
                if isinstance(t, str) and len(t.strip()) >= 3
            ]
            
            # åªä¿ç•™æœ‰æœ¯è¯­çš„æ ·æœ¬
            if not filtered_terms:
                continue
            
            # éªŒè¯éŸ³é¢‘æ–‡ä»¶
            audio_path = sample.get("term_chunk_audio", "")
            is_valid, _ = is_audio_valid(audio_path)
            
            if is_valid:
                sample = dict(sample)
                sample['term_chunk_audio_ground_truth_terms'] = filtered_terms
                valid_samples.append(sample)
            else:
                invalid_count += 1
        
        if dist.get_rank() == 0:
            print(f"[INFO] Filtered {len(valid_samples)} valid samples from {len(all_samples)} total")
            print(f"[INFO] Invalid audio files: {invalid_count}")
        
        if use_split_logic:
            # æ•°æ®åˆ†å‰²
            import random
            random.seed(42)
            random.shuffle(valid_samples)
            
            split_idx = int(len(valid_samples) * train_ratio)
            
            if split == "train":
                self.samples = valid_samples[:split_idx]
            elif split == "test":
                self.samples = valid_samples[split_idx:]
            else:
                raise ValueError(f"Invalid split: {split}")
        else:
            self.samples = valid_samples
        
        if dist.get_rank() == 0:
            print(f"[INFO] {split} dataset: {len(self.samples)} samples")

    def __getitem__(self, idx):
        sample = self.samples[idx]
        audio_path = sample["term_chunk_audio"]
        chunk_text = sample["term_chunk_text"]
        ground_truth_terms = sample.get('term_chunk_audio_ground_truth_terms', [])
        
        return ground_truth_terms, audio_path, chunk_text

    def __len__(self):
        return len(self.samples)


def train_step(model, batch, device, args, temperature=0.07):
    """ç®€åŒ–çš„è®­ç»ƒæ­¥éª¤"""
    raw_model = model.module if isinstance(model, DDP) else model

    if len(batch) < 2:
        return None  # è¿”å›Noneè¡¨ç¤ºæ— æ•ˆbatch

    # è§£åŒ…æ•°æ®
    ground_truth_terms_list, audio_paths, chunk_texts = zip(*batch)
    
    # å°å†™å¤„ç†
    ground_truth_terms_list = [[t.lower() for t in terms if isinstance(t, str)] for terms in ground_truth_terms_list]
    chunk_texts = [text.lower() if isinstance(text, str) else "" for text in chunk_texts]

    # å…¼å®¹ä¸¤ç§è¾“å…¥ï¼š
    # - æ–‡ä»¶è·¯å¾„ï¼ˆTermLevelDatasetï¼‰â†’ éœ€è¦è¿›è¡Œè·¯å¾„æ ¡éªŒä¸è¿‡æ»¤
    # - å¼ é‡ï¼ˆTermLevelDatasetMMapï¼‰â†’ è·³è¿‡è·¯å¾„æ ¡éªŒï¼Œç›´æ¥é€å…¥ç¼–ç å™¨
    is_path_input = isinstance(audio_paths[0], str)
    if is_path_input:
        valid_audio_paths, valid_indices = validate_audio_batch(audio_paths, verbose=False)
        if len(valid_audio_paths) < 2:
            return None  # è¿”å›Noneè¡¨ç¤ºæ— æ•ˆbatch
        if len(valid_audio_paths) != len(audio_paths):
            valid_batch_data = []
            for idx in valid_indices:
                valid_batch_data.append((
                    ground_truth_terms_list[idx],
                    audio_paths[idx], 
                    chunk_texts[idx]
                ))
            ground_truth_terms_list, audio_paths, chunk_texts = zip(*valid_batch_data)
            ground_truth_terms_list = list(ground_truth_terms_list)
            audio_paths = list(audio_paths)
            chunk_texts = list(chunk_texts)
    else:
        # å¼ é‡è¾“å…¥ï¼šç¡®ä¿æ•°é‡è¶³å¤Ÿ
        if len(audio_paths) < 2:
            return None

    try:
        # ç¼–ç éŸ³é¢‘å’Œæ–‡æœ¬ï¼ˆencode_audio éœ€åŒæ—¶æ”¯æŒè·¯å¾„æˆ–å¼ é‡åˆ—è¡¨ï¼‰
        audio_emb = raw_model.encode_audio(audio_paths)
        text_emb = raw_model.encode_text(chunk_texts) if args.audio_text_loss_ratio > 0 else torch.zeros_like(audio_emb)
        
        # æ£€æŸ¥NaN/Inf
        if torch.isnan(audio_emb).any() or torch.isinf(audio_emb).any():
            return None  # è¿”å›Noneè¡¨ç¤ºç¼–ç å¤±è´¥
        
    except Exception as e:
        if dist.get_rank() == 0:
            print(f"[ERROR] Encoding failed: {e}")
        return None  # è¿”å›Noneè¡¨ç¤ºç¼–ç å¼‚å¸¸

    batch_size = len(audio_paths)
    
    # éŸ³é¢‘-æ–‡æœ¬å¯¹æ¯”æŸå¤±
    contrastive_loss = torch.tensor(0.0, device=device)
    if args.audio_text_loss_ratio > 0:
        try:
            sim_matrix = (audio_emb @ text_emb.T) / temperature
            labels = torch.arange(batch_size, dtype=torch.long, device=device)
            loss_a2t = F.cross_entropy(sim_matrix, labels)
            loss_t2a = F.cross_entropy(sim_matrix.T, labels)
            contrastive_loss = (loss_a2t + loss_t2a) / 2
        except Exception as e:
            if dist.get_rank() == 0:
                print(f"[ERROR] Contrastive loss failed: {e}")
                print(f"[DEBUG] sim_matrix shape: {sim_matrix.shape if 'sim_matrix' in locals() else 'undefined'}")
                print(f"[DEBUG] labels shape: {labels.shape if 'labels' in locals() else 'undefined'}")
                print(f"[DEBUG] batch_size: {batch_size}")

    # éŸ³é¢‘-æœ¯è¯­å¯¹æ¯”æŸå¤±
    audio_term_loss = torch.tensor(0.0, device=device)
    all_gt_terms = []
    audio_term_pairs = []
    
    for i, terms in enumerate(ground_truth_terms_list):
        for term in terms:
            if term and len(term.strip()) > 0:
                term_idx = len(all_gt_terms)
                all_gt_terms.append(term.strip())
                audio_term_pairs.append((i, term_idx))
    
    if len(all_gt_terms) > 0:
        try:
            # æ–‡æœ¬ç«¯ä¸å›ä¼ æ¢¯åº¦ï¼Œå‡å°å›¾ä¸æ˜¾å­˜
            with torch.no_grad():
                terms_emb = raw_model.encode_text(all_gt_terms)
            terms_emb = terms_emb.detach()
            audio_term_sim = (audio_emb @ terms_emb.T) / temperature
            
            # æ„å»ºæ ‡ç­¾
            audio_term_labels = []
            for i in range(batch_size):
                positive_terms = [term_idx for audio_idx, term_idx in audio_term_pairs if audio_idx == i]
                if positive_terms:
                    import random
                    audio_term_labels.append(random.choice(positive_terms))
                else:
                    audio_term_labels.append(-1)
            
            # è®¡ç®—æŸå¤±
            valid_indices = [i for i, label in enumerate(audio_term_labels) if label >= 0]
            if len(valid_indices) > 0:
                valid_sim = audio_term_sim[valid_indices]
                valid_labels = torch.tensor([audio_term_labels[i] for i in valid_indices], dtype=torch.long, device=device)
                audio_term_loss = F.cross_entropy(valid_sim, valid_labels)
                
        except Exception as e:
            if dist.get_rank() == 0:
                print(f"[ERROR] Audio-term loss failed: {e}")
                print(f"[DEBUG] audio_emb shape: {audio_emb.shape if 'audio_emb' in locals() else 'undefined'}")
                print(f"[DEBUG] terms_emb shape: {terms_emb.shape if 'terms_emb' in locals() else 'undefined'}")
                print(f"[DEBUG] audio_term_sim shape: {audio_term_sim.shape if 'audio_term_sim' in locals() else 'undefined'}")
                print(f"[DEBUG] valid_indices: {valid_indices if 'valid_indices' in locals() else 'undefined'}")
                print(f"[DEBUG] valid_labels shape: {valid_labels.shape if 'valid_labels' in locals() else 'undefined'}")
                print(f"[DEBUG] all_gt_terms count: {len(all_gt_terms)}")
                print(f"[DEBUG] audio_term_pairs count: {len(audio_term_pairs)}")

    # ç»„åˆæŸå¤±
    total_loss = args.audio_text_loss_ratio * contrastive_loss + args.audio_term_loss_ratio * audio_term_loss
    
    if torch.isnan(total_loss) or torch.isinf(total_loss):
        return None  # è¿”å›Noneè¡¨ç¤ºæŸå¤±è®¡ç®—å¼‚å¸¸
    
    return total_loss


def extract_all_used_terms(dataset):
    """æå–æ•°æ®é›†ä¸­æ‰€æœ‰ä½¿ç”¨çš„æœ¯è¯­"""
    used_terms = set()
    for i, sample in enumerate(dataset):
        if sample is None:
            continue
        try:
            ground_truth_terms, _, _ = sample
            for t in ground_truth_terms:
                if isinstance(t, str) and len(t.strip()) > 0:
                    used_terms.add(t.lower())
        except Exception as e:
            print(f"[DEBUG] Error extracting terms from sample {i}: {e}")
            print(f"[DEBUG] Sample: {sample}")
            continue
    
    print(f"[DEBUG] extract_all_used_terms found {len(used_terms)} terms from {len(dataset)} samples")
    return list(used_terms)


def encode_audio_tensors_in_batches(model, audio_tensors, batch_size=128, device="cuda"):
    """Encode audio tensors in batches using the model's audio encoder"""
    all_embeddings = []
    
    for i in range(0, len(audio_tensors), batch_size):
        batch_tensors = audio_tensors[i:i + batch_size]
        try:
            # å°†å¼ é‡ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡å¹¶ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            processed_tensors = []
            for tensor in batch_tensors:
                if isinstance(tensor, torch.Tensor):
                    # ç¡®ä¿å¼ é‡æ˜¯ float32 ç±»å‹å¹¶åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    tensor = tensor.float().to(device)
                processed_tensors.append(tensor)
            
            # ä½¿ç”¨ä¸åŸå§‹ encode_audio ç›¸åŒçš„æ–¹å¼å¤„ç†ï¼Œä¸é¢å¤–æ·»åŠ AMPï¼ˆmodel.encode_audioå†…éƒ¨å·²å¤„ç†ï¼‰
            if model.training:
                embeddings = model.encode_audio(processed_tensors)
            else:
                with torch.no_grad():
                    embeddings = model.encode_audio(processed_tensors)
            
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32ï¼Œä½†ä¿æŒæ¢¯åº¦ï¼ˆè®­ç»ƒæ—¶ï¼‰æˆ–æ–­é“¾ï¼ˆè¯„ä¼°æ—¶ï¼‰
            embeddings = embeddings.float()
            if not model.training:
                embeddings = embeddings.detach()
            all_embeddings.append(embeddings)
        except Exception as e:
            print(f"[ERROR] Failed to encode audio tensor batch {i//batch_size}: {e}")
            print(f"[DEBUG] Batch tensor types: {[type(t) for t in batch_tensors]}")
            print(f"[DEBUG] Batch tensor shapes: {[t.shape if isinstance(t, torch.Tensor) else 'Not tensor' for t in batch_tensors]}")
            print(f"[DEBUG] Batch tensor dtypes: {[t.dtype if isinstance(t, torch.Tensor) else 'Not tensor' for t in batch_tensors]}")
            # Create dummy embeddings
            dummy_emb = torch.zeros(len(batch_tensors), 512, dtype=torch.float32, device=device)
            all_embeddings.append(dummy_emb)
    
    return torch.cat(all_embeddings, dim=0)


def evaluate_topk_recall(model, retriever, dataset, device, top_ks=(5, 10), max_eval=1000, train_terms_set=None):
    """ç®€åŒ–çš„è¯„ä¼°å‡½æ•° - é€‚é…mmapæ•°æ®æ ¼å¼ï¼Œåˆ†åˆ«è®¡ç®—seenå’Œunseenæœ¯è¯­çš„recall"""
    if dist.get_rank() != 0:
        return {}
    
    model.eval()
    recall_dict = {k: [] for k in top_ks}
    seen_recall_dict = {k: [] for k in top_ks}  # seenæœ¯è¯­çš„recall
    unseen_recall_dict = {k: [] for k in top_ks}  # unseenæœ¯è¯­çš„recall
    
    # é‡å»ºç´¢å¼•
    text_terms = [term['term'] for term in retriever.term_list]
    raw_model = model.module if isinstance(model, DDP) else model
    text_emb = encode_texts_in_batches(raw_model, text_terms, device=device)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„embeddings
    if text_emb.size(0) == 0:
        print("[WARN] No valid text embeddings, skipping evaluation")
        return {k: [0.0] for k in top_ks}
    
    retriever.index.reset()
    if isinstance(text_emb, torch.Tensor):
        text_emb_numpy = text_emb.detach().cpu().float().numpy()
    else:
        text_emb_numpy = text_emb.astype(np.float32)
    retriever.index.add(text_emb_numpy)
    
    # éšæœºé‡‡æ ·è¯„ä¼°æ ·æœ¬
    import random
    random.seed(42)
    eval_indices = random.sample(range(len(dataset)), min(max_eval, len(dataset)))
    
    # æ”¶é›†æœ‰æ•ˆæ ·æœ¬ - é€‚é…mmapæ ¼å¼: (ground_truth_terms, audio_tensor, chunk_text)
    valid_samples = []
    valid_audio_tensors = []
    
    for i in eval_indices:
        sample = dataset[i]
        if sample is not None:
            ground_truth_terms, audio_tensor, chunk_text = sample
            if ground_truth_terms and isinstance(audio_tensor, torch.Tensor) and audio_tensor.numel() > 0:
                valid_samples.append(sample)
                valid_audio_tensors.append(audio_tensor)
    
    if not valid_samples:
        print("[WARN] No valid samples found for evaluation")
        return recall_dict
    
    print(f"[INFO] Evaluating on {len(valid_samples)} valid samples")
    
    # ç¼–ç éŸ³é¢‘å¼ é‡ - ä½¿ç”¨é€‚ä¸­çš„batch sizeå¹³è¡¡æ€§èƒ½å’Œå†…å­˜
    audio_embs = encode_audio_tensors_in_batches(raw_model, valid_audio_tensors, batch_size=128, device=device)
    if isinstance(audio_embs, torch.Tensor):
        audio_embs = audio_embs.detach().cpu().float().numpy()
    
    # ç»Ÿè®¡seenå’Œunseenæ ·æœ¬æ•°é‡
    seen_samples = 0
    unseen_samples = 0
    mixed_samples = 0
    
    # è¯„ä¼°
    for j, sample in enumerate(valid_samples):
        ground_truth_terms, _, _ = sample
        gt_terms = [t.lower() for t in ground_truth_terms]
        audio_emb = audio_embs[j:j+1]
        
        # å¦‚æœæä¾›äº†è®­ç»ƒæœ¯è¯­é›†åˆï¼Œåˆ†åˆ«è®¡ç®—seenå’Œunseenæœ¯è¯­çš„recall
        if train_terms_set is not None:
            seen_gt_terms = [t for t in gt_terms if t in train_terms_set]
            unseen_gt_terms = [t for t in gt_terms if t not in train_terms_set]
            
            # åˆ†ç±»æ ·æœ¬
            if len(seen_gt_terms) > 0 and len(unseen_gt_terms) > 0:
                mixed_samples += 1
            elif len(seen_gt_terms) > 0:
                seen_samples += 1
            elif len(unseen_gt_terms) > 0:
                unseen_samples += 1
        
        for top_k in top_ks:
            D, I = retriever.index.search(audio_emb, top_k)
            retrieved_terms = [retriever.term_list[idx]['term'].lower() for idx in I[0]]
            
            # æ•´ä½“recall
            matched = sum(gt_term in retrieved_terms for gt_term in gt_terms)
            sample_recall = matched / len(gt_terms) if gt_terms else 0.0
            recall_dict[top_k].append(sample_recall)
            
            # åˆ†åˆ«è®¡ç®—seenå’Œunseenæœ¯è¯­çš„recall
            if train_terms_set is not None:
                seen_gt_terms = [t for t in gt_terms if t in train_terms_set]
                unseen_gt_terms = [t for t in gt_terms if t not in train_terms_set]
                
                # Seenæœ¯è¯­recall
                if len(seen_gt_terms) > 0:
                    seen_matched = sum(gt_term in retrieved_terms for gt_term in seen_gt_terms)
                    seen_sample_recall = seen_matched / len(seen_gt_terms)
                    seen_recall_dict[top_k].append(seen_sample_recall)
                
                # Unseenæœ¯è¯­recall
                if len(unseen_gt_terms) > 0:
                    unseen_matched = sum(gt_term in retrieved_terms for gt_term in unseen_gt_terms)
                    unseen_sample_recall = unseen_matched / len(unseen_gt_terms)
                    unseen_recall_dict[top_k].append(unseen_sample_recall)
    
    # æ‰“å°ç»“æœ
    for top_k in top_ks:
        if recall_dict[top_k]:
            avg_recall = sum(recall_dict[top_k]) / len(recall_dict[top_k])
            print(f"[EVAL] Overall Recall@{top_k}: {avg_recall:.2%} ({len(recall_dict[top_k])} samples)")
            
            # æ‰“å°seenå’Œunseenæœ¯è¯­çš„recall
            if train_terms_set is not None:
                if seen_recall_dict[top_k]:
                    seen_avg_recall = sum(seen_recall_dict[top_k]) / len(seen_recall_dict[top_k])
                    print(f"[EVAL] Seen Recall@{top_k}: {seen_avg_recall:.2%} ({len(seen_recall_dict[top_k])} samples)")
                
                if unseen_recall_dict[top_k]:
                    unseen_avg_recall = sum(unseen_recall_dict[top_k]) / len(unseen_recall_dict[top_k])
                    print(f"[EVAL] Unseen Recall@{top_k}: {unseen_avg_recall:.2%} ({len(unseen_recall_dict[top_k])} samples)")
    
    # æ‰“å°æ ·æœ¬åˆ†å¸ƒç»Ÿè®¡
    if train_terms_set is not None:
        total_samples = seen_samples + unseen_samples + mixed_samples
        print(f"[EVAL] Sample distribution: Seen-only: {seen_samples}, Unseen-only: {unseen_samples}, Mixed: {mixed_samples}, Total: {total_samples}")
    
    model.train()
    
    # è¿”å›åŒ…å«è¯¦ç»†ç»“æœçš„å­—å…¸
    result = {
        'overall': recall_dict,
        'seen': seen_recall_dict,
        'unseen': unseen_recall_dict
    }
    
    return result


def setup_ddp(rank, world_size):
    """è®¾ç½®DDPç¯å¢ƒ - Modalç‰ˆæœ¬"""
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    master_addr = os.environ.get('MASTER_ADDR', '127.0.0.1')
    master_port = os.environ.get('MASTER_PORT', '29500')
    
    print(f"[INFO] Rank {rank}: Setting up DDP with {master_addr}:{master_port}")
    
    # è®¾ç½®NCCLç¯å¢ƒ - å¯ç”¨P2P/NVLinkä»¥æå‡å¤šå¡é€šä¿¡æ€§èƒ½
    os.environ['NCCL_DEBUG'] = 'WARN'
    os.environ['NCCL_P2P_DISABLE'] = '0'  # å¯ç”¨P2Pé€šä¿¡
    os.environ['NCCL_IB_DISABLE'] = '1'   # æ²¡æœ‰InfiniBandä¿æŒç¦ç”¨
    os.environ['NCCL_SHM_DISABLE'] = '0'  # å¯ç”¨å…±äº«å†…å­˜
    # ç§»é™¤SOCKET_IFNAMEé™åˆ¶ï¼Œè®©NCCLè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¥å£
    os.environ.pop('NCCL_SOCKET_IFNAME', None)
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    import datetime
    timeout = datetime.timedelta(minutes=10)
    
    try:
        dist.init_process_group(
            backend="nccl", 
            rank=rank, 
            world_size=world_size, 
            timeout=timeout
        )
        print(f"[INFO] Rank {rank}: DDP initialized successfully")
    except Exception as e:
        print(f"[ERROR] Rank {rank}: DDP initialization failed: {e}")
        raise
    
    torch.cuda.set_device(rank)
    
    if rank == 0:
        print(f"[INFO] Using device: cuda:{rank}")
        print(f"[INFO] Device name: {torch.cuda.get_device_name(rank)}")


def cleanup_ddp():
    """æ¸…ç†DDPç¯å¢ƒ"""
    dist.destroy_process_group()


def quick_performance_test(model, sample_batch, device, rank):
    """å¿«é€Ÿæ€§èƒ½æµ‹è¯•å‡½æ•°"""
    if rank != 0:
        return
    
    import time
    print("[INFO] Running quick performance test...")
    
    # æµ‹è¯•æ•°æ®åŠ è½½æ—¶é—´
    t0 = time.time()
    # æ¨¡æ‹Ÿä¸€ä¸ªå° batch
    test_batch = sample_batch[:min(8, len(sample_batch))]
    t1 = time.time()
    
    # æµ‹è¯•éŸ³é¢‘ç¼–ç æ—¶é—´
    with torch.cuda.amp.autocast():
        try:
            raw_model = model.module if hasattr(model, 'module') else model
            audio_items = [x[1] for x in test_batch]  # æå–éŸ³é¢‘æ•°æ®
            audio_emb = raw_model.encode_audio(audio_items)
            torch.cuda.synchronize()  # ç¡®ä¿ GPU è®¡ç®—å®Œæˆ
            t2 = time.time()
            
            print(f"[PERF] Data prep: {(t1-t0)*1000:.1f}ms")
            print(f"[PERF] Audio encoding ({len(test_batch)} samples): {(t2-t1)*1000:.1f}ms")
            print(f"[PERF] Audio embedding shape: {audio_emb.shape}")
            print(f"[PERF] GPU memory used: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
            
        except Exception as e:
            print(f"[PERF] Performance test failed: {e}")


def check_lora_training_status(model, step, rank):
    """æ£€æŸ¥LoRAè®­ç»ƒçŠ¶æ€çš„è¾…åŠ©å‡½æ•°"""
    if rank != 0:
        return
    
    try:
        raw_model = model.module if hasattr(model, 'module') else model
        if hasattr(raw_model, 'check_lora_gradients'):
            raw_model.check_lora_gradients(step=step)
        else:
            print(f"[WARN] Model does not have check_lora_gradients method")
    except Exception as e:
        print(f"[ERROR] Failed to check LoRA status: {e}")


def eval_only(rank, world_size, args):
    """ä»…è¯„ä¼°æ¨¡å¼ï¼šæµ‹è¯•åŸå§‹æ¨¡å‹çš„recallæ•ˆæœ"""
    setup_ddp(rank, world_size)
    device = torch.device(f"cuda:{rank}")
    
    if rank == 0:
        print(f"[INFO] Starting evaluation-only mode with {world_size} GPUs")
        print(f"[INFO] Will evaluate on maximum {args.eval_max_samples} samples")
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŸå§‹æ¨¡å‹ï¼Œä¸åŠ è½½ä»»ä½•é¢„è®­ç»ƒæƒé‡ï¼‰
    speech_encoder = Qwen2AudioSpeechEncoder(model_name=args.model_name, device=device)
    text_encoder = Qwen2AudioTextEncoder(
        model_name=args.model_name, 
        device=device, 
        shared_model=speech_encoder.get_shared_model()
    )
    
    model = ContrastiveQwen2AudioModel(
        speech_encoder, text_encoder, 
        proj_dim=512,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout
    ).to(device)
    
    # åŒ…è£…ä¸ºDDPï¼ˆå³ä½¿åªæ˜¯è¯„ä¼°ä¹Ÿéœ€è¦ï¼Œä¿æŒä¸€è‡´æ€§ï¼‰
    model = DDP(model, device_ids=[rank], find_unused_parameters=False)
    
    # åŠ è½½æ•°æ®é›†
    mmap_shard_dir = getattr(args, 'mmap_shard_dir', None)
    
    if mmap_shard_dir and os.path.exists(mmap_shard_dir):
        # ä½¿ç”¨ mmap æ•°æ®é›†
        if rank == 0:
            print(f"[INFO] Using mmap dataset from: {mmap_shard_dir}")
        
        if args.test_samples_path:
            test_dataset = TermLevelDatasetMMap(None, mmap_shard_dir, split="test", test_path=args.test_samples_path)
        else:
            # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æµ‹è¯•éƒ¨åˆ†
            test_dataset = TermLevelDatasetMMap(args.train_samples_path, mmap_shard_dir, split="test", train_ratio=args.train_ratio)
    else:
        # ä½¿ç”¨ä¼ ç»Ÿæ•°æ®é›†
        if rank == 0:
            print("[INFO] Using traditional dataset (file-based audio loading)")
        
        if args.test_samples_path:
            test_dataset = TermLevelDataset(None, split="test", test_path=args.test_samples_path)
        else:
            # ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æµ‹è¯•éƒ¨åˆ†
            test_dataset = TermLevelDataset(args.train_samples_path, split="test", train_ratio=args.train_ratio)
    
    # è®¾ç½®è¯„ä¼°ç”¨çš„retrieverï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if rank == 0:
        # ä»æµ‹è¯•æ•°æ®é›†æå–æœ¯è¯­
        used_terms = extract_all_used_terms(test_dataset)
        used_terms = list(set(t.lower() for t in used_terms))
        
        print(f"[INFO] Extracted {len(used_terms)} unique terms from test dataset")
        if len(used_terms) == 0:
            print("[ERROR] No terms found in test dataset! This should not happen.")
            cleanup_ddp()
            return
        
        retriever = SimpleRetriever(enable_fusion=True, device=device)
        retriever.model = model.module
        retriever.index = faiss.IndexFlatL2(512)
        retriever.term_list = [{'term': t} for t in used_terms]
        
        print(f"[INFO] Setup complete. Test: {len(test_dataset)}, Terms: {len(used_terms)}")
        
        # è¿›è¡Œè¯„ä¼°
        print(f"[INFO] Starting evaluation on original model...")
        model.eval()
        
        recall_results = evaluate_topk_recall(
            model, retriever, test_dataset, device, 
            top_ks=(1, 5, 10, 20), max_eval=args.eval_max_samples,
            train_terms_set=None  # eval_onlyæ¨¡å¼ä¸‹æ²¡æœ‰è®­ç»ƒé›†ï¼Œæ— æ³•åŒºåˆ†seen/unseen
        )
        
        print(f"\n[RESULTS] Original Model Evaluation Results:")
        print(f"[RESULTS] Dataset: {len(test_dataset)} total samples")
        print(f"[RESULTS] Terms: {len(used_terms)} unique terms")
        print(f"[RESULTS] Evaluated on: {min(args.eval_max_samples, len(test_dataset))} samples")
        
        # å¤„ç†æ–°çš„è¿”å›æ ¼å¼
        overall_results = recall_results.get('overall', recall_results)
        
        for top_k in [1, 5, 10, 20]:
            if overall_results.get(top_k) and len(overall_results[top_k]) > 0:
                avg_recall = sum(overall_results[top_k]) / len(overall_results[top_k])
                print(f"[RESULTS] Recall@{top_k}: {avg_recall:.2%} ({len(overall_results[top_k])} samples)")
            else:
                print(f"[RESULTS] Recall@{top_k}: No valid results")
        
        print(f"\n[INFO] Evaluation completed. Original model baseline established.")
    
    cleanup_ddp()

def train_ddp(rank, world_size, args):
    """DDPè®­ç»ƒä¸»å‡½æ•°"""
    setup_ddp(rank, world_size)
    device = torch.device(f"cuda:{rank}")
    
    if rank == 0:
        print(f"[INFO] Starting DDP training with {world_size} GPUs")
    
    # åˆå§‹åŒ–æ¨¡å‹
    speech_encoder = Qwen2AudioSpeechEncoder(model_name=args.model_name, device=device)
    text_encoder = Qwen2AudioTextEncoder(
        model_name=args.model_name, 
        device=device, 
        shared_model=speech_encoder.get_shared_model()
    )
    
    model = ContrastiveQwen2AudioModel(
        speech_encoder, text_encoder, 
        proj_dim=512,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout
    ).to(device)
    
    # å¼ºåˆ¶å¯ç”¨LoRAå‚æ•°æ¢¯åº¦ï¼ˆå¦‚æœå®ƒä»¬è¢«æ„å¤–ç¦ç”¨ï¼‰
    if rank == 0:
        print(f"\nğŸ”§ DETAILED MODEL ANALYSIS BEFORE DDP")
        print("-" * 60)
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„
        print(f"[DEBUG] Model type: {type(model)}")
        print(f"[DEBUG] Model training mode: {model.training}")
        
        # æ£€æŸ¥å„ä¸ªç»„ä»¶
        print(f"[DEBUG] Speech encoder model type: {type(model.speech_encoder.model)}")
        print(f"[DEBUG] Text encoder model type: {type(model.text_encoder.model)}")
        print(f"[DEBUG] Models are shared: {model.speech_encoder.model is model.text_encoder.model}")
        
        # å¼ºåˆ¶å¯ç”¨LoRAæ¢¯åº¦
        model.force_enable_lora_gradients()
        
        # æ£€æŸ¥LoRAçŠ¶æ€
        model.check_lora_gradients(step=0)
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if args.best_model_path and os.path.exists(args.best_model_path):
        if rank == 0:
            print(f"[INFO] Loading weights from {args.best_model_path}")
        try:
            checkpoint = torch.load(args.best_model_path, map_location=device)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
            
            # å¤„ç†DDPå‰ç¼€
            if list(state_dict.keys())[0].startswith('module.'):
                new_state_dict = {}
                for k, v in state_dict.items():
                    new_state_dict[k[7:]] = v
                state_dict = new_state_dict
            
            model.load_state_dict(state_dict)
            if rank == 0:
                print("[INFO] Weights loaded successfully")
        except Exception as e:
            if rank == 0:
                print(f"[ERROR] Failed to load weights: {e}")
    
    # åŒ…è£…ä¸ºDDP
    model = DDP(model, device_ids=[rank], find_unused_parameters=False)
    
    # DDPåŒ…è£…åè¿è¡Œå®Œæ•´çš„LoRAè¯Šæ–­ï¼ˆè¿™å¾ˆé‡è¦ï¼ï¼‰
    if rank == 0:
        print(f"\nğŸ”§ POST-DDP LoRA DIAGNOSIS")
        print("-" * 60)
        raw_model = model.module
        print(f"[DEBUG] Post-DDP model type: {type(raw_model)}")
        print(f"[DEBUG] Post-DDP training mode: {raw_model.training}")
        
        # è¿è¡Œå®Œæ•´çš„è¯Šæ–­æµç¨‹
        raw_model.diagnose_lora_step_by_step()
    
    # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä»…åŒ…å«éœ€è¦è®­ç»ƒçš„å‚æ•°ï¼Œå¦‚LoRAå’ŒæŠ•å½±å±‚ï¼‰
    trainable_params = [p for p in model.module.parameters() if p.requires_grad]
    if rank == 0:
        print(f"[INFO] Optimizer will update {len(trainable_params)} parameter tensors")
    optimizer = torch.optim.AdamW(trainable_params, lr=args.lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2, verbose=(rank == 0)
    )

    # å¦‚æœå¯è®­ç»ƒå‚æ•°ä»ä¸º FP16ï¼Œåˆ™ç¦ç”¨ GradScaler é¿å… unscale æŠ¥é”™
    contains_fp16_params = any(param.dtype == torch.float16 for param in trainable_params)
    if rank == 0 and contains_fp16_params:
        print("[WARN] Trainable parameters include FP16 tensors; disabling GradScaler to avoid unscale errors")
    
    # åŠ è½½æ•°æ®é›†
    mmap_shard_dir = getattr(args, 'mmap_shard_dir', None)
    
    if mmap_shard_dir and os.path.exists(mmap_shard_dir):
        # ä½¿ç”¨ mmap æ•°æ®é›†
        if rank == 0:
            print(f"[INFO] Using mmap dataset from: {mmap_shard_dir}")
        
        if args.test_samples_path:
            train_dataset = TermLevelDatasetMMap(args.train_samples_path, mmap_shard_dir, split="train", train_ratio=1.0)
            test_dataset = TermLevelDatasetMMap(None, mmap_shard_dir, split="test", test_path=args.test_samples_path)
        else:
            # å¦‚æœå¯ç”¨æµ‹è¯•é›†é‡æ„ï¼Œéœ€è¦æ›´å¤§çš„æµ‹è¯•é›†æ¥ç­›é€‰æ ·æœ¬
            effective_train_ratio = args.train_ratio
            if args.rebuild_test_set:
                # åŠ¨æ€è°ƒæ•´train_ratioä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ ·æœ¬
                # ç›®æ ‡ï¼šè‡³å°‘éœ€è¦target_test_size * 2çš„æµ‹è¯•æ ·æœ¬æ¥ç­›é€‰
                min_test_ratio = max(0.05, args.target_test_size * 2 / 50000)  # å‡è®¾å¤§çº¦5ä¸‡æ ·æœ¬ï¼Œè‡³å°‘5%æµ‹è¯•é›†
                if (1.0 - args.train_ratio) < min_test_ratio:
                    effective_train_ratio = 1.0 - min_test_ratio
                    print(f"[INFO] Adjusting train_ratio from {args.train_ratio:.3f} to {effective_train_ratio:.3f} for test set rebuilding")
                    print(f"[INFO] This ensures at least {min_test_ratio:.1%} of data for test set to achieve target unseen ratio")
                else:
                    print(f"[INFO] Current train_ratio {args.train_ratio:.3f} provides sufficient test samples")
            
            train_dataset = TermLevelDatasetMMap(args.train_samples_path, mmap_shard_dir, split="train", train_ratio=effective_train_ratio)
            test_dataset = TermLevelDatasetMMap(args.train_samples_path, mmap_shard_dir, split="test", train_ratio=effective_train_ratio)
    else:
        # ä½¿ç”¨ä¼ ç»Ÿæ•°æ®é›†
        if rank == 0:
            print("[INFO] Using traditional dataset (file-based audio loading)")
        
        if args.test_samples_path:
            train_dataset = TermLevelDataset(args.train_samples_path, split="train", train_ratio=1.0)
            test_dataset = TermLevelDataset(None, split="test", test_path=args.test_samples_path)
        else:
            # å¦‚æœå¯ç”¨æµ‹è¯•é›†é‡æ„ï¼Œéœ€è¦æ›´å¤§çš„æµ‹è¯•é›†æ¥ç­›é€‰æ ·æœ¬
            effective_train_ratio = args.train_ratio
            if args.rebuild_test_set:
                # åŠ¨æ€è°ƒæ•´train_ratioä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ ·æœ¬
                min_test_ratio = max(0.05, args.target_test_size * 2 / 50000)  # å‡è®¾å¤§çº¦5ä¸‡æ ·æœ¬ï¼Œè‡³å°‘5%æµ‹è¯•é›†
                if (1.0 - args.train_ratio) < min_test_ratio:
                    effective_train_ratio = 1.0 - min_test_ratio
                    print(f"[INFO] Adjusting train_ratio from {args.train_ratio:.3f} to {effective_train_ratio:.3f} for test set rebuilding")
                    print(f"[INFO] This ensures at least {min_test_ratio:.1%} of data for test set to achieve target unseen ratio")
                else:
                    print(f"[INFO] Current train_ratio {args.train_ratio:.3f} provides sufficient test samples")
            
            train_dataset = TermLevelDataset(args.train_samples_path, split="train", train_ratio=effective_train_ratio)
            test_dataset = TermLevelDataset(args.train_samples_path, split="test", train_ratio=effective_train_ratio)
    
    # æ•°æ®åŠ è½½å™¨
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size // world_size,
        sampler=train_sampler,
        collate_fn=collate_keep,
        num_workers=16,  # æé«˜åˆ° 16 ä»¥å……åˆ†åˆ©ç”¨ 64 vCPU
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4,  # å¢åŠ é¢„å–å› å­
        worker_init_fn=worker_init_fn,
        drop_last=True  # é¿å…æœ€åä¸€ä¸ªbatchå¤§å°ä¸ä¸€è‡´å¯¼è‡´çš„DDPé—®é¢˜
    )
    
    # é‡æ–°æ„å»ºæµ‹è¯•é›†ä»¥ç¡®ä¿unseenæœ¯è¯­æ¯”ä¾‹ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    if rank == 0:
        # åˆ†åˆ«æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æœ¯è¯­
        used_terms_train = extract_all_used_terms(train_dataset)
        used_terms_test = extract_all_used_terms(test_dataset)
        
        # è½¬æ¢ä¸ºå°å†™å¹¶å»é‡
        train_terms_set = set(t.lower() for t in used_terms_train)
        test_terms_set = set(t.lower() for t in used_terms_test)
        
        print(f"[DEBUG] Original - Train terms: {len(train_terms_set)}, Test terms: {len(test_terms_set)}")
        
        # è®¡ç®—unseen termsï¼ˆåœ¨æµ‹è¯•é›†ä¸­ä½†ä¸åœ¨è®­ç»ƒé›†ä¸­çš„æœ¯è¯­ï¼‰
        unseen_terms = test_terms_set - train_terms_set
        seen_terms = test_terms_set & train_terms_set
        
        initial_unseen_ratio = len(unseen_terms) / len(test_terms_set) if len(test_terms_set) > 0 else 0.0
        print(f"[INFO] Original test set unseen terms ratio: {initial_unseen_ratio:.2%} ({len(unseen_terms)}/{len(test_terms_set)})")
        
        # é‡æ–°æ„å»ºæµ‹è¯•é›†ä»¥ç¡®ä¿unseenæœ¯è¯­æ¯”ä¾‹è¾¾åˆ°ç›®æ ‡å€¼
        if args.rebuild_test_set:
            print(f"[INFO] Rebuilding test set to ensure {args.target_unseen_ratio:.0%} unseen terms ratio...")
            
            # æŒ‰æœ¯è¯­ç±»å‹åˆ†ç±»æµ‹è¯•æ ·æœ¬
            seen_samples = []    # åŒ…å«seenæœ¯è¯­çš„æ ·æœ¬
            unseen_samples = []  # åŒ…å«unseenæœ¯è¯­çš„æ ·æœ¬
            mixed_samples = []   # åŒæ—¶åŒ…å«seenå’Œunseenæœ¯è¯­çš„æ ·æœ¬
            
            for i, sample in enumerate(test_dataset.samples):
                ground_truth_terms = sample.get('term_chunk_audio_ground_truth_terms', [])
                sample_terms = set(t.lower() for t in ground_truth_terms if isinstance(t, str))
                
                sample_seen_terms = sample_terms & train_terms_set
                sample_unseen_terms = sample_terms - train_terms_set
                
                if sample_unseen_terms and sample_seen_terms:
                    mixed_samples.append((i, sample, sample_seen_terms, sample_unseen_terms))
                elif sample_unseen_terms:
                    unseen_samples.append((i, sample, set(), sample_unseen_terms))
                elif sample_seen_terms:
                    seen_samples.append((i, sample, sample_seen_terms, set()))
            
            print(f"[DEBUG] Sample classification: seen={len(seen_samples)}, unseen={len(unseen_samples)}, mixed={len(mixed_samples)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„unseenæ ·æœ¬
            total_unseen_contributing = len(unseen_samples) + len(mixed_samples)
            if total_unseen_contributing < int(args.target_test_size * args.target_unseen_ratio * 0.5):  # è‡³å°‘è¦æœ‰ç›®æ ‡çš„50%
                print(f"[WARN] Insufficient unseen samples ({total_unseen_contributing}) to achieve target ratio. Consider adjusting train_ratio.")
                print(f"[WARN] Current test set size: {len(test_dataset)}, unseen-contributing samples: {total_unseen_contributing}")
                print(f"[WARN] Proceeding with available samples...")
            
            # ä¼˜å…ˆé€‰æ‹©åŒ…å«unseenæœ¯è¯­çš„æ ·æœ¬
            selected_samples = []
            import random
            random.seed(42)  # ç¡®ä¿å¯å¤ç°
            
            # 1. ä¼˜å…ˆé€‰æ‹©mixedæ ·æœ¬ï¼ˆå®ƒä»¬è´¡çŒ®unseenæœ¯è¯­ï¼‰
            random.shuffle(mixed_samples)
            for idx, sample, seen_terms, unseen_terms in mixed_samples:
                if len(selected_samples) < args.target_test_size:
                    selected_samples.append(sample)
            
            # 2. æ·»åŠ pure unseenæ ·æœ¬
            random.shuffle(unseen_samples)
            for idx, sample, seen_terms, unseen_terms in unseen_samples:
                if len(selected_samples) < args.target_test_size:
                    selected_samples.append(sample)
            
            # 3. ç”¨seenæ ·æœ¬å¡«å……å‰©ä½™ä½ç½®
            random.shuffle(seen_samples)
            for idx, sample, seen_terms, unseen_terms in seen_samples:
                if len(selected_samples) < args.target_test_size:
                    selected_samples.append(sample)
            
            # å¦‚æœæ ·æœ¬ä¸è¶³ç›®æ ‡æ•°é‡ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ·æœ¬
            final_test_size = min(args.target_test_size, len(selected_samples))
            test_dataset.samples = selected_samples[:final_test_size]
            
            # é‡æ–°è®¡ç®—æœ¯è¯­ç»Ÿè®¡
            used_terms_test_new = extract_all_used_terms(test_dataset)
            test_terms_set_new = set(t.lower() for t in used_terms_test_new)
            unseen_terms_new = test_terms_set_new - train_terms_set
            final_unseen_ratio = len(unseen_terms_new) / len(test_terms_set_new) if len(test_terms_set_new) > 0 else 0.0
            
            print(f"[INFO] Rebuilt test set: {len(test_dataset)} samples (target: {args.target_test_size})")
            print(f"[INFO] New test terms: {len(test_terms_set_new)} (unseen: {len(unseen_terms_new)}, ratio: {final_unseen_ratio:.2%})")
            
            if final_unseen_ratio < args.target_unseen_ratio * 0.8:  # å¦‚æœè¾¾ä¸åˆ°ç›®æ ‡çš„80%
                print(f"[WARN] Final unseen ratio ({final_unseen_ratio:.2%}) is significantly lower than target ({args.target_unseen_ratio:.2%})")
                print(f"[WARN] Consider using a smaller train_ratio to get more test samples")
            
            # æ›´æ–°å˜é‡
            used_terms_test = used_terms_test_new
            test_terms_set = test_terms_set_new
            unseen_terms = unseen_terms_new
        else:
            print(f"[INFO] Test set rebuilding disabled, using original test set ({len(test_dataset)} samples)")
    
    # è®¾ç½®è¯„ä¼°ç”¨çš„retrieverï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
    retriever = None
    if rank == 0:
        
        # ä½¿ç”¨å…¨é‡æœ¯è¯­å»ºç«‹æ£€ç´¢ç´¢å¼•ï¼ˆè®­ç»ƒé›†+æµ‹è¯•é›†çš„æ‰€æœ‰æœ¯è¯­ï¼‰
        # è¿™æ ·æ‰èƒ½æ­£ç¡®å¬å›æ‰€æœ‰å¯èƒ½çš„æœ¯è¯­ï¼ŒåŒ…æ‹¬unseen terms
        all_used_terms = list(train_terms_set | test_terms_set)
        
        print(f"[DEBUG] Using {len(all_used_terms)} terms for retriever (train+test all terms)")
        if len(all_used_terms) == 0:
            print("[ERROR] No terms found in datasets! This should not happen.")
            print(f"[DEBUG] Train dataset size: {len(train_dataset)}")
            print(f"[DEBUG] Test dataset size: {len(test_dataset)}")
            # æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬
            if len(train_dataset) > 0:
                sample = train_dataset[0]
                print(f"[DEBUG] Sample structure: {type(sample)}")
                print(f"[DEBUG] Sample content: {sample}")
        
        retriever = SimpleRetriever(enable_fusion=True, device=device)
        retriever.model = model.module
        retriever.index = faiss.IndexFlatL2(512)
        retriever.term_list = [{'term': t} for t in all_used_terms]
        
        print(f"[INFO] Setup complete. Training: {len(train_dataset)}, Test: {len(test_dataset)}")
        print(f"[INFO] Retriever terms: {len(all_used_terms)} (full vocabulary for proper evaluation)")
        print(f"[INFO] Unseen terms in retriever: {len(unseen_terms)} ({len(unseen_terms)/len(all_used_terms)*100:.1f}% of retriever vocabulary)")
        print(f"[INFO] Batch configuration: {args.batch_size} physical Ã— {args.gradient_accumulation_steps} accumulation = {args.batch_size * args.gradient_accumulation_steps} effective")
        print(f"[INFO] Per-GPU batch size: {args.batch_size // world_size}")
    
    # è®­ç»ƒå¾ªç¯
    best_recall = 0.0
    no_improve_epochs = 0
    scaler = torch.cuda.amp.GradScaler(enabled=not contains_fp16_params)
    if rank == 0:
        scaler_status = "enabled" if scaler.is_enabled() else "disabled"
        print(f"[INFO] GradScaler is {scaler_status} (trainable_params dtype check)")
        if args.check_lora_every > 0:
            print(f"[INFO] LoRA gradient inspection scheduled every {args.check_lora_every} optimizer steps")
        else:
            print("[INFO] LoRA gradient inspection disabled (--check_lora_every 0)")
    global_step = 0
    
    for epoch in range(args.epochs):
        if rank == 0:
            print(f"\n[INFO] Epoch {epoch+1}/{args.epochs}")
        
        train_sampler.set_epoch(epoch)
        model.train()
        total_loss = 0.0
        
        # è®­ç»ƒ
        pbar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}") if rank == 0 else train_dataloader
        
        valid_batches = 0
        empty_batches = 0
        failed_batches = 0
        accumulated_loss = 0.0
        accumulation_steps = 0
        
        for batch_idx, batch in enumerate(pbar):
            # åœ¨ç¬¬ä¸€ä¸ª epoch çš„ç¬¬ä¸€ä¸ª batch è¿›è¡Œæ€§èƒ½æµ‹è¯•
            if epoch == 0 and batch_idx == 0 and batch and len(batch) > 0:
                quick_performance_test(model, batch, device, rank)
            
            # æ ‡è®°æœ¬æ­¥æ˜¯å¦è¿›è¡Œäº† backward
            did_backward = torch.tensor([0], device=device, dtype=torch.int)

            # 1) åˆ¤ç©ºï¼šç©º batch ç›´æ¥è·³è¿‡ï¼ˆä¸è°ƒç”¨ scaler.updateï¼‰
            if not batch or len(batch) == 0:
                empty_batches += 1
                if rank == 0 and empty_batches <= 5:  # åªæ‰“å°å‰5ä¸ªç©ºbatch
                    print(f"[DEBUG] Empty batch at index {batch_idx}, total empty so far: {empty_batches}")
                # åŒæ­¥ä¸€ä¸‹ï¼Œä¿æŒå„ rank åœ¨ç›¸è¿‘æ­¥é•¿ï¼Œä½†ä¸åš update/step
                dist.all_reduce(did_backward, op=dist.ReduceOp.MIN)
                optimizer.zero_grad(set_to_none=True)
                continue

            # æ£€æŸ¥batchå†…å®¹
            if rank == 0 and batch_idx < 3:  # æ‰“å°å‰3ä¸ªbatchçš„ä¿¡æ¯
                print(f"[DEBUG] Batch {batch_idx} size: {len(batch)}")
                if len(batch) > 0:
                    sample = batch[0]
                    print(f"[DEBUG] Sample structure: {type(sample)}, length: {len(sample) if hasattr(sample, '__len__') else 'N/A'}")

            # 2) è®¡ç®— lossï¼ˆAMPï¼‰
            loss = None
            with torch.cuda.amp.autocast():
                try:
                    loss = train_step(model, batch, device, args)
                    valid_batches += 1
                except Exception as e:
                    failed_batches += 1
                    if rank == 0 and failed_batches <= 5:
                        print(f"[WARN] Batch {batch_idx} processing failed: {e}")
                    # ä¸è°ƒç”¨ scaler.updateï¼›åªåšåŒæ­¥é—¨æ§
                    dist.all_reduce(did_backward, op=dist.ReduceOp.MIN)
                    optimizer.zero_grad(set_to_none=True)
                    continue

            # 3) åªæœ‰æœ‰æ•ˆ loss æ‰ backwardï¼ˆä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼‰
            if (loss is not None) and loss.requires_grad and torch.isfinite(loss):
                # Scale loss by accumulation steps for proper averaging
                scaled_loss = loss / args.gradient_accumulation_steps
                scaler.scale(scaled_loss).backward()
                did_backward.fill_(1)
                accumulated_loss += float(loss.detach().item())
                accumulation_steps += 1

            # 4) è·¨ rank å¯¹é½ï¼šåªæœ‰å½“æ‰€æœ‰ rank éƒ½ did_backward==1 æ‰å¯èƒ½ step()
            dist.all_reduce(did_backward, op=dist.ReduceOp.MIN)
            
            # 5) æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç´¯ç§¯æ­¥æ•°æˆ–æœ€åä¸€ä¸ªbatch
            should_step = (accumulation_steps >= args.gradient_accumulation_steps) or (batch_idx == len(train_dataloader) - 1)
            
            if int(did_backward.item()) == 1 and should_step:
                if scaler.is_enabled():
                    scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)

                global_step += 1
                if rank == 0 and args.check_lora_every > 0 and global_step % args.check_lora_every == 0:
                    check_lora_training_status(model, global_step, rank)

                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
                total_loss += accumulated_loss
                accumulated_loss = 0.0
                accumulation_steps = 0
            elif int(did_backward.item()) == 0:
                # æœ‰ rank æœª backwardï¼šæ¸…é›¶ä½†ä¸ step
                optimizer.zero_grad(set_to_none=True)
                accumulated_loss = 0.0
                accumulation_steps = 0
        
        # åŒæ­¥æŸå¤±
        avg_loss = total_loss / len(train_dataloader) if len(train_dataloader) > 0 else 0.0
        loss_tensor = torch.tensor(avg_loss, device=device)
        dist.all_reduce(loss_tensor, op=dist.ReduceOp.SUM)
        avg_loss = loss_tensor.item() / world_size
        
        if rank == 0:
            print(f"[INFO] Epoch {epoch+1} avg loss: {avg_loss:.4f}")
            print(f"[INFO] Batch statistics: valid={valid_batches}, empty={empty_batches}, failed={failed_batches}")
        
        # è¯„ä¼°ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if rank == 0:
            recall_results = evaluate_topk_recall(
                model, retriever, test_dataset, device, 
                top_ks=(5, 10), max_eval=min(1000, len(test_dataset)),
                train_terms_set=train_terms_set  # ä¼ é€’è®­ç»ƒæœ¯è¯­é›†åˆç”¨äºseen/unseenåŒºåˆ†
            )
            
            # å¤„ç†æ–°çš„è¿”å›æ ¼å¼
            overall_results = recall_results.get('overall', recall_results)
            current_recall = sum(overall_results[10]) / len(overall_results[10]) if overall_results.get(10) else 0.0
            scheduler.step(current_recall)
            
            if current_recall > best_recall:
                best_recall = current_recall
                no_improve_epochs = 0
                best_model_path = args.save_path.replace('.pt', '_best.pt')
                torch.save(model.state_dict(), best_model_path)
                print(f"[INFO] New best model saved (Overall Recall@10: {best_recall:.2%})")
            else:
                no_improve_epochs += 1
                print(f"[INFO] No improvement for {no_improve_epochs} epochs")
                
                if no_improve_epochs >= args.patience:
                    print(f"[INFO] Early stopping. Best Overall Recall@10: {best_recall:.2%}")
                    break
        
        dist.barrier()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if rank == 0:
        torch.save(model.state_dict(), args.save_path)
        print(f"[INFO] Training completed. Best Recall@10: {best_recall:.2%}")
    
    cleanup_ddp()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch_size', type=int, default=512)  # Total batch size across 8 GPUs (64 per GPU)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--patience', type=int, default=2)
    parser.add_argument('--train_samples_path', type=str, required=True)
    parser.add_argument('--test_samples_path', type=str, default=None)
    parser.add_argument('--train_ratio', type=float, default=0.998)
    parser.add_argument('--glossary_path', type=str, default=None)
    parser.add_argument('--save_path', type=str, default="qwen2_audio_term_level.pt")
    parser.add_argument('--best_model_path', type=str, default=None)
    parser.add_argument('--audio_text_loss_ratio', type=float, default=0.3)
    parser.add_argument('--audio_term_loss_ratio', type=float, default=0.7)
    parser.add_argument('--model_name', type=str, default="Qwen/Qwen2-Audio-7B-Instruct")
    parser.add_argument('--lora_r', type=int, default=16)
    parser.add_argument('--lora_alpha', type=int, default=32)
    parser.add_argument('--lora_dropout', type=float, default=0.1)
    parser.add_argument('--mmap_shard_dir', type=str, default=None, help='Directory containing mmap audio shards')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=8, help='Number of steps to accumulate gradients (effective batch = batch_size * accumulation_steps)')
    parser.add_argument('--max_batch_per_gpu', type=int, default=None, help='Maximum batch size per GPU (will auto-adjust total batch_size if needed)')
    parser.add_argument('--check_lora_every', type=int, default=0, help='Inspect LoRA gradients every N optimizer steps (0 to disable)')
    parser.add_argument('--test_mode', action='store_true', help='Run in test mode (minimal setup for debugging)')
    parser.add_argument('--eval_only', action='store_true', help='Only run evaluation on the original model without training')
    parser.add_argument('--eval_max_samples', type=int, default=1000, help='Maximum number of samples to evaluate (for eval_only mode)')
    parser.add_argument('--rebuild_test_set', action='store_true', help='Rebuild test set to ensure 20% unseen terms ratio')
    parser.add_argument('--target_test_size', type=int, default=1000, help='Target size for rebuilt test set')
    parser.add_argument('--target_unseen_ratio', type=float, default=0.20, help='Target ratio of unseen terms in test set')
    
    args = parser.parse_args()
    
    # è·å–world size
    world_size = int(os.environ.get('WORLD_SIZE', torch.cuda.device_count()))
    
    if world_size == 0:
        print("[ERROR] No CUDA devices available!")
        return 1
    # è®­ç»ƒæ¨¡å¼
    # è‡ªåŠ¨è°ƒæ•´batch sizeä»¥é˜²æ­¢OOM
    if args.max_batch_per_gpu:
        max_total_batch = args.max_batch_per_gpu * world_size
        if args.batch_size > max_total_batch:
            print(f"[WARN] Batch size {args.batch_size} exceeds max ({max_total_batch}), adjusting...")
            args.batch_size = max_total_batch
    
    print(f"[INFO] Starting DDP training with {world_size} GPUs")
    print(f"[INFO] Physical batch size: {args.batch_size} (per GPU: {args.batch_size // world_size})")
    print(f"[INFO] Gradient accumulation steps: {args.gradient_accumulation_steps}")
    print(f"[INFO] Effective batch size: {args.batch_size * args.gradient_accumulation_steps}")
    
    # ä½¿ç”¨torchrunå¯åŠ¨æ—¶ï¼ŒLOCAL_RANKä¼šè‡ªåŠ¨è®¾ç½®
    if 'LOCAL_RANK' in os.environ:
        # torchrunæ¨¡å¼
        rank = int(os.environ['LOCAL_RANK'])
        train_ddp(rank, world_size, args)
    else:
        # æ‰‹åŠ¨å¤šè¿›ç¨‹æ¨¡å¼
        mp.set_start_method('spawn', force=True)
        mp.spawn(train_ddp, args=(world_size, args), nprocs=world_size, join=True)
    
    print("[INFO] Training completed")
    return 0


if __name__ == "__main__":
    main()
