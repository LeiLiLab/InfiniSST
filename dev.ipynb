{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare original with flashinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-21 19:09:39,288] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/siqiouya/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.6;8.9\"\n",
    "\n",
    "import torch\n",
    "import flashinfer\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "from model.qwen25 import SpeechQwenModel, SpeechQwenForCausalLM\n",
    "from model.w2v2 import SpeechEncoderW2V2RoPE, W2V2RoPECache, LayerCache\n",
    "from model.patches.patch_w2v2 import patch_w2v2\n",
    "from model.patches.patch_qwen25 import patch_qwen25\n",
    "\n",
    "from model.flashinfer.engine import init_paged_kv_cache\n",
    "from model.flashinfer.sqwen import SpeechQwenFastModel, SpeechQwenFastForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flashinfer causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.6;8.9\"\n",
    "\n",
    "import torch\n",
    "import flashinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2RotaryEmbedding, apply_rotary_pos_emb, repeat_kv\n",
    "from transformers.models.qwen2.configuration_qwen2 import Qwen2Config\n",
    "qwen_cfg = Qwen2Config.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "rotary_emb = Qwen2RotaryEmbedding(config=qwen_cfg).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2527180/47773370.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cqkv = torch.load('cor_qkv.pt')\n",
      "/tmp/ipykernel_2527180/47773370.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  nqkv = torch.load('new_qkv.pt')\n"
     ]
    }
   ],
   "source": [
    "cqkv = torch.load('cor_qkv.pt')\n",
    "nqkv = torch.load('new_qkv.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = cqkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.view(q.size(0), q.size(1), qwen_cfg.num_attention_heads, -1)\n",
    "k = k.view(k.size(0), k.size(1), qwen_cfg.num_key_value_heads, -1)\n",
    "v = v.view(v.size(0), v.size(1), qwen_cfg.num_key_value_heads, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.rand_like(q) * 10\n",
    "k = torch.rand_like(k) * 10\n",
    "v = torch.rand_like(v) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 23:51:35,253 - INFO - flashinfer.jit: Loading JIT ops: single_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_head_dim_128_posenc_1_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "/home/siqiouya/anaconda3/envs/infinisst/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/home/siqiouya/anaconda3/envs/infinisst/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "2025-04-21 23:51:48,857 - INFO - flashinfer.jit: Finished loading JIT ops: single_prefill_with_kv_cache_dtype_q_f16_dtype_kv_f16_dtype_o_f16_head_dim_128_posenc_1_use_swa_False_use_logits_cap_False_f16qk_False\n"
     ]
    }
   ],
   "source": [
    "qo_len = q.size(1)\n",
    "kv_len = k.size(1)\n",
    "num_qo_heads = qwen_cfg.num_attention_heads\n",
    "num_kv_heads = qwen_cfg.num_key_value_heads\n",
    "head_dim = qwen_cfg.hidden_size // qwen_cfg.num_attention_heads\n",
    "o = flashinfer.single_prefill_with_kv_cache(\n",
    "    q[0], \n",
    "    k[0], \n",
    "    v[0],\n",
    "    pos_encoding_mode='ROPE_LLAMA',\n",
    "    rope_scale=1.0,\n",
    "    rope_theta=qwen_cfg.rope_theta,\n",
    "    causal=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_ = q.transpose(1, 2)\n",
    "k_ = k.transpose(1, 2)\n",
    "v_ = v.transpose(1, 2)\n",
    "\n",
    "k_ = repeat_kv(k_, num_qo_heads // num_kv_heads)\n",
    "v_ = repeat_kv(v_, num_qo_heads // num_kv_heads)\n",
    "\n",
    "position_ids = torch.arange(kv_len, device=\"cuda:0\").view(1, -1)\n",
    "cos, sin = rotary_emb(k_, position_ids)\n",
    "k_, _ = apply_rotary_pos_emb(k_, k_, cos, sin)\n",
    "q_position_ids = torch.arange(kv_len - qo_len, kv_len, device=\"cuda:0\").view(1, -1)\n",
    "q_cos, q_sin = rotary_emb(q_, q_position_ids)\n",
    "q_, _ = apply_rotary_pos_emb(q_, q_, q_cos, q_sin)\n",
    "\n",
    "mask = torch.tril(\n",
    "    torch.full((qo_len, kv_len), True, device=\"cuda:0\"),\n",
    "    diagonal=(kv_len - qo_len),\n",
    ").unsqueeze(0)\n",
    "\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    q_, k_, v_,\n",
    "    attn_mask=mask,\n",
    ")\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(o, attn_output, atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9372e-05, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(o - attn_output).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.5+cu124torch2.5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flashinfer.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_qwen25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type qwen2 to instantiate a model of type SpeechQwen. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dfe0d37d78461592fa89d271b42ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SpeechQwenForCausalLM.from_pretrained(\n",
    "    \"/data/user_data/siqiouya/runs/pretrained/qwen2.5-7b-instruct\",\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map='cuda',\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type qwen2 to instantiate a model of type SpeechQwenFast. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3fa08470e849c9b98b49ebbb4cfba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_flash = SpeechQwenFastForCausalLM.from_pretrained(\n",
    "    \"/data/user_data/siqiouya/runs/pretrained/qwen2.5-7b-instruct\",\n",
    "    torch_dtype=dtype,\n",
    "    attn_implementation=\"eager\",\n",
    "    device_map='cuda',\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/user_data/siqiouya/runs/pretrained/qwen2.5-7b-instruct\",\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = \"<|finetune_right_pad_id|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, world! Hello, world! Hello, world! Hello, world!\", return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = model.get_input_embeddings()(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable = \\\n",
    "    init_paged_kv_cache(\n",
    "        1,\n",
    "        576,\n",
    "        12,\n",
    "        16,\n",
    "        128,\n",
    "        1000,\n",
    "        model.config.num_hidden_layers,\n",
    "        model.config.num_key_value_heads,\n",
    "        model.config.hidden_size // model.config.num_attention_heads,\n",
    "        dtype=dtype,\n",
    "        device_prefill='cuda:0',\n",
    "        device_decode='cuda:0'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [\n",
    "    {\n",
    "        \"input_ids\": inputs.input_ids.view(-1),\n",
    "        \"cache\": None\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = super(SpeechQwenModel, model.model).forward(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 19:09:48,594 - INFO - flashinfer.jit: Loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_1_use_swa_False_use_logits_cap_False_f16qk_False\n",
      "2025-04-21 19:09:48,791 - INFO - flashinfer.jit: Finished loading JIT ops: batch_prefill_with_kv_cache_dtype_q_bf16_dtype_kv_bf16_dtype_o_bf16_dtype_idx_i32_head_dim_qk_128_head_dim_vo_128_posenc_1_use_swa_False_use_logits_cap_False_f16qk_False\n"
     ]
    }
   ],
   "source": [
    "output_flash = super(SpeechQwenFastModel, model_flash.model).forward(\n",
    "    inputs_embeds=inputs_embeds.view(-1, inputs_embeds.size(-1)),\n",
    "    requests=requests,\n",
    "    pagetable=llm_prefill_pagetable,\n",
    "    output_hidden_states=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2109, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_idx = -2\n",
    "(output['hidden_states'][layer_idx] - output_flash[-1][layer_idx]).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6797, -1.2578, -2.1406,  ..., -2.7500,  0.3086, -2.0156],\n",
       "          [-0.7500, -0.7891, -1.1250,  ..., -2.7031,  1.1250, -4.5312],\n",
       "          [-2.5156,  1.4766, -1.0859,  ..., -0.7852,  0.5820, -0.2217],\n",
       "          ...,\n",
       "          [-1.1953,  0.6914, -0.1172,  ...,  1.9922,  0.2373, -1.2578],\n",
       "          [-1.9219, -1.2656, -0.1934,  ..., -1.0078, -2.6250, -0.0757],\n",
       "          [-1.7500,  0.3066, -0.7812,  ..., -0.5000, -0.5625, -0.5625]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>),\n",
       " tensor([[ -0.1406,  -7.6250,  -3.8906,  ...,  -8.5625,  -0.8203, -12.4375],\n",
       "         [ -1.1562,   0.1250,  -5.8125,  ...,  -7.5312,   5.0000, -13.9375],\n",
       "         [ -6.7812,   5.3438,  -5.7188,  ...,  -1.2031,   2.8125,  -0.9375],\n",
       "         ...,\n",
       "         [ -5.5312,   5.0312,  -3.9688,  ...,   9.4375,  -0.1719,  -4.3125],\n",
       "         [-10.5625,  -5.6250,  -7.1875,  ...,  -4.1250,  -8.4375,  -1.8906],\n",
       "         [ -5.1875,   2.3750,  -4.5938,  ...,  -0.0312,  -0.3281,  -1.4688]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_idx = -1\n",
    "output['hidden_states'][layer_idx], output_flash[-1][layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 18:10:14 | WARNING | model.flashinfer.modeling_qwen2 | The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    }
   ],
   "source": [
    "output = model_flash.model.layers[0].self_attn.forward_vanilla(\n",
    "    inputs_embeds, \n",
    "    position_ids=torch.arange(4).unsqueeze(0).to(\"cuda\"),\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qo_indptr = torch.tensor([0, 4], dtype=torch.int32, device=\"cuda:0\")\n",
    "paged_kv_indptr = torch.tensor([0, 1], dtype=torch.int32, device=\"cuda:0\")\n",
    "paged_kv_indices = torch.arange(1, dtype=torch.int32, device=\"cuda:0\")\n",
    "paged_kv_last_page_len = torch.tensor([4], dtype=torch.int32, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchPrefillWithPagedKVCacheWrapper' object has no attribute '_cached_q_data_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_flash \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_flash\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqo_indptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaged_kv_indptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaged_kv_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaged_kv_last_page_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_prefill_pagetable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/infinisst/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/infinisst/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/work/sllama-flashinfer/model/flashinfer/modeling_qwen2.py:387\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, qo_indptr, paged_kv_indptr, paged_kv_indices, paged_kv_last_page_len, pagetable)\u001b[0m\n\u001b[1;32m    366\u001b[0m batch_indices, positions \u001b[38;5;241m=\u001b[39m flashinfer\u001b[38;5;241m.\u001b[39mget_batch_indices_positions(\n\u001b[1;32m    367\u001b[0m     qo_indptr,\n\u001b[1;32m    368\u001b[0m     flashinfer\u001b[38;5;241m.\u001b[39mget_seq_lens(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m     qo_indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    374\u001b[0m )\n\u001b[1;32m    376\u001b[0m flashinfer\u001b[38;5;241m.\u001b[39mappend_paged_kv_cache(\n\u001b[1;32m    377\u001b[0m     key_states,\n\u001b[1;32m    378\u001b[0m     value_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     paged_kv_last_page_len,\n\u001b[1;32m    385\u001b[0m )\n\u001b[0;32m--> 387\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mpagetable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpagetable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaged_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m    392\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/infinisst/lib/python3.9/site-packages/flashinfer/prefill.py:1568\u001b[0m, in \u001b[0;36mBatchPrefillWithPagedKVCacheWrapper.run\u001b[0;34m(self, q, paged_kv_cache, k_scale, v_scale, out, lse, return_lse, *args)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute batch prefill/append attention between query and paged kv-cache.\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \n\u001b[1;32m   1526\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;124;03m    * The logsumexp of attention output, shape: ``[qo_indptr[-1], num_qo_heads]``.\u001b[39;00m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m k_cache, v_cache \u001b[38;5;241m=\u001b[39m _unpack_paged_kv_cache(paged_kv_cache, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kv_layout)\n\u001b[1;32m   1567\u001b[0m _check_cached_qkv_data_type(\n\u001b[0;32m-> 1568\u001b[0m     q, k_cache, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_q_data_type\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_kv_data_type\n\u001b[1;32m   1569\u001b[0m )\n\u001b[1;32m   1570\u001b[0m stride_block \u001b[38;5;241m=\u001b[39m k_cache\u001b[38;5;241m.\u001b[39mstride(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kv_layout \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNHD\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchPrefillWithPagedKVCacheWrapper' object has no attribute '_cached_q_data_type'"
     ]
    }
   ],
   "source": [
    "output_flash = model_flash.model.layers[0].self_attn(\n",
    "    inputs_embeds, \n",
    "    qo_indptr,\n",
    "    paged_kv_indptr,\n",
    "    paged_kv_indices,\n",
    "    paged_kv_last_page_len,\n",
    "    llm_prefill_pagetable,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0045, 0.0000, 0.0247,  ..., 0.0889, 0.4297, 0.1045],\n",
       "         [0.4512, 0.0135, 0.0408,  ..., 0.0381, 0.1436, 0.0732],\n",
       "         [0.3965, 0.0240, 0.0203,  ..., 0.0055, 2.2500, 0.0454],\n",
       "         [0.1289, 0.0166, 0.0047,  ..., 0.0000, 0.0000, 0.0208]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((output - output_flash).abs() / output.abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchPrefillWithPagedKVCacheWrapper' object has no attribute '_causal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm_prefill_pagetable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_causal\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchPrefillWithPagedKVCacheWrapper' object has no attribute '_causal'"
     ]
    }
   ],
   "source": [
    "llm_prefill_pagetable.wrapper._causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speech encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_cfg = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_batch = torch.rand(1, 15759).to(device='cuda', dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_w2v2(True)\n",
    "speech_encoder_args = [\n",
    "    \"/compute/babel-4-1/siqiouya/wav2_vec_vox_960h_pl.pt\",\n",
    "    True,\n",
    "    \"[(1024,2,2)] * 2\",\n",
    "    \n",
    "    48,\n",
    "    572,\n",
    "    4096,\n",
    "    None,\n",
    "    True,\n",
    "    False,\n",
    "]\n",
    "speech_encoder = SpeechEncoderW2V2RoPE(*speech_encoder_args).to(device='cuda', dtype=torch.bfloat16).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    speech_encoder.set_blocksize(48)\n",
    "    cache = W2V2RoPECache(\n",
    "        max_steps=speech_encoder.max_cache_size,\n",
    "        layers=[LayerCache() for _ in range(speech_encoder.s_layer)]\n",
    "    )\n",
    "    output = speech_encoder.speech_encoder.extract_features(speech_batch, cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['x'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_encoder_args = [\n",
    "    \"/compute/babel-4-1/siqiouya/wav2_vec_vox_960h_pl.pt\",\n",
    "    True,\n",
    "    \"[(1024,2,2)] * 2\",\n",
    "    \n",
    "    48,\n",
    "    572,\n",
    "    4096,\n",
    "    None,\n",
    "    True,\n",
    "    True,\n",
    "]\n",
    "speech_encoder_flash = SpeechEncoderW2V2RoPE(*speech_encoder_args).to(device='cuda', dtype=torch.bfloat16).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_cfg = speech_encoder_flash.speech_encoder.cfg\n",
    "speech_pagetable, llm_prefill_pagetable, llm_decode_pagetable = \\\n",
    "    init_paged_kv_cache(\n",
    "        1,\n",
    "        576,\n",
    "        speech_cfg.encoder_layers,\n",
    "        speech_cfg.encoder_attention_heads,\n",
    "        speech_cfg.encoder_embed_dim // speech_cfg.encoder_attention_heads,\n",
    "        1000,\n",
    "        qwen_cfg.num_hidden_layers,\n",
    "        qwen_cfg.num_key_value_heads,\n",
    "        qwen_cfg.hidden_size // qwen_cfg.num_attention_heads,\n",
    "        device_prefill='cuda:0',\n",
    "        device_decode='cuda:0'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests = [\n",
    "    {\n",
    "        \"speech\": speech_batch.view(-1),\n",
    "        \"blocksize\": 48,\n",
    "        \"cache\": None\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_flash = speech_encoder_flash.speech_encoder(requests, speech_pagetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_results = output['layer_results']\n",
    "layer_results_flash = output_flash[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 6\n",
    "(layer_results[layer_idx][0] - layer_results_flash[layer_idx][0]).abs() / layer_results[layer_idx][0].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flashinfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-21 21:37:27,281] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/siqiouya/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.6;8.9\"\n",
    "\n",
    "import torch\n",
    "import flashinfer\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "from model.flashinfer.modeling_qwen2 import Qwen2RotaryEmbedding, apply_rotary_pos_emb\n",
    "from transformers.models.qwen2.configuration_qwen2 import Qwen2Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qo_heads = 32\n",
    "num_kv_heads = 32\n",
    "head_dim = 128\n",
    "max_num_pages = 128\n",
    "page_size = 16\n",
    "# allocate 128MB workspace buffer\n",
    "workspace_buffer = torch.empty(256 * 1024 * 1024, dtype=torch.uint8, device=\"cuda:0\")\n",
    "prefill_wrapper = flashinfer.BatchPrefillWithPagedKVCacheWrapper(\n",
    "    workspace_buffer, \"NHD\"\n",
    ")\n",
    "batch_size = 1\n",
    "nnz_qo = 128\n",
    "qo_indptr = torch.tensor(\n",
    "    [0, nnz_qo], dtype=torch.int32, device=\"cuda:0\"\n",
    ")\n",
    "paged_kv_indices = torch.arange(nnz_qo // page_size).int().to(\"cuda:0\")\n",
    "paged_kv_indptr = torch.tensor(\n",
    "    [0, nnz_qo // page_size], dtype=torch.int32, device=\"cuda:0\"\n",
    ")\n",
    "# 1 <= paged_kv_last_page_len <= page_size\n",
    "paged_kv_last_page_len = torch.tensor(\n",
    "    [(nnz_qo - 1) % page_size + 1], dtype=torch.int32, device=\"cuda:0\"\n",
    ")\n",
    "q_at_layer = torch.randn(nnz_qo, num_qo_heads, head_dim, dtype=torch.bfloat16).to(\"cuda:0\")\n",
    "kv_cache_at_layer = torch.randn(\n",
    "    max_num_pages, 2, page_size, num_kv_heads, head_dim, dtype=torch.bfloat16, device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_cfg = Qwen2Config.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create auxiliary data structures for batch prefill attention\n",
    "prefill_wrapper.plan(\n",
    "    qo_indptr,\n",
    "    paged_kv_indptr,\n",
    "    paged_kv_indices,\n",
    "    paged_kv_last_page_len,\n",
    "    num_qo_heads,\n",
    "    num_kv_heads,\n",
    "    head_dim,\n",
    "    page_size,\n",
    "    causal=True,\n",
    "    pos_encoding_mode='ROPE_LLAMA',\n",
    "    rope_scale=1.0,\n",
    "    rope_theta=qwen_cfg.rope_theta,\n",
    "    q_data_type=torch.bfloat16,\n",
    ")\n",
    "o = prefill_wrapper.run(q_at_layer, kv_cache_at_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_emb = Qwen2RotaryEmbedding(config=qwen_cfg).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 32, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_cache_at_layer[:nnz_qo // page_size, 0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q_at_layer.unsqueeze(0).transpose(1, 2)\n",
    "k = kv_cache_at_layer[:nnz_qo // page_size, 0].reshape(-1, num_kv_heads, head_dim).unsqueeze(0).transpose(1, 2)\n",
    "v = kv_cache_at_layer[:nnz_qo // page_size, 1].reshape(-1, num_kv_heads, head_dim).unsqueeze(0).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q, k = rotary_emb.rotate_queries_with_cached_keys(q, k)\n",
    "cos, sin = rotary_emb(v, torch.arange(nnz_qo, device=\"cuda:0\").view(1, -1))\n",
    "q, k = apply_rotary_pos_emb(q, k, cos, sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.contiguous()\n",
    "k = k.contiguous()\n",
    "v = v.contiguous()\n",
    "\n",
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    q, k, v,\n",
    "    is_causal=True,\n",
    ")\n",
    "attn_output = attn_output.transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0006, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn_output[0] - o).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infinisst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
